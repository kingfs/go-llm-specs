id: minimax/minimax-m1
name: 'MiniMax: MiniMax M1'
provider: Minimax
description: |-
  MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom "lightning attention" mechanism, allowing it to process long sequences—up to 1 million tokens—while maintaining competitive FLOP efficiency. With 456 billion total parameters and 45.9B active per token, this variant is optimized for complex, multi-step reasoning tasks.

  Trained via a custom reinforcement learning pipeline (CISPO), M1 excels in long-context understanding, software engineering, agentic tool use, and mathematical reasoning. Benchmarks show strong performance across FullStackBench, SWE-bench, MATH, GPQA, and TAU-Bench, often outperforming other open models like DeepSeek R1 and Qwen3-235B.
description_cn: |-
  MiniMax-M1 是一款大规模开源权重推理模型，专为长上下文和高效率推理而设计。该模型采用混合专家（MoE）架构，并结合自研的“闪电注意力”机制，可处理长达100万 token 的序列，同时保持出色的 FLOP 效率。模型总参数量达4560亿，每 token 激活459亿参数，专为复杂多步推理任务优化。

  通过自研强化学习流程（CISPO）训练，M1 在长上下文理解、软件工程、智能体工具调用和数学推理方面表现突出。在 FullStackBench、SWE-bench、MATH、GPQA 和 TAU-Bench 等基准测试中成绩优异，常优于 DeepSeek R1 和 Qwen3-235B 等其他开源模型。
context_length: 1000000
max_output: 40000
price_in: 4e-07
price_out: 2.2e-06
features:
  - CapChat
  - CapFunctionCall
  - ModalityTextIn
  - ModalityTextOut
