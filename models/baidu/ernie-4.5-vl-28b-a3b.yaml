description_cn: 这是一款强大的多模态混合专家（MoE）对话模型，总参数量达 280 亿，每 token 激活 30 亿参数，依托创新的异构 MoE 架构与模态隔离路由机制，实现卓越的文本与视觉理解能力。模型基于高吞吐训练与推理的高效扩展基础设施构建，采用 SFT、DPO 和 UPO 等先进后训练技术优化性能，支持高达 131K 的上下文长度，并通过 RLVR 对齐机制显著提升跨模态推理与生成能力。
id: baidu/ernie-4.5-vl-28b-a3b
