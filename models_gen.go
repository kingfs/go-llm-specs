// Code generated by llm-specs-gen. DO NOT EDIT.
// Generated at: 2026-01-20T03:30:24Z

package llmspecs

func init() {
	staticRegistry = map[string]*modelData{
		"ai21/jamba-large-1.7": {
			IDVal:         "ai21/jamba-large-1.7",
			NameVal:       "AI21: Jamba Large 1.7",
			ProviderVal:   "Ai21",
			DescVal:       "Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, instruction-following, and overall efficiency. Built on a hybrid SSM-Transformer architecture with a 256K context window, it delivers more accurate, contextually grounded responses and better steerability than previous versions.",
			DescCNVal:     "Jamba Large 1.7 是 Jamba 开源系列的最新模型，在事实依据、指令遵循和整体效率方面均有提升。该模型基于混合 SSM-Transformer 架构，支持 256K 上下文窗口，相比前代版本可提供更准确、上下文关联更强的响应以及更优的可控性。",
			ContextLenVal: 256000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "jamba-large-1.7" },
		},
		"ai21/jamba-mini-1.7": {
			IDVal:         "ai21/jamba-mini-1.7",
			NameVal:       "AI21: Jamba Mini 1.7",
			ProviderVal:   "Ai21",
			DescVal:       "Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transformer hybrid architecture and 256K context window. Despite its compact size, it delivers accurate, contextually grounded responses and improved steerability.",
			DescCNVal:     "Jamba Mini 1.7 是 Jamba 开源模型家族中一款紧凑高效的成员，在保持 SSM-Transformer 混合架构和 256K 上下文窗口优势的同时，显著提升了事实依据能力和指令遵循能力。尽管体积小巧，仍能提供准确、上下文关联性强的响应及增强的可控性。",
			ContextLenVal: 256000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "jamba-mini-1.7" },
		},
		"aion-labs/aion-1.0": {
			IDVal:         "aion-labs/aion-1.0",
			NameVal:       "AionLabs: Aion-1.0",
			ProviderVal:   "Aion-Labs",
			DescVal:       "Aion-1.0 is a multi-model system designed for high performance across various tasks, including reasoning and coding. It is built on DeepSeek-R1, augmented with additional models and techniques such as Tree of Thoughts (ToT) and Mixture of Experts (MoE). It is Aion Lab's most powerful reasoning model.",
			DescCNVal:     "Aion-1.0 是一个多模型系统，旨在在推理、编码等多种任务上实现高性能。该系统基于 DeepSeek-R1 构建，并融合了思维树（Tree of Thoughts, ToT）和混合专家（Mixture of Experts, MoE）等额外模型与技术，是 Aion Lab 最强大的推理模型。",
			ContextLenVal: 131072,
			MaxOutputVal:  32768,
			PriceInVal:    0.000004,
			PriceOutVal:   0.000008,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "aion-1.0" },
		},
		"aion-labs/aion-1.0-mini": {
			IDVal:         "aion-labs/aion-1.0-mini",
			NameVal:       "AionLabs: Aion-1.0-Mini",
			ProviderVal:   "Aion-Labs",
			DescVal:       "Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant of a FuseAI model that outperforms R1-Distill-Qwen-32B and R1-Distill-Llama-70B, with benchmark results available on its [Hugging Face page](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview), independently replicated for verification.",
			DescCNVal:     "Aion-1.0-Mini 是一个 32B 参数模型，为 DeepSeek-R1 模型的蒸馏版本，专为数学、编码和逻辑等推理领域提供强大性能。该模型是 FuseAI 模型的一个改进变体，在基准测试中优于 R1-Distill-Qwen-32B 和 R1-Distill-Llama-70B，其基准结果可在其 [Hugging Face 页面](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview) 查阅，并已由第三方独立复现验证。",
			ContextLenVal: 131072,
			MaxOutputVal:  32768,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "aion-1.0-mini" },
		},
		"aion-labs/aion-rp-llama-3.1-8b": {
			IDVal:         "aion-labs/aion-rp-llama-3.1-8b",
			NameVal:       "AionLabs: Aion-RP 1.0 (8B)",
			ProviderVal:   "Aion-Labs",
			DescVal:       "Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other’s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.",
			DescCNVal:     "Aion-RP-Llama-3.1-8B 在 RPBench-Auto 基准的角色扮演评估部分中排名第一。RPBench-Auto 是 Arena-Hard-Auto 的角色扮演专用变体，采用大语言模型相互评估回复质量。该模型是一个经过微调的基础模型（非指令微调模型），旨在生成更自然、更多样化的文本。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "aion-rp-llama-3.1-8b" },
		},
		"alfredpros/codellama-7b-instruct-solidity": {
			IDVal:         "alfredpros/codellama-7b-instruct-solidity",
			NameVal:       "AlfredPros: CodeLLaMa 7B Instruct Solidity",
			ProviderVal:   "Alfredpros",
			DescVal:       "A finetuned 7 billion parameters Code LLaMA - Instruct model to generate Solidity smart contract using 4-bit QLoRA finetuning provided by PEFT library.",
			DescCNVal:     "基于 PEFT 库提供的 4 位 QLoRA 微调方法，对拥有 70 亿参数的 Code LLaMA - Instruct 模型进行微调，专用于生成 Solidity 智能合约。",
			ContextLenVal: 4096,
			MaxOutputVal:  4096,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "codellama-7b-instruct-solidity" },
		},
		"alibaba/tongyi-deepresearch-30b-a3b": {
			IDVal:         "alibaba/tongyi-deepresearch-30b-a3b",
			NameVal:       "Tongyi DeepResearch 30B A3B",
			ProviderVal:   "Qwen",
			DescVal:       "Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion total parameters activating only 3 billion per token. It's optimized for long-horizon, deep information-seeking tasks and delivers state-of-the-art performance on benchmarks like Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch, and FRAMES. This makes it superior for complex agentic search, reasoning, and multi-step problem-solving compared to prior models.\n\nThe model includes a fully automated synthetic data pipeline for scalable pre-training, fine-tuning, and reinforcement learning. It uses large-scale continual pre-training on diverse agentic data to boost reasoning and stay fresh. It also features end-to-end on-policy RL with a customized Group Relative Policy Optimization, including token-level gradients and negative sample filtering for stable training. The model supports ReAct for core ability checks and an IterResearch-based 'Heavy' mode for max performance through test-time scaling. It's ideal for advanced research agents, tool use, and heavy inference workflows.",
			DescCNVal:     "通义深度研究（Tongyi DeepResearch）是通义实验室研发的智能体大语言模型，总参数量为300亿，每token仅激活30亿参数。该模型专为长周期、深度信息检索任务优化，在Humanity's Last Exam、BrowserComp、BrowserComp-ZH、WebWalkerQA、GAIA、xbench-DeepSearch和FRAMES等基准测试中达到业界领先水平，相较前代模型在复杂智能体搜索、推理及多步问题求解方面表现更优。\n\n该模型采用全自动合成数据流水线，支持可扩展的预训练、微调与强化学习。通过大规模持续预训练多样化智能体数据，提升推理能力并保持知识时效性。同时，模型引入端到端在线策略强化学习，采用定制化的分组相对策略优化（Group Relative Policy Optimization），结合token级梯度与负样本过滤机制，确保训练稳定性。模型支持ReAct用于核心能力验证，并提供基于IterResearch的“重型”模式，通过测试时扩展实现极致性能，适用于高级研究智能体、工具调用及高负载推理工作流。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "tongyi-deepresearch-30b-a3b" },
		},
		"allenai/molmo-2-8b:free": {
			IDVal:         "allenai/molmo-2-8b:free",
			NameVal:       "AllenAI: Molmo2 8B (free)",
			ProviderVal:   "Allenai",
			DescVal:       "Molmo2-8B is an open vision-language model developed by the Allen Institute for AI (Ai2) as part of the Molmo2 family, supporting image, video, and multi-image understanding and grounding. It is based on Qwen3-8B and uses SigLIP 2 as its vision backbone, outperforming other open-weight, open-data models on short videos, counting, and captioning, while remaining competitive on long-video tasks.",
			DescCNVal:     "Molmo2-8B 是艾伦人工智能研究所（AI2）开发的开源视觉语言模型，属于 Molmo2 系列，支持图像、视频及多图理解与定位。该模型基于 Qwen3-8B 构建，采用 SigLIP 2 作为视觉主干网络，在短视频、计数和图像描述等任务上优于其他开源权重与开源数据的模型，同时在长视频任务中仍保持竞争力。",
			ContextLenVal: 36864,
			MaxOutputVal:  36864,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "molmo-2-8b:free" },
		},
		"allenai/olmo-2-0325-32b-instruct": {
			IDVal:         "allenai/olmo-2-0325-32b-instruct",
			NameVal:       "AllenAI: Olmo 2 32B Instruct",
			ProviderVal:   "Allenai",
			DescVal:       "OLMo-2 32B Instruct is a supervised instruction-finetuned variant of the OLMo-2 32B March 2025 base model. It excels in complex reasoning and instruction-following tasks across diverse benchmarks such as GSM8K, MATH, IFEval, and general NLP evaluation. Developed by AI2, OLMo-2 32B is part of an open, research-oriented initiative, trained primarily on English-language datasets to advance the understanding and development of open-source language models.",
			DescCNVal:     "OLMo-2 32B Instruct 是 OLMo-2 32B（2025年3月基础模型）的监督指令微调版本，在 GSM8K、MATH、IFEval 等复杂推理与指令遵循基准测试及通用 NLP 评估中表现卓越。该模型由 AI2 开发，属于一项开放、面向研究的计划，主要基于英文数据集训练，旨在推动开源语言模型的理解与发展。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-2-0325-32b-instruct" },
		},
		"allenai/olmo-3-32b-think": {
			IDVal:         "allenai/olmo-3-32b-think",
			NameVal:       "AllenAI: Olmo 3 32B Think",
			ProviderVal:   "Allenai",
			DescVal:       "Olmo 3 32B Think is a large-scale, 32-billion-parameter model purpose-built for deep reasoning, complex logic chains and advanced instruction-following scenarios. Its capacity enables strong performance on demanding evaluation tasks and highly nuanced conversational reasoning. Developed by Ai2 under the Apache 2.0 license, Olmo 3 32B Think embodies the Olmo initiative’s commitment to openness, offering full transparency across weights, code and training methodology.",
			DescCNVal:     "Olmo 3 32B Think 是一款专为深度推理、复杂逻辑链和高级指令遵循场景设计的大规模语言模型，参数量达320亿。其强大的能力使其在高难度评估任务和高度细致的对话推理中表现出色。该模型由艾伦人工智能研究所（AI2）基于 Apache 2.0 许可证开发，体现了 Olmo 项目对开放性的承诺，全面公开了模型权重、代码及训练方法。",
			ContextLenVal: 65536,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-3-32b-think" },
		},
		"allenai/olmo-3-7b-instruct": {
			IDVal:         "allenai/olmo-3-7b-instruct",
			NameVal:       "AllenAI: Olmo 3 7B Instruct",
			ProviderVal:   "Allenai",
			DescVal:       "Olmo 3 7B Instruct is a supervised instruction-fine-tuned variant of the Olmo 3 7B base model, optimized for instruction-following, question-answering, and natural conversational dialogue. By leveraging high-quality instruction data and an open training pipeline, it delivers strong performance across everyday NLP tasks while remaining accessible and easy to integrate. Developed by Ai2 under the Apache 2.0 license, the model offers a transparent, community-friendly option for instruction-driven applications.",
			DescCNVal:     "Olmo 3 7B Instruct 是 Olmo 3 7B 基础模型的监督指令微调版本，专为指令遵循、问答和自然对话交互优化。通过高质量指令数据与开源训练流程，该模型在日常 NLP 任务中表现优异，同时保持易于集成和使用。由 AI2 基于 Apache 2.0 许可证开发，为指令驱动型应用提供透明且社区友好的选择。",
			ContextLenVal: 65536,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-3-7b-instruct" },
		},
		"allenai/olmo-3-7b-think": {
			IDVal:         "allenai/olmo-3-7b-think",
			NameVal:       "AllenAI: Olmo 3 7B Think",
			ProviderVal:   "Allenai",
			DescVal:       "Olmo 3 7B Think is a research-oriented language model in the Olmo family designed for advanced reasoning and instruction-driven tasks. It excels at multi-step problem solving, logical inference, and maintaining coherent conversational context. Developed by Ai2 under the Apache 2.0 license, Olmo 3 7B Think supports transparent, fully open experimentation and provides a lightweight yet capable foundation for academic research and practical NLP workflows.",
			DescCNVal:     "Olmo 3 7B Think 是 Olmo 系列中面向研究的语言模型，专为高级推理和指令驱动任务设计，在多步问题求解、逻辑推理及维持连贯对话上下文方面表现卓越。该模型由 AI2 基于 Apache 2.0 许可证开发，支持完全透明的开放式实验，为学术研究和实用 NLP 工作流提供轻量但功能强大的基础。",
			ContextLenVal: 65536,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-3-7b-think" },
		},
		"allenai/olmo-3.1-32b-instruct": {
			IDVal:         "allenai/olmo-3.1-32b-instruct",
			NameVal:       "AllenAI: Olmo 3.1 32B Instruct",
			ProviderVal:   "Allenai",
			DescVal:       "Olmo 3.1 32B Instruct is a large-scale, 32-billion-parameter instruction-tuned language model engineered for high-performance conversational AI, multi-turn dialogue, and practical instruction following. As part of the Olmo 3.1 family, this variant emphasizes responsiveness to complex user directions and robust chat interactions while retaining strong capabilities on reasoning and coding benchmarks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Instruct reflects the Olmo initiative’s commitment to openness and transparency.",
			DescCNVal:     "Olmo 3.1 32B Instruct 是一款大规模、320亿参数的指令微调语言模型，专为高性能对话式 AI、多轮对话及实用指令遵循而设计。作为 Olmo 3.1 系列的成员，该变体强调对复杂用户指令的响应能力与稳健的聊天交互，同时在推理和编程基准测试中保持强大性能。该模型由 AI2 在 Apache 2.0 许可下开发，体现了 Olmo 计划对开放性与透明度的承诺。",
			ContextLenVal: 65536,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-3.1-32b-instruct" },
		},
		"allenai/olmo-3.1-32b-think": {
			IDVal:         "allenai/olmo-3.1-32b-think",
			NameVal:       "AllenAI: Olmo 3.1 32B Think",
			ProviderVal:   "Allenai",
			DescVal:       "Olmo 3.1 32B Think is a large-scale, 32-billion-parameter model designed for deep reasoning, complex multi-step logic, and advanced instruction following. Building on the Olmo 3 series, version 3.1 delivers refined reasoning behavior and stronger performance across demanding evaluations and nuanced conversational tasks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Think continues the Olmo initiative’s commitment to openness, providing full transparency across model weights, code, and training methodology.",
			DescCNVal:     "Olmo 3.1 32B Think 是一款大规模、320亿参数的模型，专为深度推理、复杂多步逻辑及高级指令遵循而设计。基于 Olmo 3 系列构建，3.1 版本在严苛评估和细致对话任务中展现出更精细的推理行为与更强的性能。该模型由 AI2 在 Apache 2.0 许可下开发，延续了 Olmo 计划对开放性的承诺，全面公开模型权重、代码及训练方法。",
			ContextLenVal: 65536,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "olmo-3.1-32b-think" },
		},
		"alpindale/goliath-120b": {
			IDVal:         "alpindale/goliath-120b",
			NameVal:       "Goliath 120B",
			ProviderVal:   "Alpindale",
			DescVal:       "A large LLM created by combining two fine-tuned Llama 70B models into one 120B model. Combines Xwin and Euryale.\n\nCredits to\n- [@chargoddard](https://huggingface.co/chargoddard) for developing the framework used to merge the model - [mergekit](https://github.com/cg123/mergekit).\n- [@Undi95](https://huggingface.co/Undi95) for helping with the merge ratios.\n\n#merge",
			DescCNVal:     "一款大型语言模型，通过将两个经过微调的 Llama 70B 模型合并为一个 120B 模型构建而成，融合了 Xwin 与 Euryale。\n\n致谢：\n- [@chargoddard](https://huggingface.co/chargoddard) 开发了用于模型合并的框架 [mergekit](https://github.com/cg123/mergekit)。\n- [@Undi95](https://huggingface.co/Undi95) 协助确定了模型合并比例。\n\n#merge",
			ContextLenVal: 6144,
			MaxOutputVal:  1024,
			PriceInVal:    0.000006,
			PriceOutVal:   0.000008,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "goliath-120b" },
		},
		"amazon/nova-2-lite-v1": {
			IDVal:         "amazon/nova-2-lite-v1",
			NameVal:       "Amazon: Nova 2 Lite",
			ProviderVal:   "Amazon",
			DescVal:       "Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads that can process text, images, and videos to generate text. \n\nNova 2 Lite demonstrates standout capabilities in processing documents, extracting information from videos, generating code, providing accurate grounded answers, and automating multi-step agentic workflows.",
			DescCNVal:     "Nova 2 Lite 是一款快速、高性价比的推理模型，适用于日常工作负载，可处理文本、图像和视频以生成文本。\n\nNova 2 Lite 在文档处理、视频信息提取、代码生成、提供准确的事实依据型答案以及自动化多步骤智能体工作流方面表现出色。",
			ContextLenVal: 1000000,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "nova-2-lite-v1" },
		},
		"amazon/nova-lite-v1": {
			IDVal:         "amazon/nova-lite-v1",
			NameVal:       "Amazon: Nova Lite 1.0",
			ProviderVal:   "Amazon",
			DescVal:       "Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.\n\nWith an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.",
			DescCNVal:     "Amazon Nova Lite 1.0 是亚马逊推出的超低成本多模态模型，专注于快速处理图像、视频和文本输入以生成文本输出。该模型可高精度处理实时客户交互、文档分析和视觉问答任务。\n\n凭借 30 万 token 的输入上下文长度，它可在单次输入中分析多张图像或最多 30 分钟的视频。",
			ContextLenVal: 300000,
			MaxOutputVal:  5120,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nova-lite-v1" },
		},
		"amazon/nova-micro-v1": {
			IDVal:         "amazon/nova-micro-v1",
			NameVal:       "Amazon: Nova Micro 1.0",
			ProviderVal:   "Amazon",
			DescVal:       "Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities.",
			DescCNVal:     "Amazon Nova Micro 1.0 是纯文本模型，在 Amazon Nova 系列中提供最低延迟的响应，且成本极低。其上下文长度达 12.8 万 token，针对速度与成本进行了优化，擅长文本摘要、翻译、内容分类、交互式聊天和头脑风暴等任务，并具备基础的数学推理与编码能力。",
			ContextLenVal: 128000,
			MaxOutputVal:  5120,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nova-micro-v1" },
		},
		"amazon/nova-premier-v1": {
			IDVal:         "amazon/nova-premier-v1",
			NameVal:       "Amazon: Nova Premier 1.0",
			ProviderVal:   "Amazon",
			DescVal:       "Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.",
			DescCNVal:     "Amazon Nova Premier 是亚马逊多模态模型中能力最强的版本，专为复杂推理任务设计，同时也是蒸馏定制模型的最佳教师模型。",
			ContextLenVal: 1000000,
			MaxOutputVal:  32000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000013,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nova-premier-v1" },
		},
		"amazon/nova-pro-v1": {
			IDVal:         "amazon/nova-pro-v1",
			NameVal:       "Amazon: Nova Pro 1.0",
			ProviderVal:   "Amazon",
			DescVal:       "Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).\n\nAmazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents.\n\n**NOTE**: Video input is not supported at this time.",
			DescCNVal:     "Amazon Nova Pro 1.0 是亚马逊推出的高性能多模态模型，旨在为广泛任务提供精度、速度与成本的最佳平衡。截至 2024 年 12 月，该模型在关键基准测试中达到业界领先水平，包括视觉问答（TextVQA）和视频理解（VATEX）。\n\nAmazon Nova Pro 在处理视觉与文本信息以及分析金融文档方面展现出强大能力。\n\n**注意**：当前暂不支持视频输入。",
			ContextLenVal: 300000,
			MaxOutputVal:  5120,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nova-pro-v1" },
		},
		"anthracite-org/magnum-v4-72b": {
			IDVal:         "anthracite-org/magnum-v4-72b",
			NameVal:       "Magnum v4 72B",
			ProviderVal:   "Anthracite-Org",
			DescVal:       "This is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet(https://openrouter.ai/anthropic/claude-3.5-sonnet) and Opus(https://openrouter.ai/anthropic/claude-3-opus).\n\nThe model is fine-tuned on top of [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct).",
			DescCNVal:     "该系列模型旨在复现 Claude 3 系列模型（特别是 Sonnet（https://openrouter.ai/anthropic/claude-3.5-sonnet）和 Opus（https://openrouter.ai/anthropic/claude-3-opus））的散文质量。\n\n本模型基于 [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct) 进行微调。",
			ContextLenVal: 16384,
			MaxOutputVal:  2048,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000005,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "magnum-v4-72b" },
		},
		"anthropic/claude-3-haiku": {
			IDVal:         "anthropic/claude-3-haiku",
			NameVal:       "Anthropic: Claude 3 Haiku",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal",
			DescCNVal:     "Claude 3 Haiku 是 Anthropic 推出的最快、最紧凑的模型，可实现近乎即时的响应，兼具快速性与精准的定向性能。\n\n详见发布公告及基准测试结果：[此处](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal",
			ContextLenVal: 200000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-3-haiku" },
		},
		"anthropic/claude-3.5-haiku": {
			IDVal:         "anthropic/claude-3.5-haiku",
			NameVal:       "Anthropic: Claude 3.5 Haiku",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).",
			DescCNVal:     "Claude 3.5 Haiku 在速度、代码准确性和工具使用方面能力显著增强。专为实时应用场景优化，可提供快速响应，适用于聊天交互和即时代码建议等动态任务。\n\n因此，该模型非常适合对速度与精度均有高要求的场景，如软件开发、客户服务机器人和数据管理系统。\n\n当前此模型指向 [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022)。",
			ContextLenVal: 200000,
			MaxOutputVal:  8192,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-3.5-haiku" },
		},
		"anthropic/claude-3.5-sonnet": {
			IDVal:         "anthropic/claude-3.5-sonnet",
			NameVal:       "Anthropic: Claude 3.5 Sonnet",
			ProviderVal:   "Anthropic",
			DescVal:       "New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal",
			DescCNVal:     "全新 Claude 3.5 Sonnet 在性能上超越 Opus，速度优于原有 Sonnet，价格维持 Sonnet 水平。Sonnet 尤其擅长以下领域：\n\n- 编程：在 SWE-Bench Verified 上得分约 49%，高于此前最佳成绩，且无需复杂的提示工程；\n- 数据科学：增强人类数据科学家的专业能力，能结合多种工具从非结构化数据中提取洞见；\n- 视觉处理：擅长解读图表、图形和图像，不仅能准确转录文本，还能从中挖掘超越文本本身的深层信息；\n- 智能体任务：具备卓越的工具调用能力，非常适合执行需与其他系统交互的复杂多步问题求解任务。\n\n#多模态",
			ContextLenVal: 200000,
			MaxOutputVal:  8192,
			PriceInVal:    0.000006,
			PriceOutVal:   0.000030,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-3.5-sonnet" },
		},
		"anthropic/claude-3.7-sonnet": {
			IDVal:         "anthropic/claude-3.7-sonnet",
			NameVal:       "Anthropic: Claude 3.7 Sonnet",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
			DescCNVal:     "Claude 3.7 Sonnet 是一款先进的大语言模型，在推理、编程和问题解决能力方面均有显著提升。该模型引入了混合推理方法，允许用户在快速响应与针对复杂任务的逐步深入处理之间进行选择。其在编程方面表现尤为突出，尤其在前端开发和全栈更新场景中，并在智能体工作流（agentic workflows）中表现出色，能够自主执行多步骤流程。\n\nClaude 3.7 Sonnet 在标准模式下保持与其前代模型相当的性能，同时提供扩展推理模式，以在数学、编程及指令遵循任务中实现更高精度。\n\n更多详情请参阅[此博客文章](https://www.anthropic.com/news/claude-3-7-sonnet)",
			ContextLenVal: 200000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-3.7-sonnet" },
		},
		"anthropic/claude-3.7-sonnet:thinking": {
			IDVal:         "anthropic/claude-3.7-sonnet:thinking",
			NameVal:       "Anthropic: Claude 3.7 Sonnet (thinking)",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
			DescCNVal:     "Claude 3.7 Sonnet 是一款先进的大语言模型，在推理、编程和问题解决能力方面均有显著提升。该模型引入了混合推理方法，允许用户在快速响应与针对复杂任务的逐步深入处理之间进行选择。其在编程方面表现尤为突出，尤其在前端开发和全栈更新场景中，并在智能体工作流（agentic workflows）中表现出色，能够自主执行多步骤流程。\n\nClaude 3.7 Sonnet 在标准模式下保持与其前代模型相当的性能，同时提供扩展推理模式，以在数学、编程及指令遵循任务中实现更高精度。\n\n更多详情请参阅[此博客文章](https://www.anthropic.com/news/claude-3-7-sonnet)",
			ContextLenVal: 200000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-3.7-sonnet:thinking" },
		},
		"anthropic/claude-haiku-4.5": {
			IDVal:         "anthropic/claude-haiku-4.5",
			NameVal:       "Anthropic: Claude Haiku 4.5",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\n\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring >73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.",
			DescCNVal:     "Claude Haiku 4.5 是 Anthropic 推出的最快、最高效的模型，以远低于更大规模 Claude 模型的成本和延迟提供接近前沿水平的智能。其在推理、编码和计算机使用任务上的表现媲美 Claude Sonnet 4，将前沿能力带入实时和高吞吐量应用场景。\n\n该模型首次为 Haiku 系列引入扩展思考能力，支持可控的推理深度、摘要式或交错式思维输出，以及全面支持编码、Bash、网页搜索和计算机使用工具的工具辅助工作流。在 SWE-bench Verified 基准上得分超过 73%，Haiku 4.5 跻身全球顶尖编码模型之列，同时在子代理、并行执行和规模化部署中保持卓越响应速度。",
			ContextLenVal: 200000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000005,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-haiku-4.5" },
		},
		"anthropic/claude-opus-4": {
			IDVal:         "anthropic/claude-opus-4",
			NameVal:       "Anthropic: Claude Opus 4",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)",
			DescCNVal:     "Claude Opus 4 在发布时被公认为全球最强的编程模型，可在复杂、长时间运行的任务和智能体工作流中保持稳定性能。该模型在软件工程领域树立了新标杆，在 SWE-bench（72.5%）和 Terminal-bench（43.2%）上均取得领先成绩。Opus 4 支持扩展型智能体工作流，可连续数小时处理数千个任务步骤而性能不衰减。\n\n[在此处阅读博客文章](https://www.anthropic.com/news/claude-4)",
			ContextLenVal: 200000,
			MaxOutputVal:  32000,
			PriceInVal:    0.000015,
			PriceOutVal:   0.000075,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-opus-4" },
		},
		"anthropic/claude-opus-4.1": {
			IDVal:         "anthropic/claude-opus-4.1",
			NameVal:       "Anthropic: Claude Opus 4.1",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.",
			DescCNVal:     "Claude Opus 4.1 是 Anthropic 旗舰模型的更新版本，在编码、推理和智能体任务方面性能显著提升。该模型在 SWE-bench Verified 上达到 74.5% 的准确率，并在多文件代码重构、调试精度和细节导向推理方面取得显著进步。模型支持最多 64K tokens 的扩展推理，专为研究、数据分析和工具辅助推理等任务优化。",
			ContextLenVal: 200000,
			MaxOutputVal:  32000,
			PriceInVal:    0.000015,
			PriceOutVal:   0.000075,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-opus-4.1" },
		},
		"anthropic/claude-opus-4.5": {
			IDVal:         "anthropic/claude-opus-4.5",
			NameVal:       "Anthropic: Claude Opus 4.5",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.",
			DescCNVal:     "Anthropic 最强大的模型，具备极高的推理能力。",
			ContextLenVal: 200000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000005,
			PriceOutVal:   0.000025,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-opus-4.5", "opus-4.5" },
		},
		"anthropic/claude-sonnet-4": {
			IDVal:         "anthropic/claude-sonnet-4",
			NameVal:       "Anthropic: Claude Sonnet 4",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)",
			DescCNVal:     "Claude Sonnet 4 相较前代 Sonnet 3.7 显著提升，在编程与推理任务中展现出更高的精度与可控性。该模型在 SWE-bench（72.7%）上达到业界领先水平，兼顾强大能力与计算效率，适用于从日常编码到复杂软件开发的广泛场景。关键改进包括更优的自主代码库导航能力、更低的智能体工作流错误率，以及对复杂指令更强的遵循可靠性。Sonnet 4 针对日常实用场景优化，在各类内外部应用中提供先进推理能力的同时，保持高效响应。\n\n[在此处阅读博客文章](https://www.anthropic.com/news/claude-4)",
			ContextLenVal: 1000000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-sonnet-4" },
		},
		"anthropic/claude-sonnet-4.5": {
			IDVal:         "anthropic/claude-sonnet-4.5",
			NameVal:       "Anthropic: Claude Sonnet 4.5",
			ProviderVal:   "Anthropic",
			DescVal:       "Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.",
			DescCNVal:     "Claude Sonnet 4.5 是 Anthropic 迄今最先进的 Sonnet 模型，专为现实世界智能体和编码工作流优化。该模型在 SWE-bench Verified 等编码基准测试中达到业界领先水平，在系统设计、代码安全性和规范遵循方面均有显著提升。其设计支持长时间自主运行，可在会话间保持任务连续性，并提供基于事实的进度追踪。\n\nSonnet 4.5 还增强了智能体能力，包括改进的工具编排、推测性并行执行以及更高效的上下文与内存管理。凭借强化的上下文追踪能力和对工具调用中 token 使用情况的感知，该模型特别适用于多上下文及长时间运行的工作流。典型应用场景涵盖软件工程、网络安全、金融分析、研究智能体及其他需要持续推理与工具调用的领域。",
			ContextLenVal: 1000000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "claude-sonnet-4.5" },
		},
		"arcee-ai/coder-large": {
			IDVal:         "arcee-ai/coder-large",
			NameVal:       "Arcee AI: Coder Large",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Coder‑Large is a 32\u202fB‑parameter offspring of Qwen\u202f2.5‑Instruct that has been further trained on permissively‑licensed GitHub, CodeSearchNet and synthetic bug‑fix corpora. It supports a 32k context window, enabling multi‑file refactoring or long diff review in a single call, and understands 30‑plus programming languages with special attention to TypeScript, Go and Terraform. Internal benchmarks show 5–8\u202fpt gains over CodeLlama‑34\u202fB‑Python on HumanEval and competitive BugFix scores thanks to a reinforcement pass that rewards compilable output. The model emits structured explanations alongside code blocks by default, making it suitable for educational tooling as well as production copilot scenarios. Cost‑wise, Together AI prices it well below proprietary incumbents, so teams can scale interactive coding without runaway spend. ",
			DescCNVal:     "Coder‑Large 是基于 Qwen\u202f2.5‑Instruct 微调的 320 亿参数模型，进一步在采用宽松许可证的 GitHub、CodeSearchNet 及合成缺陷修复语料库上训练而成。该模型支持 32k 上下文窗口，可在单次调用中完成多文件重构或长差异审查，并支持 30 多种编程语言，尤其针对 TypeScript、Go 和 Terraform 进行了优化。内部基准测试表明，得益于强化学习阶段对可编译输出的奖励机制，其在 HumanEval 上比 CodeLlama‑34B‑Python 高出 5–8 分，在 BugFix 任务上表现同样优异。模型默认在代码块旁生成结构化解释，既适用于教育工具，也适用于生产级编程助手场景。在成本方面，Together AI 的定价远低于主流闭源竞品，使团队能在控制支出的同时规模化部署交互式编码功能。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "coder-large" },
		},
		"arcee-ai/maestro-reasoning": {
			IDVal:         "arcee-ai/maestro-reasoning",
			NameVal:       "Arcee AI: Maestro Reasoning",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Maestro Reasoning is Arcee's flagship analysis model: a 32\u202fB‑parameter derivative of Qwen\u202f2.5‑32\u202fB tuned with DPO and chain‑of‑thought RL for step‑by‑step logic. Compared to the earlier 7\u202fB preview, the production 32\u202fB release widens the context window to 128\u202fk tokens and doubles pass‑rate on MATH and GSM‑8K, while also lifting code completion accuracy. Its instruction style encourages structured \"thought → answer\" traces that can be parsed or hidden according to user preference. That transparency pairs well with audit‑focused industries like finance or healthcare where seeing the reasoning path matters. In Arcee Conductor, Maestro is automatically selected for complex, multi‑constraint queries that smaller SLMs bounce. ",
			DescCNVal:     "Maestro Reasoning 是 Arcee 旗舰级分析模型：基于 Qwen 2.5‑32B 构建的 320 亿参数模型，采用 DPO 与思维链强化学习（chain‑of‑thought RL）进行微调，专精于逐步逻辑推理。相比早期 70 亿参数预览版，正式发布的 320 亿参数版本将上下文窗口扩展至 128k tokens，并在 MATH 与 GSM‑8K 基准测试中的通过率翻倍，同时提升了代码补全准确率。其指令风格鼓励生成结构化的“思考 → 答案”轨迹，用户可根据偏好选择解析或隐藏该轨迹。这种透明性特别契合金融、医疗等注重审计的行业，因其需追溯推理路径。在 Arcee Conductor 中，Maestro 会自动用于处理小型 SLM 无法应对的复杂多约束查询。",
			ContextLenVal: 131072,
			MaxOutputVal:  32000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "maestro-reasoning" },
		},
		"arcee-ai/spotlight": {
			IDVal:         "arcee-ai/spotlight",
			NameVal:       "Arcee AI: Spotlight",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Spotlight is a 7‑billion‑parameter vision‑language model derived from Qwen\u202f2.5‑VL and fine‑tuned by Arcee AI for tight image‑text grounding tasks. It offers a 32\u202fk‑token context window, enabling rich multimodal conversations that combine lengthy documents with one or more images. Training emphasized fast inference on consumer GPUs while retaining strong captioning, visual‐question‑answering, and diagram‑analysis accuracy. As a result, Spotlight slots neatly into agent workflows where screenshots, charts or UI mock‑ups need to be interpreted on the fly. Early benchmarks show it matching or out‑scoring larger VLMs such as LLaVA‑1.6 13\u202fB on popular VQA and POPE alignment tests. ",
			DescCNVal:     "Spotlight 是一款 70 亿参数的视觉语言模型，基于 Qwen 2.5‑VL 开发，并由 Arcee AI 针对紧密图文对齐任务进行微调。该模型支持 32k tokens 的上下文窗口，可实现融合长篇文档与单张或多张图像的丰富多模态对话。训练重点在于消费级 GPU 上的快速推理，同时保持强大的图像描述、视觉问答（VQA）及图表分析准确性。因此，Spotlight 能无缝嵌入智能体工作流，实时解读截图、图表或 UI 原型。早期基准测试显示，其在主流 VQA 与 POPE 对齐测试中表现媲美甚至超越 LLaVA‑1.6 13B 等更大规模的视觉语言模型。",
			ContextLenVal: 131072,
			MaxOutputVal:  65537,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "spotlight" },
		},
		"arcee-ai/trinity-mini": {
			IDVal:         "arcee-ai/trinity-mini",
			NameVal:       "Arcee AI: Trinity Mini",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.",
			DescCNVal:     "Trinity Mini 是一款稀疏混合专家（MoE）语言模型，总参数量260亿（每 token 激活约30亿），包含128个专家，每 token 激活其中8个。专为高效处理长上下文（131k tokens）而设计，具备强大的函数调用能力和多步智能体工作流支持。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "trinity-mini" },
		},
		"arcee-ai/trinity-mini:free": {
			IDVal:         "arcee-ai/trinity-mini:free",
			NameVal:       "Arcee AI: Trinity Mini (free)",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.",
			DescCNVal:     "Trinity Mini 是一款稀疏混合专家（MoE）语言模型，总参数量260亿（每 token 激活约30亿），包含128个专家，每 token 激活其中8个。专为高效处理长上下文（131k tokens）而设计，具备强大的函数调用能力和多步智能体工作流支持。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "trinity-mini:free" },
		},
		"arcee-ai/virtuoso-large": {
			IDVal:         "arcee-ai/virtuoso-large",
			NameVal:       "Arcee AI: Virtuoso Large",
			ProviderVal:   "Arcee-Ai",
			DescVal:       "Virtuoso‑Large is Arcee's top‑tier general‑purpose LLM at 72\u202fB parameters, tuned to tackle cross‑domain reasoning, creative writing and enterprise QA. Unlike many 70\u202fB peers, it retains the 128\u202fk context inherited from Qwen\u202f2.5, letting it ingest books, codebases or financial filings wholesale. Training blended DeepSeek\u202fR1 distillation, multi‑epoch supervised fine‑tuning and a final DPO/RLHF alignment stage, yielding strong performance on BIG‑Bench‑Hard, GSM‑8K and long‑context Needle‑In‑Haystack tests. Enterprises use Virtuoso‑Large as the \"fallback\" brain in Conductor pipelines when other SLMs flag low confidence. Despite its size, aggressive KV‑cache optimizations keep first‑token latency in the low‑second range on 8×\u202fH100 nodes, making it a practical production‑grade powerhouse.",
			DescCNVal:     "Virtuoso‑Large 是 Arcee 旗下 720 亿参数的顶级通用大语言模型，专为跨领域推理、创意写作及企业级问答任务优化。不同于多数 700 亿级同类模型，它保留了源自 Qwen 2.5 的 128k 上下文窗口，可一次性处理整本书籍、代码库或财务文件。训练过程融合 DeepSeek R1 蒸馏、多轮监督微调及最终的 DPO/RLHF 对齐阶段，在 BIG‑Bench‑Hard、GSM‑8K 及长上下文“大海捞针”测试中表现卓越。企业常将其作为 Conductor 流水线中的“兜底”智能核心，当其他小型 SLM 置信度不足时自动启用。尽管模型规模庞大，但凭借激进的 KV 缓存优化，在 8× H100 节点上首 token 延迟仍控制在低秒级，是一款实用的生产级高性能模型。",
			ContextLenVal: 131072,
			MaxOutputVal:  64000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "virtuoso-large" },
		},
		"baidu/ernie-4.5-21b-a3b": {
			IDVal:         "baidu/ernie-4.5-21b-a3b",
			NameVal:       "Baidu: ERNIE 4.5 21B A3B",
			ProviderVal:   "Baidu",
			DescVal:       "A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B activated per token, delivering exceptional multimodal understanding and generation through heterogeneous MoE structures and modality-isolated routing. Supporting an extensive 131K token context length, the model achieves efficient inference via multi-expert parallel collaboration and quantization, while advanced post-training techniques including SFT, DPO, and UPO ensure optimized performance across diverse applications with specialized routing and balancing losses for superior task handling.",
			DescCNVal:     "这是一款先进的纯文本混合专家（MoE）模型，总参数量达 210 亿，每 token 激活 30 亿参数，通过异构 MoE 结构与模态隔离路由机制实现卓越的多模态理解与生成能力。模型支持长达 131K token 的上下文，并借助多专家并行协作与量化技术实现高效推理；结合 SFT、DPO 和 UPO 等先进后训练方法，辅以专用路由与均衡损失函数，确保在各类应用场景中均具备优异性能。",
			ContextLenVal: 120000,
			MaxOutputVal:  8000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ernie-4.5-21b-a3b" },
		},
		"baidu/ernie-4.5-21b-a3b-thinking": {
			IDVal:         "baidu/ernie-4.5-21b-a3b-thinking",
			NameVal:       "Baidu: ERNIE 4.5 21B A3B Thinking",
			ProviderVal:   "Baidu",
			DescVal:       "ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to boost reasoning depth and quality for top-tier performance in logical puzzles, math, science, coding, text generation, and expert-level academic benchmarks.",
			DescCNVal:     "ERNIE-4.5-21B-A3B-Thinking 是百度推出的升级版轻量级 MoE 模型，经过优化以提升推理深度与质量，在逻辑谜题、数学、科学、编程、文本生成及专家级学术基准测试中表现卓越。",
			ContextLenVal: 131072,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ernie-4.5-21b-a3b-thinking" },
		},
		"baidu/ernie-4.5-300b-a47b": {
			IDVal:         "baidu/ernie-4.5-300b-a47b",
			NameVal:       "Baidu: ERNIE 4.5 300B A47B ",
			ProviderVal:   "Baidu",
			DescVal:       "ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model developed by Baidu as part of the ERNIE 4.5 series. It activates 47B parameters per token and supports text generation in both English and Chinese. Optimized for high-throughput inference and efficient scaling, it uses a heterogeneous MoE structure with advanced routing and quantization strategies, including FP8 and 2-bit formats. This version is fine-tuned for language-only tasks and supports reasoning, tool parameters, and extended context lengths up to 131k tokens. Suitable for general-purpose LLM applications with high reasoning and throughput demands.",
			DescCNVal:     "ERNIE-4.5-300B-A47B 是百度推出的 ERNIE 4.5 系列中的 3000 亿参数混合专家（MoE）语言模型，每 token 激活 470 亿参数，支持中英文文本生成。该模型采用异构 MoE 架构，结合先进的路由机制与量化策略（包括 FP8 和 2-bit 格式），针对高吞吐推理和高效扩展进行了优化。此版本专为纯语言任务微调，支持推理、工具调用及最长 131k tokens 的上下文长度，适用于对推理能力和吞吐量要求较高的通用大模型应用场景。",
			ContextLenVal: 123000,
			MaxOutputVal:  12000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ernie-4.5-300b-a47b" },
		},
		"baidu/ernie-4.5-vl-28b-a3b": {
			IDVal:         "baidu/ernie-4.5-vl-28b-a3b",
			NameVal:       "Baidu: ERNIE 4.5 VL 28B A3B",
			ProviderVal:   "Baidu",
			DescVal:       "A powerful multimodal Mixture-of-Experts chat model featuring 28B total parameters with 3B activated per token, delivering exceptional text and vision understanding through its innovative heterogeneous MoE structure with modality-isolated routing. Built with scaling-efficient infrastructure for high-throughput training and inference, the model leverages advanced post-training techniques including SFT, DPO, and UPO for optimized performance, while supporting an impressive 131K context length and RLVR alignment for superior cross-modal reasoning and generation capabilities.",
			DescCNVal:     "这是一款强大的多模态混合专家（MoE）对话模型，总参数量达 280 亿，每 token 激活 30 亿参数，依托创新的异构 MoE 架构与模态隔离路由机制，实现卓越的文本与视觉理解能力。模型基于高吞吐训练与推理的高效扩展基础设施构建，采用 SFT、DPO 和 UPO 等先进后训练技术优化性能，支持高达 131K 的上下文长度，并通过 RLVR 对齐机制显著提升跨模态推理与生成能力。",
			ContextLenVal: 30000,
			MaxOutputVal:  8000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ernie-4.5-vl-28b-a3b" },
		},
		"baidu/ernie-4.5-vl-424b-a47b": {
			IDVal:         "baidu/ernie-4.5-vl-424b-a47b",
			NameVal:       "Baidu: ERNIE 4.5 VL 424B A47B ",
			ProviderVal:   "Baidu",
			DescVal:       "ERNIE-4.5-VL-424B-A47B is a multimodal Mixture-of-Experts (MoE) model from Baidu’s ERNIE 4.5 series, featuring 424B total parameters with 47B active per token. It is trained jointly on text and image data using a heterogeneous MoE architecture and modality-isolated routing to enable high-fidelity cross-modal reasoning, image understanding, and long-context generation (up to 131k tokens). Fine-tuned with techniques like SFT, DPO, UPO, and RLVR, this model supports both “thinking” and non-thinking inference modes. Designed for vision-language tasks in English and Chinese, it is optimized for efficient scaling and can operate under 4-bit/8-bit quantization.",
			DescCNVal:     "ERNIE-4.5-VL-424B-A47B 是百度 ERNIE 4.5 系列中的多模态混合专家（MoE）模型，总参数量达 4240 亿，每 token 激活 470 亿参数。该模型基于异构 MoE 架构，采用模态隔离路由机制，在文本与图像数据上联合训练，实现高保真跨模态推理、图像理解及长达 131k tokens 的上下文生成。通过 SFT、DPO、UPO 和 RLVR 等技术微调，支持“思考”与非思考两种推理模式，专为中英文视觉-语言任务设计，并针对高效扩展优化，可在 4-bit/8-bit 量化下运行。",
			ContextLenVal: 123000,
			MaxOutputVal:  16000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ernie-4.5-vl-424b-a47b" },
		},
		"bytedance-seed/seed-1.6": {
			IDVal:         "bytedance-seed/seed-1.6",
			NameVal:       "ByteDance Seed: Seed 1.6",
			ProviderVal:   "Bytedance-Seed",
			DescVal:       "Seed 1.6 is a general-purpose model released by the ByteDance Seed team. It incorporates multimodal capabilities and adaptive deep thinking with a 256K context window.",
			DescCNVal:     "Seed 1.6 是字节跳动 Seed 团队发布的一款通用模型，具备多模态能力与自适应深度思考功能，上下文窗口达 256K。",
			ContextLenVal: 262144,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "seed-1.6" },
		},
		"bytedance-seed/seed-1.6-flash": {
			IDVal:         "bytedance-seed/seed-1.6-flash",
			NameVal:       "ByteDance Seed: Seed 1.6 Flash",
			ProviderVal:   "Bytedance-Seed",
			DescVal:       "Seed 1.6 Flash is an ultra-fast multimodal deep thinking model by ByteDance Seed, supporting both text and visual understanding. It features a 256k context window and can generate outputs of up to 16k tokens.",
			DescCNVal:     "Seed 1.6 Flash 是字节跳动 Seed 团队推出的超高速多模态深度思考模型，支持文本与视觉理解，具备 256K 上下文窗口，并可生成最多 16K 个输出 token。",
			ContextLenVal: 262144,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "seed-1.6-flash" },
		},
		"bytedance/ui-tars-1.5-7b": {
			IDVal:         "bytedance/ui-tars-1.5-7b",
			NameVal:       "ByteDance: UI-TARS 7B ",
			ProviderVal:   "Bytedance",
			DescVal:       "UI-TARS-1.5 is a multimodal vision-language agent optimized for GUI-based environments, including desktop interfaces, web browsers, mobile systems, and games. Built by ByteDance, it builds upon the UI-TARS framework with reinforcement learning-based reasoning, enabling robust action planning and execution across virtual interfaces.\n\nThis model achieves state-of-the-art results on a range of interactive and grounding benchmarks, including OSworld, WebVoyager, AndroidWorld, and ScreenSpot. It also demonstrates perfect task completion across diverse Poki games and outperforms prior models in Minecraft agent tasks. UI-TARS-1.5 supports thought decomposition during inference and shows strong scaling across variants, with the 1.5 version notably exceeding the performance of earlier 72B and 7B checkpoints.",
			DescCNVal:     "UI-TARS-1.5 是一款专为图形用户界面（GUI）环境优化的多模态视觉语言智能体，适用于桌面界面、网页浏览器、移动系统及游戏场景。该模型由字节跳动开发，在 UI-TARS 框架基础上引入基于强化学习的推理机制，可在各类虚拟界面中实现稳健的动作规划与执行。\n\n该模型在多项交互式与具身智能基准测试中达到业界领先水平，包括 OSworld、WebVoyager、AndroidWorld 和 ScreenSpot。此外，它在多种 Poki 游戏中实现了完美任务完成率，并在《我的世界》（Minecraft）智能体任务中显著超越先前模型。UI-TARS-1.5 支持推理过程中的思维分解，并在不同规模版本中展现出强大的性能扩展能力，其中 1.5 版本的性能明显优于早期的 72B 和 7B 检查点。",
			ContextLenVal: 128000,
			MaxOutputVal:  2048,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ui-tars-1.5-7b" },
		},
		"cognitivecomputations/dolphin-mistral-24b-venice-edition:free": {
			IDVal:         "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
			NameVal:       "Venice: Uncensored (free)",
			ProviderVal:   "Cognitivecomputations",
			DescVal:       "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.",
			DescCNVal:     "Venice Uncensored Dolphin Mistral 24B Venice Edition 是 Mistral-Small-24B-Instruct-2501 的微调变体，由 dphn.ai 与 Venice.ai 联合开发。该模型定位为“无审查”的指令微调大语言模型，保留用户对对齐策略、系统提示及行为模式的完全控制权。面向高级且无限制的应用场景，Venice Uncensored 强调可引导性与行为透明性，移除了主流助手模型中常见的默认安全与对齐机制。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "dolphin-mistral-24b-venice-edition:free" },
		},
		"cohere/command-a": {
			IDVal:         "cohere/command-a",
			NameVal:       "Cohere: Command A",
			ProviderVal:   "Cohere",
			DescVal:       "Command A is an open-weights 111B parameter model with a 256k context window focused on delivering great performance across agentic, multilingual, and coding use cases.\nCompared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks.",
			DescCNVal:     "Command A 是一款开源权重的 1110 亿参数模型，支持 256k 上下文窗口，专注于在智能体、多语言和编程等应用场景中提供卓越性能。相较于其他主流闭源及开源模型，Command A 在显著降低硬件成本的同时实现最高性能，尤其擅长处理对业务至关重要的智能体与多语言任务。",
			ContextLenVal: 256000,
			MaxOutputVal:  8192,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "command-a" },
		},
		"cohere/command-r-08-2024": {
			IDVal:         "cohere/command-r-08-2024",
			NameVal:       "Cohere: Command R (08-2024)",
			ProviderVal:   "Cohere",
			DescVal:       "command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
			DescCNVal:     "command-r-08-2024 是 [Command R](/models/cohere/command-r) 的更新版本，在多语言检索增强生成（RAG）和工具使用方面性能更优。总体而言，该模型在数学、代码和推理能力上均有提升，性能可与上一代更大规模的 Command R+ 模型相媲美。\n\n发布详情请参阅[此处](https://docs.cohere.com/changelog/command-gets-refreshed)。\n\n使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。",
			ContextLenVal: 128000,
			MaxOutputVal:  4000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "command-r-08-2024" },
		},
		"cohere/command-r-plus-08-2024": {
			IDVal:         "cohere/command-r-plus-08-2024",
			NameVal:       "Cohere: Command R+ (08-2024)",
			ProviderVal:   "Cohere",
			DescVal:       "command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
			DescCNVal:     "command-r-plus-08-2024 是 [Command R+](/models/cohere/command-r-plus) 的更新版本，相较于前代 Command R+，吞吐量提升约 50%，延迟降低 25%，同时保持相同的硬件资源占用。\n\n发布详情请参阅[此处](https://docs.cohere.com/changelog/command-gets-refreshed)。\n\n使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。",
			ContextLenVal: 128000,
			MaxOutputVal:  4000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "command-r-plus-08-2024" },
		},
		"cohere/command-r7b-12-2024": {
			IDVal:         "cohere/command-r7b-12-2024",
			NameVal:       "Cohere: Command R7B (12-2024)",
			ProviderVal:   "Cohere",
			DescVal:       "Command R7B (12-2024) is a small, fast update of the Command R+ model, delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps.\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).",
			DescCNVal:     "Command R7B（2024 年 12 月版）是 Command R+ 模型的小幅快速更新版本，于 2024 年 12 月发布。该模型在检索增强生成（RAG）、工具调用、智能体等需要复杂推理和多步骤操作的任务中表现卓越。\n\n使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。",
			ContextLenVal: 128000,
			MaxOutputVal:  4000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "command-r7b-12-2024" },
		},
		"deepcogito/cogito-v2-preview-llama-109b-moe": {
			IDVal:         "deepcogito/cogito-v2-preview-llama-109b-moe",
			NameVal:       "Cogito V2 Preview Llama 109B",
			ProviderVal:   "Deepcogito",
			DescVal:       "An instruction-tuned, hybrid-reasoning Mixture-of-Experts model built on Llama-4-Scout-17B-16E. Cogito v2 can answer directly or engage an extended “thinking” phase, with alignment guided by Iterated Distillation & Amplification (IDA). It targets coding, STEM, instruction following, and general helpfulness, with stronger multilingual, tool-calling, and reasoning performance than size-equivalent baselines. The model supports long-context use (up to 10M tokens) and standard Transformers workflows. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "一款基于 Llama-4-Scout-17B-16E 构建的指令微调混合推理专家混合（Mixture-of-Experts）模型。Cogito v2 可直接作答，也可启用扩展的“思考”阶段，其对齐机制由迭代蒸馏与放大（IDA）引导。该模型专注于编程、STEM、指令遵循和通用助理性任务，在多语言能力、工具调用和推理性能方面均优于同等规模的基线模型。支持长上下文使用（最高达 1000 万 tokens）及标准 Transformers 工作流。用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 32767,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "cogito-v2-preview-llama-109b-moe" },
		},
		"deepcogito/cogito-v2-preview-llama-405b": {
			IDVal:         "deepcogito/cogito-v2-preview-llama-405b",
			NameVal:       "Deep Cogito: Cogito V2 Preview Llama 405B",
			ProviderVal:   "Deepcogito",
			DescVal:       "Cogito v2 405B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. It represents a significant step toward frontier intelligence with dense architecture delivering performance competitive with leading closed models. This advanced reasoning system combines policy improvement with massive scale for exceptional capabilities.\n",
			DescCNVal:     "Cogito v2 405B 是一种密集型混合推理模型，兼具直接回答能力与高级自省机制。该模型采用密集架构，在性能上可与领先的闭源模型相媲美，代表了迈向前沿智能的重要一步。这一先进推理系统结合策略优化与超大规模，展现出卓越能力。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "cogito-v2-preview-llama-405b" },
		},
		"deepcogito/cogito-v2-preview-llama-70b": {
			IDVal:         "deepcogito/cogito-v2-preview-llama-70b",
			NameVal:       "Deep Cogito: Cogito V2 Preview Llama 70B",
			ProviderVal:   "Deepcogito",
			DescVal:       "Cogito v2 70B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. Built with iterative policy improvement, it delivers strong performance across reasoning tasks while maintaining efficiency through shorter reasoning chains and improved intuition.",
			DescCNVal:     "Cogito v2 70B 是一款稠密型混合推理模型，兼具直接作答能力与高级自省机制。通过迭代策略优化构建，在保持较短推理链和更强直觉的同时，在各类推理任务中展现出卓越性能。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "cogito-v2-preview-llama-70b" },
		},
		"deepcogito/cogito-v2.1-671b": {
			IDVal:         "deepcogito/cogito-v2.1-671b",
			NameVal:       "Deep Cogito: Cogito v2.1 671B",
			ProviderVal:   "Deepcogito",
			DescVal:       "Cogito v2.1 671B MoE represents one of the strongest open models globally, matching performance of frontier closed and open models. This model is trained using self play with reinforcement learning to reach state-of-the-art performance on multiple categories (instruction following, coding, longer queries and creative writing). This advanced system demonstrates significant progress toward scalable superintelligence through policy improvement.",
			DescCNVal:     "Cogito v2.1 671B MoE 是全球最强的开源模型之一，性能媲美前沿闭源与开源模型。该模型通过自博弈强化学习训练，在指令遵循、编程、长查询及创意写作等多个领域达到业界领先水平，展现了通过策略优化迈向可扩展超级智能的重要进展。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "cogito-v2.1-671b" },
		},
		"deepseek/deepseek-chat": {
			IDVal:         "deepseek/deepseek-chat",
			NameVal:       "DeepSeek: DeepSeek V3",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
			DescCNVal:     "DeepSeek-V3 是 DeepSeek 团队推出的最新模型，在前代版本的指令遵循与编码能力基础上进一步提升。该模型在近 15 万亿 token 上完成预训练，公开评测显示其性能超越其他开源模型，并可媲美主流闭源模型。\n\n有关模型详情，请访问 [DeepSeek-V3 代码仓库](https://github.com/deepseek-ai/DeepSeek-V3) 或参阅[发布公告](https://api-docs.deepseek.com/news/news1226)。",
			ContextLenVal: 163840,
			MaxOutputVal:  163840,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-chat" },
		},
		"deepseek/deepseek-chat-v3-0324": {
			IDVal:         "deepseek/deepseek-chat-v3-0324",
			NameVal:       "DeepSeek: DeepSeek V3 0324",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
			DescCNVal:     "DeepSeek V3 是 DeepSeek 团队最新推出的旗舰级对话模型系列的最新版本，采用混合专家（Mixture-of-Experts）架构，参数量达 6850 亿。\n\n该模型继 [DeepSeek V3](/deepseek/deepseek-chat-v3) 之后推出，在各类任务中均表现出色。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-chat-v3-0324" },
		},
		"deepseek/deepseek-chat-v3.1": {
			IDVal:         "deepseek/deepseek-chat-v3.1",
			NameVal:       "DeepSeek: DeepSeek V3.1",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. \n\nIt succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.",
			DescCNVal:     "DeepSeek-V3.1 是一款大型混合推理模型（总参数 6710 亿，激活参数 370 亿），通过提示模板支持“思考”与“非思考”两种模式。该模型在 DeepSeek-V3 基础上采用两阶段长上下文训练流程，支持最多 128K tokens，并利用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该模型在工具调用、代码生成和推理效率方面均有提升，在高难度基准测试中性能媲美 DeepSeek-R1，同时响应速度更快。支持结构化工具调用、代码代理和搜索代理，适用于科研、编程及智能体工作流。\n\n此模型接替 [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324)，在多种任务上表现优异。",
			ContextLenVal: 32768,
			MaxOutputVal:  7168,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-chat-v3.1" },
		},
		"deepseek/deepseek-r1": {
			IDVal:         "deepseek/deepseek-r1",
			NameVal:       "DeepSeek: R1",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!",
			DescCNVal:     "DeepSeek R1 现已发布：性能媲美 [OpenAI o1](/openai/o1)，但完全开源且推理 token 全部开放。模型总参数量达 6710 亿，单次推理激活 370 亿参数。\n\n完全开源模型及[技术报告](https://api-docs.deepseek.com/news/news250120)。\n\n采用 MIT 许可证：可自由蒸馏与商业化！",
			ContextLenVal: 64000,
			MaxOutputVal:  16000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1" },
		},
		"deepseek/deepseek-r1-0528": {
			IDVal:         "deepseek/deepseek-r1-0528",
			NameVal:       "DeepSeek: R1 0528",
			ProviderVal:   "DeepSeek",
			DescVal:       "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
			DescCNVal:     "5月28日更新版[原始 DeepSeek R1](/deepseek/deepseek-r1)，性能与[OpenAI o1](/openai/o1)相当，但完全开源且推理过程中的所有推理 token 均公开。模型总参数量为6710亿，单次推理激活370亿参数。\n\n完全开源模型。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1-0528" },
		},
		"deepseek/deepseek-r1-0528:free": {
			IDVal:         "deepseek/deepseek-r1-0528:free",
			NameVal:       "DeepSeek: R1 0528 (free)",
			ProviderVal:   "DeepSeek",
			DescVal:       "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
			DescCNVal:     "5月28日更新版[原始 DeepSeek R1](/deepseek/deepseek-r1)，性能与[OpenAI o1](/openai/o1)相当，但完全开源且推理过程中的所有推理 token 均公开。模型总参数量为6710亿，单次推理激活370亿参数。\n\n完全开源模型。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1-0528:free" },
		},
		"deepseek/deepseek-r1-distill-llama-70b": {
			IDVal:         "deepseek/deepseek-r1-distill-llama-70b",
			NameVal:       "DeepSeek: R1 Distill Llama 70B",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
			DescCNVal:     "DeepSeek R1 Distill Llama 70B 是基于 [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct) 并利用 [DeepSeek R1](/deepseek/deepseek-r1) 输出蒸馏而成的大语言模型。该模型融合先进蒸馏技术，在多项基准测试中表现卓越，包括：\n\n- AIME 2024 pass@1：70.0\n- MATH-500 pass@1：94.5\n- CodeForces 评分：1633\n\n通过 DeepSeek R1 输出的微调，该模型实现了可与更大规模前沿模型相媲美的竞争力。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1-distill-llama-70b" },
		},
		"deepseek/deepseek-r1-distill-qwen-32b": {
			IDVal:         "deepseek/deepseek-r1-distill-qwen-32b",
			NameVal:       "DeepSeek: R1 Distill Qwen 32B",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
			DescCNVal:     "DeepSeek R1 Distill Qwen 32B 是基于 [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B) 并利用 [DeepSeek R1](/deepseek/deepseek-r1) 输出蒸馏而成的大语言模型。该模型在多项基准测试中超越 OpenAI o1-mini，为稠密模型树立了新的性能标杆。\n\n其他基准测试结果包括：\n\n- AIME 2024 pass@1：72.6\n- MATH-500 pass@1：94.3\n- CodeForces 评分：1691\n\n通过 DeepSeek R1 输出的微调，该模型实现了可与更大规模前沿模型相媲美的竞争力。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1-distill-qwen-32b" },
		},
		"deepseek/deepseek-v3.1-terminus": {
			IDVal:         "deepseek/deepseek-v3.1-terminus",
			NameVal:       "DeepSeek: DeepSeek V3.1 Terminus",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. ",
			DescCNVal:     "DeepSeek-V3.1 Terminus 是 [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) 的一次更新，在保留模型原有能力的同时，解决了用户反馈的问题（包括语言一致性和智能体能力），并进一步优化了其在代码和搜索智能体方面的性能。该模型是一个大型混合推理模型（总参数 671B，激活参数 37B），支持“思考”与“非思考”两种模式。它在 DeepSeek-V3 基础上通过两阶段长上下文训练流程扩展至最多 128K tokens，并采用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该模型提升了工具使用、代码生成和推理效率，在高难度基准测试中表现媲美 DeepSeek-R1，同时响应速度更快。它支持结构化工具调用、代码智能体和搜索智能体，适用于研究、编码及智能体工作流。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.1-terminus" },
		},
		"deepseek/deepseek-v3.1-terminus:exacto": {
			IDVal:         "deepseek/deepseek-v3.1-terminus:exacto",
			NameVal:       "DeepSeek: DeepSeek V3.1 Terminus (exacto)",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. ",
			DescCNVal:     "DeepSeek-V3.1 Terminus 是 [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) 的一次更新，在保留模型原有能力的同时，解决了用户反馈的问题（包括语言一致性和智能体能力），并进一步优化了其在代码和搜索智能体方面的性能。该模型是一个大型混合推理模型（总参数 671B，激活参数 37B），支持“思考”与“非思考”两种模式。它在 DeepSeek-V3 基础上通过两阶段长上下文训练流程扩展至最多 128K tokens，并采用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该模型提升了工具使用、代码生成和推理效率，在高难度基准测试中表现媲美 DeepSeek-R1，同时响应速度更快。它支持结构化工具调用、代码智能体和搜索智能体，适用于研究、编码及智能体工作流。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.1-terminus:exacto" },
		},
		"deepseek/deepseek-v3.2": {
			IDVal:         "deepseek/deepseek-v3.2",
			NameVal:       "DeepSeek: DeepSeek V3.2",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "DeepSeek-V3.2 是一款在计算效率与强推理及智能体工具使用能力之间取得平衡的大语言模型。它引入了 DeepSeek 稀疏注意力（DSA）机制——一种细粒度稀疏注意力方法，在长上下文场景中显著降低训练与推理成本的同时保持性能。通过可扩展的强化学习后训练框架进一步提升推理能力，据报告其性能达到 GPT-5 级别，并在2025年国际数学奥林匹克（IMO）和国际信息学奥林匹克（IOI）中斩获金牌。V3.2 还采用大规模智能体任务合成流水线，将推理能力更有效地融入工具使用场景，从而提升交互环境中的指令遵循性与泛化能力。\n\n用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.2" },
		},
		"deepseek/deepseek-v3.2-exp": {
			IDVal:         "deepseek/deepseek-v3.2-exp",
			NameVal:       "DeepSeek: DeepSeek V3.2 Exp",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.",
			DescCNVal:     "DeepSeek-V3.2-Exp 是 DeepSeek 发布的实验性大语言模型，作为 V3.1 与未来架构之间的中间版本。该模型引入了 DeepSeek 稀疏注意力（DSA）机制——一种细粒度稀疏注意力机制，旨在长上下文场景下提升训练与推理效率，同时保持输出质量。用户可通过 `reasoning` `enabled` 布尔参数控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该模型在与 V3.1-Terminus 对齐的条件下训练，便于直接对比。基准测试表明，其在推理、编码及智能体工具使用任务上的性能大致与 V3.1 相当，不同领域略有取舍。本次发布重点在于验证面向扩展上下文长度的架构优化，而非提升原始任务准确率，因此主要作为研究导向模型，用于探索高效 Transformer 设计。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.2-exp" },
		},
		"deepseek/deepseek-v3.2-speciale": {
			IDVal:         "deepseek/deepseek-v3.2-speciale",
			NameVal:       "DeepSeek: DeepSeek V3.2 Speciale",
			ProviderVal:   "DeepSeek",
			DescVal:       "DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.",
			DescCNVal:     "DeepSeek-V3.2-Speciale 是 DeepSeek-V3.2 的高性能计算变体，专为极致推理与智能体性能优化。它基于 DeepSeek 稀疏注意力（DSA）实现高效的长上下文处理，并通过更大规模的强化学习后训练进一步超越基础模型的能力。评估结果显示，Speciale 在高难度推理任务上优于 GPT-5，能力接近 Gemini-3.0-Pro，同时保持出色的代码生成与工具使用可靠性。与 V3.2 一样，它也受益于大规模智能体任务合成流水线，显著提升交互环境中的指令遵循性与泛化能力。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.2-speciale" },
		},
		"eleutherai/llemma_7b": {
			IDVal:         "eleutherai/llemma_7b",
			NameVal:       "EleutherAI: Llemma 7b",
			ProviderVal:   "Eleutherai",
			DescVal:       "Llemma 7B is a language model for mathematics. It was initialized with Code Llama 7B weights, and trained on the Proof-Pile-2 for 200B tokens. Llemma models are particularly strong at chain-of-thought mathematical reasoning and using computational tools for mathematics, such as Python and formal theorem provers.",
			DescCNVal:     "Llemma 7B 是一款面向数学领域的语言模型。该模型以 Code Llama 7B 的权重初始化，并在 Proof-Pile-2 数据集上训练了 2000 亿个 token。Llemma 系列模型在数学领域的思维链推理以及使用 Python 和形式化定理证明器等计算工具方面表现尤为突出。",
			ContextLenVal: 4096,
			MaxOutputVal:  4096,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llemma_7b" },
		},
		"essentialai/rnj-1-instruct": {
			IDVal:         "essentialai/rnj-1-instruct",
			NameVal:       "EssentialAI: Rnj 1 Instruct",
			ProviderVal:   "Essentialai",
			DescVal:       "Rnj-1 is an 8B-parameter, dense, open-weight model family developed by Essential AI and trained from scratch with a focus on programming, math, and scientific reasoning. The model demonstrates strong performance across multiple programming languages, tool-use workflows, and agentic execution environments (e.g., mini-SWE-agent). ",
			DescCNVal:     "Rnj-1 是由 Essential AI 开发的 80 亿参数密集型开源权重模型系列，从零开始训练，专注于编程、数学和科学推理。该模型在多种编程语言、工具调用工作流及智能体执行环境（如 mini-SWE-agent）中均展现出强大性能。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "rnj-1-instruct" },
		},
		"google/gemini-2.0-flash-001": {
			IDVal:         "google/gemini-2.0-flash-001",
			NameVal:       "Google: Gemini 2.0 Flash",
			ProviderVal:   "Google",
			DescVal:       "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
			DescCNVal:     "Gemini Flash 2.0 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首令牌延迟（TTFT），同时保持与 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大模型相当的质量。该版本在多模态理解、编码能力、复杂指令遵循和函数调用方面均有显著增强，共同带来更流畅、更稳健的智能体体验。",
			ContextLenVal: 1048576,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.0-flash-001" },
		},
		"google/gemini-2.0-flash-exp:free": {
			IDVal:         "google/gemini-2.0-flash-exp:free",
			NameVal:       "Google: Gemini 2.0 Flash Experimental (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
			DescCNVal:     "Gemini Flash 2.0 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首 token 延迟（TTFT），同时保持与 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大模型相当的质量。该版本在多模态理解、编码能力、复杂指令遵循及函数调用方面均有显著增强，共同带来更流畅、稳健的智能体体验。",
			ContextLenVal: 1048576,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemini-2.0-flash-exp:free" },
		},
		"google/gemini-2.0-flash-lite-001": {
			IDVal:         "google/gemini-2.0-flash-lite-001",
			NameVal:       "Google: Gemini 2.0 Flash Lite",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.",
			DescCNVal:     "Gemini 2.0 Flash Lite 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首 token 延迟（TTFT），同时在输出质量上媲美 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大规模模型，并以极具性价比的 token 价格提供服务。",
			ContextLenVal: 1048576,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.0-flash-lite-001" },
		},
		"google/gemini-2.5-flash": {
			IDVal:         "google/gemini-2.5-flash",
			NameVal:       "Google: Gemini 2.5 Flash",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
			DescCNVal:     "Gemini 2.5 Flash 是 Google 最先进的主力模型，专为高级推理、编程、数学和科学任务设计。其内置“思考”能力，可提供更高准确性和更精细的上下文处理。\n\n此外，Gemini 2.5 Flash 可通过“推理最大 token 数”参数进行配置，详见文档（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-flash" },
		},
		"google/gemini-2.5-flash-image": {
			IDVal:         "google/gemini-2.5-flash-image",
			NameVal:       "Google: Gemini 2.5 Flash Image (Nano Banana)",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)",
			DescCNVal:     "Gemini 2.5 Flash Image（又称“Nano Banana”）现已正式上线。这是一款具备上下文理解能力的前沿图像生成模型，支持图像生成、编辑及多轮对话。可通过 [image_config API 参数](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration) 控制图像宽高比。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000003,
			Features:      CapJsonMode | ModalityImageIn | ModalityImageOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemini-2.5-flash-image" },
		},
		"google/gemini-2.5-flash-lite": {
			IDVal:         "google/gemini-2.5-flash-lite",
			NameVal:       "Google: Gemini 2.5 Flash Lite",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
			DescCNVal:     "Gemini 2.5 Flash-Lite 是 Gemini 2.5 系列中的轻量级推理模型，专为超低延迟与高成本效益而优化。相比早期 Flash 模型，它在吞吐量、令牌生成速度及常见基准测试性能方面均有提升。默认情况下，“思考”功能（即多轮推理）已禁用以优先保障速度，但开发者可通过 [Reasoning API 参数](https://openrouter.ai/docs/use-cases/reasoning-tokens) 启用该功能，在特定场景下以成本换取更高智能水平。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-flash-lite" },
		},
		"google/gemini-2.5-flash-lite-preview-09-2025": {
			IDVal:         "google/gemini-2.5-flash-lite-preview-09-2025",
			NameVal:       "Google: Gemini 2.5 Flash Lite Preview 09-2025",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
			DescCNVal:     "Gemini 2.5 Flash-Lite 是 Gemini 2.5 系列中的轻量级推理模型，专为超低延迟和成本效益优化。相比早期 Flash 模型，它在吞吐量、令牌生成速度及常见基准测试性能方面均有提升。默认禁用“思考”（即多轮推理）以优先保障速度，但开发者可通过 [Reasoning API 参数](https://openrouter.ai/docs/use-cases/reasoning-tokens) 启用该功能，在成本与智能之间进行权衡。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-flash-lite-preview-09-2025" },
		},
		"google/gemini-2.5-flash-preview-09-2025": {
			IDVal:         "google/gemini-2.5-flash-preview-09-2025",
			NameVal:       "Google: Gemini 2.5 Flash Preview 09-2025",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
			DescCNVal:     "Gemini 2.5 Flash Preview（2025 年 9 月检查点）是 Google 最先进的主力模型，专为高级推理、编程、数学和科学任务设计。它内置“思考”能力，可提供更高准确度和更精细的上下文处理。\n\n此外，Gemini 2.5 Flash 可通过“max tokens for reasoning”参数进行配置，详情参见文档（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-flash-preview-09-2025" },
		},
		"google/gemini-2.5-pro": {
			IDVal:         "google/gemini-2.5-pro",
			NameVal:       "Google: Gemini 2.5 Pro",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
			DescCNVal:     "Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学和科学任务设计。该模型具备“思考”能力，能以更高的准确性和更精细的上下文理解生成响应。Gemini 2.5 Pro 在多项基准测试中表现卓越，包括在 LMArena 排行榜上位列第一，体现出优异的人类偏好对齐能力和复杂问题解决能力。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65536,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-pro" },
		},
		"google/gemini-2.5-pro-preview": {
			IDVal:         "google/gemini-2.5-pro-preview",
			NameVal:       "Google: Gemini 2.5 Pro Preview 06-05",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n",
			DescCNVal:     "Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学和科学任务设计。该模型具备“思考”能力，能以更高的准确性和更精细的上下文理解生成响应。Gemini 2.5 Pro 在多项基准测试中表现卓越，包括在 LMArena 排行榜上位列第一，体现出优异的人类偏好对齐能力和复杂问题解决能力。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65536,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemini-2.5-pro-preview" },
		},
		"google/gemini-2.5-pro-preview-05-06": {
			IDVal:         "google/gemini-2.5-pro-preview-05-06",
			NameVal:       "Google: Gemini 2.5 Pro Preview 05-06",
			ProviderVal:   "Google",
			DescVal:       "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
			DescCNVal:     "Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学及科学任务设计。其具备“思考”能力，可通过增强的准确性与细致的上下文处理进行推理。Gemini 2.5 Pro 在多项基准测试中表现顶尖，包括在 LMArena 排行榜上位列第一，体现出卓越的人类偏好对齐能力与复杂问题解决实力。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65535,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-2.5-pro-preview-05-06" },
		},
		"google/gemini-3-flash-preview": {
			IDVal:         "google/gemini-3-flash-preview",
			NameVal:       "Google: Gemini 3 Flash Preview",
			ProviderVal:   "Google",
			DescVal:       "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\n\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models.",
			DescCNVal:     "Gemini 3 Flash Preview 是一款高速、高性价比的思维模型，专为智能体工作流、多轮对话和编程辅助而设计。其推理与工具使用性能接近 Pro 级别，但延迟显著低于更大的 Gemini 变体，非常适合交互式开发、长时间运行的智能体循环及协作编程任务。相比 Gemini 2.5 Flash，它在推理、多模态理解及可靠性方面均有全面提升。\n\n该模型支持 100 万 token 的上下文窗口，可处理包括文本、图像、音频、视频和 PDF 在内的多模态输入，并输出文本。支持通过思维层级（minimal、low、medium、high）配置推理强度、结构化输出、工具调用及自动上下文缓存。Gemini 3 Flash Preview 面向希望获得强大推理与智能体行为，同时避免前沿大模型高昂成本与延迟的用户。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-3-flash-preview" },
		},
		"google/gemini-3-pro-image-preview": {
			IDVal:         "google/gemini-3-pro-image-preview",
			NameVal:       "Google: Nano Banana Pro (Gemini 3 Pro Image Preview)",
			ProviderVal:   "Google",
			DescVal:       "Nano Banana Pro is Google’s most advanced image-generation and editing model, built on Gemini 3 Pro. It extends the original Nano Banana with significantly improved multimodal reasoning, real-world grounding, and high-fidelity visual synthesis. The model generates context-rich graphics, from infographics and diagrams to cinematic composites, and can incorporate real-time information via Search grounding.\n\nIt offers industry-leading text rendering in images (including long passages and multilingual layouts), consistent multi-image blending, and accurate identity preservation across up to five subjects. Nano Banana Pro adds fine-grained creative controls such as localized edits, lighting and focus adjustments, camera transformations, and support for 2K/4K outputs and flexible aspect ratios. It is designed for professional-grade design, product visualization, storyboarding, and complex multi-element compositions while remaining efficient for general image creation workflows.",
			DescCNVal:     "Nano Banana Pro 是谷歌基于 Gemini 3 Pro 构建的最先进图像生成与编辑模型。相比初代 Nano Banana，它在多模态推理、现实世界对齐和高保真视觉合成方面显著提升。该模型可生成富含上下文的图形，涵盖信息图、示意图到电影级合成图像，并可通过搜索对齐整合实时信息。\n\n其在图像中文本渲染（包括长段落与多语言排版）、多图一致性融合以及最多五个主体的身份精准保留方面处于行业领先地位。Nano Banana Pro 新增细粒度创意控制功能，如局部编辑、光照与焦点调节、相机视角变换，并支持 2K/4K 输出及灵活宽高比。该模型专为专业级设计、产品可视化、分镜脚本及复杂多元素构图打造，同时兼顾通用图像创作工作流的高效性。",
			ContextLenVal: 65536,
			MaxOutputVal:  32768,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000012,
			Features:      CapJsonMode | ModalityImageIn | ModalityImageOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemini-3-pro-image-preview" },
		},
		"google/gemini-3-pro-preview": {
			IDVal:         "google/gemini-3-pro-preview",
			NameVal:       "Google: Gemini 3 Pro Preview",
			ProviderVal:   "Google",
			DescVal:       "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.",
			DescCNVal:     "Gemini 3 Pro 是谷歌面向高精度多模态推理的旗舰前沿模型，融合文本、图像、视频、音频和代码领域的强大能力，并支持 100 万 token 的上下文窗口。使用多轮工具调用时必须保留推理细节，详见文档：https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks。该模型在通用推理、STEM 问题求解、事实问答及多模态理解等基准测试中表现卓越，在 LMArena、GPQA Diamond、MathArena Apex、MMMU-Pro 和 Video-MMMU 等评测中均取得领先分数。其交互强调深度与可解释性：模型旨在以最少提示推断用户意图，并输出直接、聚焦洞察的回应。\n\n专为高级开发与智能体工作流构建，Gemini 3 Pro 提供强大的工具调用能力、长周期规划稳定性，以及在复杂 UI、可视化和编程任务中的出色零样本生成能力。它在智能体编程（SWE-Bench Verified、Terminal-Bench 2.0）、多模态分析及结构化长文本任务（如研究综述、规划与交互式学习体验）方面尤为突出。适用场景包括自主智能体、编程助手、多模态分析、科学推理及高上下文信息处理。",
			ContextLenVal: 1048576,
			MaxOutputVal:  65536,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000012,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "gemini-3-pro-preview" },
		},
		"google/gemma-2-27b-it": {
			IDVal:         "google/gemma-2-27b-it",
			NameVal:       "Google: Gemma 2 27B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini).\n\nGemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).",
			DescCNVal:     "Google 推出的 Gemma 2 27B 是一款开源模型，基于与 [Gemini 系列模型](/models?q=gemini) 相同的研究和技术构建。\n\nGemma 模型适用于多种文本生成任务，包括问答、摘要和推理。\n\n更多详情请参阅 [发布公告](https://blog.google/technology/developers/google-gemma-2/)。Gemma 的使用需遵守 Google 的 [Gemma 使用条款](https://ai.google.dev/gemma/terms)。",
			ContextLenVal: 8192,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-2-27b-it" },
		},
		"google/gemma-2-9b-it": {
			IDVal:         "google/gemma-2-9b-it",
			NameVal:       "Google: Gemma 2 9B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.\n\nDesigned for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).",
			DescCNVal:     "Google 推出的 Gemma 2 9B 是一款先进的开源语言模型，在其规模级别中树立了效率与性能的新标杆。\n\n该模型专为广泛任务设计，赋能开发者和研究人员构建创新应用，同时兼顾可访问性、安全性与成本效益。\n\n更多详情请参阅 [发布公告](https://blog.google/technology/developers/google-gemma-2/)。Gemma 的使用需遵守 Google 的 [Gemma 使用条款](https://ai.google.dev/gemma/terms)。",
			ContextLenVal: 8192,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-2-9b-it" },
		},
		"google/gemma-3-12b-it": {
			IDVal:         "google/gemma-3-12b-it",
			NameVal:       "Google: Gemma 3 12B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 12B 是 Gemma 3 系列中仅次于 [Gemma 3 27B](google/gemma-3-27b-it) 的第二大模型。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-12b-it" },
		},
		"google/gemma-3-12b-it:free": {
			IDVal:         "google/gemma-3-12b-it:free",
			NameVal:       "Google: Gemma 3 12B (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 12B 是 Gemma 3 系列中仅次于 [Gemma 3 27B](google/gemma-3-27b-it) 的第二大模型。",
			ContextLenVal: 32768,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-12b-it:free" },
		},
		"google/gemma-3-27b-it": {
			IDVal:         "google/gemma-3-27b-it",
			NameVal:       "Google: Gemma 3 27B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 27B 是 Google 最新推出的开源模型，为 [Gemma 2](google/gemma-2-27b-it) 的继任者。",
			ContextLenVal: 96000,
			MaxOutputVal:  96000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-27b-it" },
		},
		"google/gemma-3-27b-it:free": {
			IDVal:         "google/gemma-3-27b-it:free",
			NameVal:       "Google: Gemma 3 27B (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 27B 是 Google 最新推出的开源模型，为 [Gemma 2](google/gemma-2-27b-it) 的继任者。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-27b-it:free" },
		},
		"google/gemma-3-4b-it": {
			IDVal:         "google/gemma-3-4b-it",
			NameVal:       "Google: Gemma 3 4B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。",
			ContextLenVal: 96000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-4b-it" },
		},
		"google/gemma-3-4b-it:free": {
			IDVal:         "google/gemma-3-4b-it:free",
			NameVal:       "Google: Gemma 3 4B (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.",
			DescCNVal:     "Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。",
			ContextLenVal: 32768,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3-4b-it:free" },
		},
		"google/gemma-3n-e2b-it:free": {
			IDVal:         "google/gemma-3n-e2b-it:free",
			NameVal:       "Google: Gemma 3n 2B (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.",
			DescCNVal:     "Gemma 3n E2B IT 是 Google DeepMind 开发的多模态指令微调模型，基于 60 亿参数架构，有效参数规模约为 20 亿。该模型采用 MatFormer 架构，支持嵌套子模型及通过 Mix-and-Match 框架进行模块化组合。Gemma 3n 系列针对低资源部署优化，提供 32K 上下文长度，在主流基准测试中展现出卓越的多语言能力与推理性能。此变体在包含代码、数学、网页及多模态数据的多样化语料上进行训练。",
			ContextLenVal: 8192,
			MaxOutputVal:  2048,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3n-e2b-it:free" },
		},
		"google/gemma-3n-e4b-it": {
			IDVal:         "google/gemma-3n-e4b-it",
			NameVal:       "Google: Gemma 3n 4B",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\n\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)",
			DescCNVal:     "Gemma 3n E4B-it 针对手机、笔记本电脑和平板等移动及低资源设备的高效执行进行了优化。该模型支持多模态输入——包括文本、视觉数据和音频——可执行文本生成、语音识别、翻译及图像分析等多样化任务。借助逐层嵌入（PLE）缓存和 MatFormer 架构等创新技术，Gemma 3n 能动态管理内存使用与计算负载，通过选择性激活模型参数显著降低运行时资源需求。\n\n该模型支持广泛的语种（训练涵盖 140 多种语言），并具备灵活的 32K tokens 上下文窗口。Gemma 3n 可根据任务或设备能力选择性加载参数，优化内存与计算效率，非常适合注重隐私、支持离线运行的应用及端侧 AI 解决方案。[阅读博客文章了解更多](https://developers.googleblog.com/en/introducing-gemma-3n/)",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3n-e4b-it" },
		},
		"google/gemma-3n-e4b-it:free": {
			IDVal:         "google/gemma-3n-e4b-it:free",
			NameVal:       "Google: Gemma 3n 4B (free)",
			ProviderVal:   "Google",
			DescVal:       "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\n\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)",
			DescCNVal:     "Gemma 3n E4B-it 针对手机、笔记本电脑和平板等移动及低资源设备的高效执行进行了优化。该模型支持多模态输入——包括文本、视觉数据和音频——可执行文本生成、语音识别、翻译及图像分析等多样化任务。借助逐层嵌入（PLE）缓存和 MatFormer 架构等创新技术，Gemma 3n 能动态管理内存使用与计算负载，通过选择性激活模型参数显著降低运行时资源需求。\n\n该模型支持广泛的语种（训练涵盖 140 多种语言），并具备灵活的 32K tokens 上下文窗口。Gemma 3n 可根据任务或设备能力选择性加载参数，优化内存与计算效率，非常适合注重隐私、支持离线运行的应用及端侧 AI 解决方案。[阅读博客文章了解更多](https://developers.googleblog.com/en/introducing-gemma-3n/)",
			ContextLenVal: 8192,
			MaxOutputVal:  2048,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gemma-3n-e4b-it:free" },
		},
		"gryphe/mythomax-l2-13b": {
			IDVal:         "gryphe/mythomax-l2-13b",
			NameVal:       "MythoMax 13B",
			ProviderVal:   "Gryphe",
			DescVal:       "One of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge",
			DescCNVal:     "Llama 2 13B 表现最佳且最受欢迎的微调模型之一，擅长生成丰富描述和角色扮演。#merge",
			ContextLenVal: 4096,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mythomax-l2-13b" },
		},
		"ibm-granite/granite-4.0-h-micro": {
			IDVal:         "ibm-granite/granite-4.0-h-micro",
			NameVal:       "IBM: Granite 4.0 Micro",
			ProviderVal:   "Ibm-Granite",
			DescVal:       "Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These models are the latest in a series of models released by IBM. They are fine-tuned for long context tool calling. ",
			DescCNVal:     "Granite-4.0-H-Micro 是 IBM Granite 4 系列中的一个 30 亿参数模型。该系列是 IBM 最新发布的模型家族，专为长上下文工具调用场景进行了微调。",
			ContextLenVal: 131000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "granite-4.0-h-micro" },
		},
		"inception/mercury": {
			IDVal:         "inception/mercury",
			NameVal:       "Inception: Mercury",
			ProviderVal:   "Inception",
			DescVal:       "Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like GPT-4.1 Nano and Claude 3.5 Haiku while matching their performance. Mercury's speed enables developers to provide responsive user experiences, including with voice agents, search interfaces, and chatbots. Read more in the [blog post]\n(https://www.inceptionlabs.ai/blog/introducing-mercury) here. ",
			DescCNVal:     "Mercury 是全球首款扩散式大语言模型（dLLM）。该模型采用突破性的离散扩散方法，推理速度比 GPT-4.1 Nano 和 Claude 3.5 Haiku 等已优化速度的模型快 5–10 倍，同时性能相当。Mercury 的高速度使开发者能够构建响应迅速的用户体验，适用于语音助手、搜索界面和聊天机器人等场景。更多详情请参阅[博客文章](https://www.inceptionlabs.ai/blog/introducing-mercury)。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mercury" },
		},
		"inception/mercury-coder": {
			IDVal:         "inception/mercury-coder",
			NameVal:       "Inception: Mercury Coder",
			ProviderVal:   "Inception",
			DescVal:       "Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like Claude 3.5 Haiku and GPT-4o Mini while matching their performance. Mercury Coder's speed means that developers can stay in the flow while coding, enjoying rapid chat-based iteration and responsive code completion suggestions. On Copilot Arena, Mercury Coder ranks 1st in speed and ties for 2nd in quality. Read more in the [blog post here](https://www.inceptionlabs.ai/blog/introducing-mercury).",
			DescCNVal:     "Mercury Coder 是全球首款扩散式大语言模型（dLLM）。该模型采用突破性的离散扩散方法，运行速度比 Claude 3.5 Haiku 和 GPT-4o Mini 等已优化速度的模型快 5–10 倍，同时性能相当。其卓越的速度使开发者在编码时能保持流畅状态，享受快速的聊天式迭代和响应迅速的代码补全建议。在 Copilot Arena 中，Mercury Coder 在速度方面排名第一，质量方面并列第二。更多详情请参阅[此博客文章](https://www.inceptionlabs.ai/blog/introducing-mercury)。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mercury-coder" },
		},
		"inflection/inflection-3-pi": {
			IDVal:         "inflection/inflection-3-pi",
			NameVal:       "Inflection: Inflection 3 Pi",
			ProviderVal:   "Inflection",
			DescVal:       "Inflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional intelligence, productivity, and safety. It has access to recent news, and excels in scenarios like customer support and roleplay.\n\nPi has been trained to mirror your tone and style, if you use more emojis, so will Pi! Try experimenting with various prompts and conversation styles.",
			DescCNVal:     "Inflection 3 Pi 为 Inflection 的 [Pi](https://pi.ai) 聊天机器人提供支持，涵盖背景故事、情感智能、生产力和安全性。该模型可访问最新新闻，在客户服务和角色扮演等场景中表现卓越。\n\nPi 经过训练可模仿您的语气和风格——若您使用更多表情符号，Pi 也会如此！不妨尝试各种提示词和对话风格。",
			ContextLenVal: 8000,
			MaxOutputVal:  1024,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "inflection-3-pi" },
		},
		"inflection/inflection-3-productivity": {
			IDVal:         "inflection/inflection-3-productivity",
			NameVal:       "Inflection: Inflection 3 Productivity",
			ProviderVal:   "Inflection",
			DescVal:       "Inflection 3 Productivity is optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines. It has access to recent news.\n\nFor emotional intelligence similar to Pi, see [Inflect 3 Pi](/inflection/inflection-3-pi)\n\nSee [Inflection's announcement](https://inflection.ai/blog/enterprise) for more details.",
			DescCNVal:     "Inflection 3 Productivity 针对指令遵循进行了优化，更适合需要 JSON 输出或严格遵循指定指南的任务。该模型可访问最新新闻。\n\n如需类似 Pi 的情感智能，请参阅 [Inflection 3 Pi](/inflection/inflection-3-pi)。\n\n更多详情请见 [Inflection 官方公告](https://inflection.ai/blog/enterprise)。",
			ContextLenVal: 8000,
			MaxOutputVal:  1024,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "inflection-3-productivity" },
		},
		"kwaipilot/kat-coder-pro": {
			IDVal:         "kwaipilot/kat-coder-pro",
			NameVal:       "Kwaipilot: KAT-Coder-Pro V1",
			ProviderVal:   "Kwaipilot",
			DescVal:       "KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed specifically for agentic coding tasks, it excels in real-world software engineering scenarios, achieving 73.4% solve rate on the SWE-Bench Verified benchmark. \n\nThe model has been optimized for tool-use capability, multi-turn interaction, instruction following, generalization, and comprehensive capabilities through a multi-stage training process, including mid-training, supervised fine-tuning (SFT), reinforcement fine-tuning (RFT), and scalable agentic RL.",
			DescCNVal:     "KAT-Coder-Pro V1 是快手 KwaiKAT 推出的 KAT-Coder 系列中最先进的智能体编程模型，专为智能体编程任务设计，在真实软件工程场景中表现卓越，在 SWE-Bench Verified 基准测试中达到 73.4% 的解决率。\n\n该模型通过多阶段训练流程（包括中期训练、监督微调（SFT）、强化微调（RFT）及可扩展智能体强化学习）优化了工具使用能力、多轮交互、指令遵循、泛化能力及综合性能。",
			ContextLenVal: 256000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kat-coder-pro" },
		},
		"liquid/lfm-2.2-6b": {
			IDVal:         "liquid/lfm-2.2-6b",
			NameVal:       "LiquidAI/LFM2-2.6B",
			ProviderVal:   "Liquid",
			DescVal:       "LFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.",
			DescCNVal:     "LFM2 是 Liquid AI 开发的新一代混合模型，专为边缘 AI 和端侧部署而设计，在质量、速度和内存效率方面树立了新标准。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "lfm-2.2-6b" },
		},
		"liquid/lfm2-8b-a1b": {
			IDVal:         "liquid/lfm2-8b-a1b",
			NameVal:       "LiquidAI/LFM2-8B-A1B",
			ProviderVal:   "Liquid",
			DescVal:       "Model created via inbox interface",
			DescCNVal:     "通过收件箱界面创建的模型",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "lfm2-8b-a1b" },
		},
		"mancer/weaver": {
			IDVal:         "mancer/weaver",
			NameVal:       "Mancer: Weaver (alpha)",
			ProviderVal:   "Mancer",
			DescVal:       "An attempt to recreate Claude-style verbosity, but don't expect the same level of coherence or memory. Meant for use in roleplay/narrative situations.",
			DescCNVal:     "旨在复现 Claude 风格的详尽表达，但连贯性与记忆能力不及原版，适用于角色扮演或叙事场景。",
			ContextLenVal: 8000,
			MaxOutputVal:  2000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "weaver" },
		},
		"meituan/longcat-flash-chat": {
			IDVal:         "meituan/longcat-flash-chat",
			NameVal:       "Meituan: LongCat Flash Chat",
			ProviderVal:   "Meituan",
			DescVal:       "LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of which 18.6B–31.3B (≈27B on average) are dynamically activated per input. It introduces a shortcut-connected MoE design to reduce communication overhead and achieve high throughput while maintaining training stability through advanced scaling strategies such as hyperparameter transfer, deterministic computation, and multi-stage optimization.\n\nThis release, LongCat-Flash-Chat, is a non-thinking foundation model optimized for conversational and agentic tasks. It supports long context windows up to 128K tokens and shows competitive performance across reasoning, coding, instruction following, and domain benchmarks, with particular strengths in tool use and complex multi-step interactions.",
			DescCNVal:     "LongCat-Flash-Chat 是一个大规模混合专家（MoE）模型，总参数量达5600亿，每输入动态激活186亿至313亿参数（平均约270亿）。其创新性地采用捷径连接的MoE架构，有效降低通信开销，在保证训练稳定性的同时实现高吞吐量，这得益于超参迁移、确定性计算和多阶段优化等先进扩展策略。\n\n本次发布的 LongCat-Flash-Chat 是一款非推理型基础模型，专为对话与智能体任务优化。支持最长128K token上下文窗口，在推理、编程、指令遵循及领域基准测试中表现优异，尤其擅长工具调用与复杂多步交互场景。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "longcat-flash-chat" },
		},
		"meta-llama/llama-3-70b-instruct": {
			IDVal:         "meta-llama/llama-3-70b-instruct",
			NameVal:       "Meta: Llama 3 70B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Meta 最新推出的 Llama 3 系列模型包含多种规模与版本。此 70B 指令微调版本专为高质量对话场景优化。\n\n在人工评估中，其表现优于主流闭源模型。\n\n了解更多模型发布信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 8192,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3-70b-instruct" },
		},
		"meta-llama/llama-3-8b-instruct": {
			IDVal:         "meta-llama/llama-3-8b-instruct",
			NameVal:       "Meta: Llama 3 8B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Meta 最新推出的 Llama 3 系列模型包含多种规模与版本。此 8B 指令微调版本专为高质量对话场景优化。\n\n在人工评估中，其表现优于主流闭源模型。\n\n了解更多模型发布信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 8192,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3-8b-instruct" },
		},
		"meta-llama/llama-3.1-405b": {
			IDVal:         "meta-llama/llama-3.1-405b",
			NameVal:       "Meta: Llama 3.1 405B (base)",
			ProviderVal:   "Meta",
			DescVal:       "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This is the base 405B pre-trained version.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此为 405B 参数的基础预训练版本。\n\n在人工评估中，该模型展现出优于主流闭源模型的强劲性能。\n\n欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000004,
			PriceOutVal:   0.000004,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-405b" },
		},
		"meta-llama/llama-3.1-405b-instruct": {
			IDVal:         "meta-llama/llama-3.1-405b-instruct",
			NameVal:       "Meta: Llama 3.1 405B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "万众期待的 Llama 3 百亿级（400B）模型现已发布！支持 128k 上下文长度，并在多项评测中表现卓越，Meta AI 团队持续推动开源大语言模型的前沿边界。\n\nMeta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 405B 指令微调版本专为高质量对话场景优化。\n\n在评测中，其性能显著优于包括 GPT-4o 和 Claude 3.5 Sonnet 在内的主流闭源模型。\n\n欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 10000,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-405b-instruct" },
		},
		"meta-llama/llama-3.1-405b-instruct:free": {
			IDVal:         "meta-llama/llama-3.1-405b-instruct:free",
			NameVal:       "Meta: Llama 3.1 405B Instruct (free)",
			ProviderVal:   "Meta",
			DescVal:       "The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "万众期待的 Llama 3 百亿级（400B）模型现已发布！支持 128k 上下文长度，并在多项评测中表现卓越，Meta AI 团队持续推动开源大语言模型的前沿边界。\n\nMeta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 405B 指令微调版本专为高质量对话场景优化。\n\n在评测中，其性能显著优于包括 GPT-4o 和 Claude 3.5 Sonnet 在内的主流闭源模型。\n\n欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-405b-instruct:free" },
		},
		"meta-llama/llama-3.1-70b-instruct": {
			IDVal:         "meta-llama/llama-3.1-70b-instruct",
			NameVal:       "Meta: Llama 3.1 70B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 70B 指令微调版本专为高质量对话场景优化。\n\n在人工评估中，该模型展现出优于主流闭源模型的强劲性能。\n\n欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-70b-instruct" },
		},
		"meta-llama/llama-3.1-8b-instruct": {
			IDVal:         "meta-llama/llama-3.1-8b-instruct",
			NameVal:       "Meta: Llama 3.1 8B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 8B 指令微调版本兼具速度与效率。\n\n在人工评估中，该模型展现出优于主流闭源模型的强劲性能。\n\n欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 16384,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-8b-instruct" },
		},
		"meta-llama/llama-3.2-11b-vision-instruct": {
			IDVal:         "meta-llama/llama-3.2-11b-vision-instruct",
			NameVal:       "Meta: Llama 3.2 11B Vision Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Llama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
			DescCNVal:     "Llama 3.2 11B Vision 是一款拥有 110 亿参数的多模态模型，专为处理视觉与文本融合任务而设计，在图像描述生成和视觉问答等任务中表现卓越，有效弥合了语言生成与视觉推理之间的鸿沟。该模型在海量图文对数据集上预训练，在复杂高精度图像分析任务中表现出色。\n\n其将视觉理解与语言处理相结合的能力，使其成为内容创作、AI 驱动的客户服务及科研等需要综合视觉-语言 AI 应用领域的理想解决方案。\n\n点击[此处](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)查看原始模型卡。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.2-11b-vision-instruct" },
		},
		"meta-llama/llama-3.2-1b-instruct": {
			IDVal:         "meta-llama/llama-3.2-1b-instruct",
			NameVal:       "Meta: Llama 3.2 1B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Llama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
			DescCNVal:     "Llama 3.2 1B 是一款拥有 10 亿参数的语言模型，专注于高效执行自然语言任务，如摘要生成、对话交互和多语言文本分析。其较小的体量使其能在低资源环境中高效运行，同时保持出色的性能。\n\n该模型支持八种核心语言，并可微调以支持更多语言，是寻求轻量级但功能强大的 AI 解决方案的企业或开发者的理想选择，可在多样化的多语言场景中运行，且无需大型模型所需的高计算资源。\n\n点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。",
			ContextLenVal: 60000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.2-1b-instruct" },
		},
		"meta-llama/llama-3.2-3b-instruct": {
			IDVal:         "meta-llama/llama-3.2-3b-instruct",
			NameVal:       "Meta: Llama 3.2 3B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
			DescCNVal:     "Llama 3.2 3B 是一款拥有 30 亿参数的多语言大语言模型，专为高级自然语言处理任务（如对话生成、推理和摘要）而优化。基于最新的 Transformer 架构，支持包括英语、西班牙语和印地语在内的八种语言，并可适配更多语言。\n\n该模型在 9 万亿 token 上进行训练，在指令遵循、复杂推理和工具调用方面表现出色。其均衡的性能使其成为跨语言文本生成应用的理想选择，兼顾准确性与效率。\n\n点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.2-3b-instruct" },
		},
		"meta-llama/llama-3.2-3b-instruct:free": {
			IDVal:         "meta-llama/llama-3.2-3b-instruct:free",
			NameVal:       "Meta: Llama 3.2 3B Instruct (free)",
			ProviderVal:   "Meta",
			DescVal:       "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
			DescCNVal:     "Llama 3.2 3B 是一款拥有 30 亿参数的多语言大语言模型，专为高级自然语言处理任务（如对话生成、推理和摘要）而优化。基于最新的 Transformer 架构，支持包括英语、西班牙语和印地语在内的八种语言，并可适配更多语言。\n\n该模型在 9 万亿 token 上进行训练，在指令遵循、复杂推理和工具调用方面表现出色。其均衡的性能使其成为跨语言文本生成应用的理想选择，兼顾准确性与效率。\n\n点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.2-3b-instruct:free" },
		},
		"meta-llama/llama-3.3-70b-instruct": {
			IDVal:         "meta-llama/llama-3.3-70b-instruct",
			NameVal:       "Meta: Llama 3.3 70B Instruct",
			ProviderVal:   "Meta",
			DescVal:       "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
			DescCNVal:     "Meta Llama 3.3 多语言大语言模型（LLM）是一款经过预训练和指令微调的 70B 参数生成式模型（纯文本输入/输出）。该模型专为多语言对话场景优化，在主流行业基准测试中性能优于众多现有开源及闭源聊天模型。\n\n支持语言：英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。\n\n[模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.3-70b-instruct" },
		},
		"meta-llama/llama-3.3-70b-instruct:free": {
			IDVal:         "meta-llama/llama-3.3-70b-instruct:free",
			NameVal:       "Meta: Llama 3.3 70B Instruct (free)",
			ProviderVal:   "Meta",
			DescVal:       "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
			DescCNVal:     "Meta Llama 3.3 多语言大语言模型（LLM）是一款经过预训练和指令微调的 70B 参数生成式模型（纯文本输入/输出）。该模型专为多语言对话场景优化，在主流行业基准测试中性能优于众多现有开源及闭源聊天模型。\n\n支持语言：英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。\n\n[模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.3-70b-instruct:free" },
		},
		"meta-llama/llama-4-maverick": {
			IDVal:         "meta-llama/llama-4-maverick",
			NameVal:       "Meta: Llama 4 Maverick",
			ProviderVal:   "Meta",
			DescVal:       "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
			DescCNVal:     "Llama 4 Maverick 17B Instruct（128E）是 Meta 推出的高性能多模态语言模型，基于混合专家（MoE）架构，包含 128 个专家，每次前向传播激活 170 亿参数（总计 4000 亿参数）。该模型支持 12 种语言的多语言文本与图像输入，并生成多语言文本与代码输出。Maverick 针对视觉-语言任务进行了优化，通过指令微调实现类助手行为、图像推理及通用多模态交互。\n\nMaverick 采用早期融合（early fusion）实现原生多模态能力，并支持 100 万 token 的上下文窗口。其训练数据涵盖约 22 万亿 token，包括精选的公开数据、授权数据及 Meta 平台数据，知识截止于 2024 年 8 月。该模型于 2025 年 4 月 5 日依据 Llama 4 社区许可证发布，适用于需要高级多模态理解与高吞吐性能的研究及商业应用。",
			ContextLenVal: 1048576,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-4-maverick" },
		},
		"meta-llama/llama-4-scout": {
			IDVal:         "meta-llama/llama-4-scout",
			NameVal:       "Meta: Llama 4 Scout",
			ProviderVal:   "Meta",
			DescVal:       "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
			DescCNVal:     "Llama 4 Scout 17B Instruct（16E）是 Meta 开发的混合专家（MoE）语言模型，总参数量为 1090 亿，每次前向传播激活 170 亿参数。该模型支持原生多模态输入（文本与图像）及 12 种语言的多语言输出（文本与代码），专为助手式交互与视觉推理设计，每次前向传播使用 16 个专家，上下文长度达 1000 万 token，训练语料规模约为 40 万亿 token。\n\nLlama 4 Scout 采用早期融合技术以实现无缝模态集成，兼顾高效率与本地或商业部署需求。该模型经过指令微调，适用于多语言对话、图像描述生成及图像理解等任务。依据 Llama 4 社区许可证发布，训练数据截止于 2024 年 8 月，并于 2025 年 4 月 5 日公开发布。",
			ContextLenVal: 327680,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-4-scout" },
		},
		"meta-llama/llama-guard-2-8b": {
			IDVal:         "meta-llama/llama-guard-2-8b",
			NameVal:       "Meta: LlamaGuard 2 8B",
			ProviderVal:   "Meta",
			DescVal:       "This safeguard model has 8B parameters and is based on the Llama 3 family. Just like is predecessor, [LlamaGuard 1](https://huggingface.co/meta-llama/LlamaGuard-7b), it can do both prompt and response classification.\n\nLlamaGuard 2 acts as a normal LLM would, generating text that indicates whether the given input/output is safe/unsafe. If deemed unsafe, it will also share the content categories violated.\n\nFor best results, please use raw prompt input or the `/completions` endpoint, instead of the chat API.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "此安全防护模型拥有 80 亿参数，基于 Llama 3 系列构建。与前代 [LlamaGuard 1](https://huggingface.co/meta-llama/LlamaGuard-7b) 类似，它可对提示（prompt）和响应（response）进行分类。\n\nLlamaGuard 2 的工作方式类似于普通大语言模型，会生成文本以判断给定输入/输出是否安全。若判定为不安全，还会指出违反的内容类别。\n\n为获得最佳效果，请使用原始提示输入或 `/completions` 端点，而非聊天 API。\n\n在人工评估中，其表现优于主流闭源模型。\n\n有关模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。本模型的使用须遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 8192,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-guard-2-8b" },
		},
		"meta-llama/llama-guard-3-8b": {
			IDVal:         "meta-llama/llama-guard-3-8b",
			NameVal:       "Llama Guard 3 8B",
			ProviderVal:   "Meta",
			DescVal:       "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n",
			DescCNVal:     "Llama Guard 3 是基于 Llama-3.1-8B 预训练模型微调而成的内容安全分类模型。与早期版本类似，它可用于对大语言模型的输入（提示分类）和输出（响应分类）进行内容安全评估。该模型以文本生成方式输出判断结果，明确指出给定提示或响应是否安全；若不安全，还会列出违反的具体内容类别。\n\nLlama Guard 3 依据 MLCommons 标准化风险分类体系进行对齐，并专为支持 Llama 3.1 的能力而设计。具体而言，它支持 8 种语言的内容审核，并针对搜索及代码解释器工具调用场景的安全性与可靠性进行了优化。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-guard-3-8b" },
		},
		"meta-llama/llama-guard-4-12b": {
			IDVal:         "meta-llama/llama-guard-4-12b",
			NameVal:       "Meta: Llama Guard 4 12B",
			ProviderVal:   "Meta",
			DescVal:       "Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM—generating text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 4 was aligned to safeguard against the standardized MLCommons hazards taxonomy and designed to support multimodal Llama 4 capabilities. Specifically, it combines features from previous Llama Guard models, providing content moderation for English and multiple supported languages, along with enhanced capabilities to handle mixed text-and-image prompts, including multiple images. Additionally, Llama Guard 4 is integrated into the Llama Moderations API, extending robust safety classification to text and images.",
			DescCNVal:     "Llama Guard 4 是基于 Llama 4 Scout 衍生的多模态预训练模型，经微调用于内容安全分类。与早期版本类似，它可用于对大语言模型输入（提示分类）和输出（响应分类）进行安全评估。该模型以文本形式输出判断结果，明确指出给定提示或响应是否安全；若不安全，还会列出违反的具体内容类别。\n\nLlama Guard 4 依据标准化的 MLCommons 危害分类体系进行对齐，并专为支持 Llama 4 的多模态能力而设计。具体而言，它融合了前代 Llama Guard 模型的特性，提供对英语及多种支持语言的内容审核能力，并增强了对混合文本与图像提示（包括多图输入）的处理能力。此外，Llama Guard 4 已集成至 Llama Moderations API，为文本和图像提供强大的安全分类支持。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-guard-4-12b" },
		},
		"microsoft/phi-4": {
			IDVal:         "microsoft/phi-4",
			NameVal:       "Microsoft: Phi 4",
			ProviderVal:   "Microsoft",
			DescVal:       "[Microsoft Research](/microsoft) Phi-4 is designed to perform well in complex reasoning tasks and can operate efficiently in situations with limited memory or where quick responses are needed. \n\nAt 14 billion parameters, it was trained on a mix of high-quality synthetic datasets, data from curated websites, and academic materials. It has undergone careful improvement to follow instructions accurately and maintain strong safety standards. It works best with English language inputs.\n\nFor more information, please see [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)\n",
			DescCNVal:     "[Microsoft Research](/microsoft) Phi-4 专为复杂推理任务设计，可在内存受限或需快速响应的场景中高效运行。\n\n该模型拥有 140 亿参数，训练数据涵盖高质量合成数据集、精选网站内容及学术资料，并经过精细优化以准确遵循指令并维持高标准的安全性。其对英文输入效果最佳。\n\n更多信息请参阅 [Phi-4 技术报告](https://arxiv.org/pdf/2412.08905)",
			ContextLenVal: 16384,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "phi-4" },
		},
		"microsoft/wizardlm-2-8x22b": {
			IDVal:         "microsoft/wizardlm-2-8x22b",
			NameVal:       "WizardLM-2 8x22B",
			ProviderVal:   "Microsoft",
			DescVal:       "WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe",
			DescCNVal:     "WizardLM-2 8x22B 是微软 AI 推出的最先进的 Wizard 系列模型，在性能上可与领先闭源模型高度竞争，并持续超越所有现有开源顶尖模型。\n\n该模型基于 [Mixtral 8x22B](/models/mistralai/mixtral-8x22b) 进行指令微调。\n\n了解更多模型发布信息，请[点击此处](https://wizardlm.github.io/WizardLM2/)。\n\n#moe",
			ContextLenVal: 65536,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "wizardlm-2-8x22b" },
		},
		"minimax/minimax-01": {
			IDVal:         "minimax/minimax-01",
			NameVal:       "MiniMax: MiniMax-01",
			ProviderVal:   "Minimax",
			DescVal:       "MiniMax-01 is a combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding. It has 456 billion parameters, with 45.9 billion parameters activated per inference, and can handle a context of up to 4 million tokens.\n\nThe text model adopts a hybrid architecture that combines Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). The image model adopts the “ViT-MLP-LLM” framework and is trained on top of the text model.\n\nTo read more about the release, see: https://www.minimaxi.com/en/news/minimax-01-series-2",
			DescCNVal:     "MiniMax-01 融合了用于文本生成的 MiniMax-Text-01 与用于图像理解的 MiniMax-VL-01。模型总参数量达 4560 亿，每次推理激活 459 亿参数，支持高达 400 万 token 的上下文长度。\n\n文本模型采用混合架构，结合 Lightning Attention、Softmax Attention 与混合专家（MoE）机制；图像模型采用“ViT-MLP-LLM”框架，并在文本模型基础上进行训练。\n\n更多发布详情请见：https://www.minimaxi.com/en/news/minimax-01-series-2",
			ContextLenVal: 1000192,
			MaxOutputVal:  1000192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "minimax-01" },
		},
		"minimax/minimax-m1": {
			IDVal:         "minimax/minimax-m1",
			NameVal:       "MiniMax: MiniMax M1",
			ProviderVal:   "Minimax",
			DescVal:       "MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom \"lightning attention\" mechanism, allowing it to process long sequences—up to 1 million tokens—while maintaining competitive FLOP efficiency. With 456 billion total parameters and 45.9B active per token, this variant is optimized for complex, multi-step reasoning tasks.\n\nTrained via a custom reinforcement learning pipeline (CISPO), M1 excels in long-context understanding, software engineering, agentic tool use, and mathematical reasoning. Benchmarks show strong performance across FullStackBench, SWE-bench, MATH, GPQA, and TAU-Bench, often outperforming other open models like DeepSeek R1 and Qwen3-235B.",
			DescCNVal:     "MiniMax-M1 是一款大规模开源权重推理模型，专为长上下文和高效率推理而设计。该模型采用混合专家（MoE）架构，并结合自研的“闪电注意力”机制，可处理长达100万 token 的序列，同时保持出色的 FLOP 效率。模型总参数量达4560亿，每 token 激活459亿参数，专为复杂多步推理任务优化。\n\n通过自研强化学习流程（CISPO）训练，M1 在长上下文理解、软件工程、智能体工具调用和数学推理方面表现突出。在 FullStackBench、SWE-bench、MATH、GPQA 和 TAU-Bench 等基准测试中成绩优异，常优于 DeepSeek R1 和 Qwen3-235B 等其他开源模型。",
			ContextLenVal: 1000000,
			MaxOutputVal:  40000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "minimax-m1" },
		},
		"minimax/minimax-m2": {
			IDVal:         "minimax/minimax-m2",
			NameVal:       "MiniMax: MiniMax M2",
			ProviderVal:   "Minimax",
			DescVal:       "MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and agentic workflows. With 10 billion activated parameters (230 billion total), it delivers near-frontier intelligence across general reasoning, tool use, and multi-step task execution while maintaining low latency and deployment efficiency.\n\nThe model excels in code generation, multi-file editing, compile-run-fix loops, and test-validated repair, showing strong results on SWE-Bench Verified, Multi-SWE-Bench, and Terminal-Bench. It also performs competitively in agentic evaluations such as BrowseComp and GAIA, effectively handling long-horizon planning, retrieval, and recovery from execution errors.\n\nBenchmarked by [Artificial Analysis](https://artificialanalysis.ai/models/minimax-m2), MiniMax-M2 ranks among the top open-source models for composite intelligence, spanning mathematics, science, and instruction-following. Its small activation footprint enables fast inference, high concurrency, and improved unit economics, making it well-suited for large-scale agents, developer assistants, and reasoning-driven applications that require responsiveness and cost efficiency.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).",
			DescCNVal:     "MiniMax-M2 是一款紧凑高效的大型语言模型，针对端到端编码和智能体工作流进行了优化。该模型激活参数达 100 亿（总计 2300 亿），在通用推理、工具调用和多步骤任务执行方面提供接近前沿的智能水平，同时保持低延迟和高部署效率。\n\n该模型在代码生成、多文件编辑、编译-运行-修复循环以及测试验证修复等任务上表现卓越，在 SWE-Bench Verified、Multi-SWE-Bench 和 Terminal-Bench 基准测试中取得优异成绩。在 BrowseComp 和 GAIA 等智能体评估中也具有竞争力，能有效处理长周期规划、信息检索及执行错误恢复。\n\n据 [Artificial Analysis](https://artificialanalysis.ai/models/minimax-m2) 测评，MiniMax-M2 在综合智能（涵盖数学、科学和指令遵循）方面位列顶尖开源模型之列。其较小的激活占用空间支持快速推理、高并发处理和更优的单位经济性，非常适合大规模智能体、开发者助手及对响应速度和成本效益有高要求的推理驱动型应用。\n\n为避免性能下降，MiniMax 强烈建议在对话轮次间保留推理过程。更多关于如何使用 reasoning_details 回传推理内容的信息，请参阅我们的 [文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)。",
			ContextLenVal: 196608,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "minimax-m2" },
		},
		"minimax/minimax-m2.1": {
			IDVal:         "minimax/minimax-m2.1",
			NameVal:       "MiniMax: MiniMax M2.1",
			ProviderVal:   "Minimax",
			DescVal:       "MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic workflows, and modern application development. With only 10 billion activated parameters, it delivers a major jump in real-world capability while maintaining exceptional latency, scalability, and cost efficiency.\n\nCompared to its predecessor, M2.1 delivers cleaner, more concise outputs and faster perceived response times. It shows leading multilingual coding performance across major systems and application languages, achieving 49.4% on Multi-SWE-Bench and 72.5% on SWE-Bench Multilingual, and serves as a versatile agent “brain” for IDEs, coding tools, and general-purpose assistance.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks).",
			DescCNVal:     "MiniMax-M2.1 是一款轻量级、业界领先的大型语言模型，针对编程、智能体工作流及现代应用开发进行了优化。仅激活 100 亿参数，即可实现真实场景能力的显著跃升，同时保持卓越的延迟表现、可扩展性与成本效益。\n\n相比前代模型，M2.1 输出更简洁清晰，感知响应速度更快。其在主流系统与应用语言中展现出领先的多语言编程性能，在 Multi-SWE-Bench 上达到 49.4%，在 SWE-Bench Multilingual 上达到 72.5%，可作为 IDE、编程工具及通用助手中的多功能智能体“大脑”。\n\n为避免性能下降，MiniMax 强烈建议在对话轮次间保留推理过程。更多关于如何通过 reasoning_details 传递推理信息的内容，请参阅[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)。",
			ContextLenVal: 196608,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "minimax-m2.1" },
		},
		"mistralai/codestral-2508": {
			IDVal:         "mistralai/codestral-2508",
			NameVal:       "Mistral: Codestral 2508",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test generation.\n\n[Blog Post](https://mistral.ai/news/codestral-25-08)",
			DescCNVal:     "Mistral 于 2025 年 7 月底发布的前沿代码语言模型。Codestral 专为低延迟、高频率任务设计，例如中间填充（FIM）、代码修正和测试生成。\n\n[博客文章](https://mistral.ai/news/codestral-25-08)",
			ContextLenVal: 256000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "codestral-2508" },
		},
		"mistralai/devstral-2512": {
			IDVal:         "mistralai/devstral-2512",
			NameVal:       "Mistral: Devstral 2 2512",
			ProviderVal:   "Mistral",
			DescVal:       "Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\n\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.",
			DescCNVal:     "Devstral 2 是 Mistral AI 推出的前沿开源模型，专精于智能体编程。该模型为 1230 亿参数的稠密 Transformer 架构，支持 256K 上下文窗口。\n\nDevstral 2 能够在保持架构级上下文的前提下探索代码库并协调多文件变更。它可追踪框架依赖关系、检测失败并自动重试修正，有效应对缺陷修复和遗留系统现代化等挑战。该模型支持微调以优先处理特定语言或针对大型企业代码库进行优化，并采用修改版 MIT 许可证发布。",
			ContextLenVal: 262144,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "devstral-2512" },
		},
		"mistralai/devstral-2512:free": {
			IDVal:         "mistralai/devstral-2512:free",
			NameVal:       "Mistral: Devstral 2 2512 (free)",
			ProviderVal:   "Mistral",
			DescVal:       "Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\n\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.",
			DescCNVal:     "Devstral 2 是 Mistral AI 推出的前沿开源模型，专精于智能体编程。该模型为 1230 亿参数的稠密 Transformer 架构，支持 256K 上下文窗口。\n\nDevstral 2 能够在保持架构级上下文的前提下探索代码库并协调多文件变更。它可追踪框架依赖关系、检测失败并自动重试修正，有效应对缺陷修复和遗留系统现代化等挑战。该模型支持微调以优先处理特定语言或针对大型企业代码库进行优化，并采用修改版 MIT 许可证发布。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "devstral-2512:free" },
		},
		"mistralai/devstral-medium": {
			IDVal:         "mistralai/devstral-medium",
			NameVal:       "Mistral: Devstral Medium",
			ProviderVal:   "Mistral",
			DescVal:       "Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SWE-Bench Verified, placing it ahead of Gemini 2.5 Pro and GPT-4.1 in code-related tasks, at a fraction of the cost. It is designed for generalization across prompt styles and tool use in code agents and frameworks.\n\nDevstral Medium is available via API only (not open-weight), and supports enterprise deployment on private infrastructure, with optional fine-tuning capabilities.",
			DescCNVal:     "Devstral Medium 是由 Mistral AI 与 All Hands AI 联合开发的高性能代码生成与智能体推理模型。作为 Devstral Small 的升级版，其在 SWE-Bench Verified 基准上达到 61.6% 的准确率，超越 Gemini 2.5 Pro 与 GPT-4.1 等模型，同时成本显著更低。该模型专为在各类代码智能体与框架中泛化不同提示风格及工具使用而设计。\n\nDevstral Medium 仅通过 API 提供（非开源权重），支持在私有基础设施上进行企业级部署，并可选配微调功能。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "devstral-medium" },
		},
		"mistralai/devstral-small": {
			IDVal:         "mistralai/devstral-small",
			NameVal:       "Mistral: Devstral Small 1.1",
			ProviderVal:   "Mistral",
			DescVal:       "Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats.\n\nDesigned for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes.\n",
			DescCNVal:     "Devstral Small 1.1 是由 Mistral AI 与 All Hands AI 联合开发的 240 亿参数开源语言模型，专为软件工程智能体打造。该模型基于 Mistral Small 3.1 微调而成，采用 Apache 2.0 许可证发布，支持 128K 令牌上下文窗口，并兼容 Mistral 风格的函数调用与 XML 输出格式。\n\n专为智能体编码工作流设计，Devstral Small 1.1 针对代码库探索、多文件编辑及集成至 OpenHands、Cline 等自主开发智能体等任务进行了优化。其在 SWE-Bench Verified 基准上取得 53.6% 的成绩，超越所有其他开源模型，同时足够轻量，可在单张 RTX 4090 GPU 或 Apple Silicon 设备上运行。该模型采用 Tekken 分词器，词表大小达 131K，可通过 vLLM、Transformers、Ollama、LM Studio 及其他兼容 OpenAI 的运行时部署。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "devstral-small" },
		},
		"mistralai/ministral-14b-2512": {
			IDVal:         "mistralai/ministral-14b-2512",
			NameVal:       "Mistral: Ministral 3 14B 2512",
			ProviderVal:   "Mistral",
			DescVal:       "The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and performance comparable to its larger Mistral Small 3.2 24B counterpart. A powerful and efficient language model with vision capabilities.",
			DescCNVal:     "Ministral 3 系列中最大的模型 Ministral 3 14B 具备前沿能力，性能可媲美更大的 Mistral Small 3.2 24B 模型，是一款兼具强大性能与高效性的多模态语言模型。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ministral-14b-2512" },
		},
		"mistralai/ministral-3b": {
			IDVal:         "mistralai/ministral-3b",
			NameVal:       "Mistral: Ministral 3B",
			ProviderVal:   "Mistral",
			DescVal:       "Ministral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, it’s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.",
			DescCNVal:     "Ministral 3B 是一款拥有 30 亿参数的模型，专为设备端和边缘计算优化。它在知识理解、常识推理和函数调用方面表现卓越，在多数基准测试中优于 Mistral 7B 等更大模型。支持高达 128K 的上下文长度，非常适合编排智能体工作流及高效执行专业任务。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ministral-3b" },
		},
		"mistralai/ministral-3b-2512": {
			IDVal:         "mistralai/ministral-3b-2512",
			NameVal:       "Mistral: Ministral 3 3B 2512",
			ProviderVal:   "Mistral",
			DescVal:       "The smallest model in the Ministral 3 family, Ministral 3 3B is a powerful, efficient tiny language model with vision capabilities.",
			DescCNVal:     "Ministral 3 系列中最小的模型 Ministral 3 3B 是一款高效、强大的微型多模态语言模型。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ministral-3b-2512" },
		},
		"mistralai/ministral-8b": {
			IDVal:         "mistralai/ministral-8b",
			NameVal:       "Mistral: Ministral 8B",
			ProviderVal:   "Mistral",
			DescVal:       "Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.",
			DescCNVal:     "Ministral 8B 是一款拥有 80 亿参数的模型，采用独特的交错滑动窗口注意力机制，实现更快、更节省内存的推理。专为边缘应用场景设计，支持高达 128K 的上下文长度，在知识理解和推理任务中表现优异。在 100 亿参数以下模型中性能领先，是低延迟、注重隐私应用的理想之选。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ministral-8b" },
		},
		"mistralai/ministral-8b-2512": {
			IDVal:         "mistralai/ministral-8b-2512",
			NameVal:       "Mistral: Ministral 3 8B 2512",
			ProviderVal:   "Mistral",
			DescVal:       "A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, efficient tiny language model with vision capabilities.",
			DescCNVal:     "Ministral 3 系列中的均衡之选 Ministral 3 8B 是一款高效、强大的微型多模态语言模型。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "ministral-8b-2512" },
		},
		"mistralai/mistral-7b-instruct": {
			IDVal:         "mistralai/mistral-7b-instruct",
			NameVal:       "Mistral: Mistral 7B Instruct",
			ProviderVal:   "Mistral",
			DescVal:       "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*",
			DescCNVal:     "这是一款高性能、业界标准的 73 亿参数模型，针对速度和上下文长度进行了优化。\n\n*Mistral 7B Instruct 包含多个版本变体，此处指最新版本。*",
			ContextLenVal: 32768,
			MaxOutputVal:  4096,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-7b-instruct" },
		},
		"mistralai/mistral-7b-instruct-v0.1": {
			IDVal:         "mistralai/mistral-7b-instruct-v0.1",
			NameVal:       "Mistral: Mistral 7B Instruct v0.1",
			ProviderVal:   "Mistral",
			DescVal:       "A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed and context length.",
			DescCNVal:     "一款拥有 73 亿参数的模型，在所有基准测试中均优于 Llama 2 13B，并针对推理速度和上下文长度进行了优化。",
			ContextLenVal: 2824,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-7b-instruct-v0.1" },
		},
		"mistralai/mistral-7b-instruct-v0.2": {
			IDVal:         "mistralai/mistral-7b-instruct-v0.2",
			NameVal:       "Mistral: Mistral 7B Instruct v0.2",
			ProviderVal:   "Mistral",
			DescVal:       "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\nAn improved version of [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1), with the following changes:\n\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention",
			DescCNVal:     "一款高性能、行业标准的 73 亿参数模型，针对推理速度和上下文长度进行了优化。\n\n这是 [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1) 的改进版本，主要变更包括：\n\n- 上下文窗口扩展至 32k（v0.1 为 8k）\n- Rope-theta = 1e6\n- 移除了滑动窗口注意力机制",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-7b-instruct-v0.2" },
		},
		"mistralai/mistral-7b-instruct-v0.3": {
			IDVal:         "mistralai/mistral-7b-instruct-v0.3",
			NameVal:       "Mistral: Mistral 7B Instruct v0.3",
			ProviderVal:   "Mistral",
			DescVal:       "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\nAn improved version of [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), with the following changes:\n\n- Extended vocabulary to 32768\n- Supports v3 Tokenizer\n- Supports function calling\n\nNOTE: Support for function calling depends on the provider.",
			DescCNVal:     "这是一款高性能、业界标准的 73 亿参数模型，针对速度和上下文长度进行了优化。\n\n这是 [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2) 的改进版本，主要变更包括：\n\n- 词表扩展至 32768\n- 支持 v3 分词器\n- 支持函数调用\n\n注意：函数调用支持取决于具体服务提供商。",
			ContextLenVal: 32768,
			MaxOutputVal:  4096,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-7b-instruct-v0.3" },
		},
		"mistralai/mistral-large": {
			IDVal:         "mistralai/mistral-large",
			NameVal:       "Mistral Large",
			ProviderVal:   "Mistral",
			DescVal:       "This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.",
			DescCNVal:     "Mistral Large 2（版本 `mistral-large-2407`）是 Mistral AI 的旗舰模型。作为一款提供权重的专有模型，它在推理、代码、JSON、聊天等任务上表现出色。发布详情请见[此处](https://mistral.ai/news/mistral-large-2407/)。\n\n支持数十种语言，包括法语、德语、西班牙语、意大利语、葡萄牙语、阿拉伯语、印地语、俄语、中文、日语和韩语，以及 80 多种编程语言，如 Python、Java、C、C++、JavaScript 和 Bash。其超长上下文窗口可从大型文档中精准提取信息。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-large" },
		},
		"mistralai/mistral-large-2407": {
			IDVal:         "mistralai/mistral-large-2407",
			NameVal:       "Mistral Large 2407",
			ProviderVal:   "Mistral",
			DescVal:       "This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.\n",
			DescCNVal:     "这是 Mistral AI 的旗舰模型 Mistral Large 2（版本 mistral-large-2407）。该模型为闭源权重可用模型，在推理、代码、JSON、对话等方面表现卓越。发布详情请参阅[此处](https://mistral.ai/news/mistral-large-2407/)。\n\n支持数十种语言，包括法语、德语、西班牙语、意大利语、葡萄牙语、阿拉伯语、印地语、俄语、中文、日语和韩语，并支持 80 多种编程语言，如 Python、Java、C、C++、JavaScript 和 Bash。其长上下文窗口可精准从大型文档中检索信息。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-large-2407" },
		},
		"mistralai/mistral-large-2411": {
			IDVal:         "mistralai/mistral-large-2411",
			NameVal:       "Mistral Large 2411",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large) released together with [Pixtral Large 2411](/mistralai/pixtral-large-2411)\n\nIt provides a significant upgrade on the previous [Mistral Large 24.07](/mistralai/mistral-large-2407), with notable improvements in long context understanding, a new system prompt, and more accurate function calling.",
			DescCNVal:     "Mistral Large 2 2411 是与 [Pixtral Large 2411](/mistralai/pixtral-large-2411) 同时发布的 [Mistral Large 2](/mistralai/mistral-large) 的更新版本。\n\n相比此前的 [Mistral Large 24.07](/mistralai/mistral-large-2407)，它在长上下文理解、新系统提示以及函数调用准确性方面均有显著提升。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-large-2411" },
		},
		"mistralai/mistral-large-2512": {
			IDVal:         "mistralai/mistral-large-2512",
			NameVal:       "Mistral: Mistral Large 3 2512",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Large 3 2512 is Mistral’s most capable model to date, featuring a sparse mixture-of-experts architecture with 41B active parameters (675B total), and released under the Apache 2.0 license.",
			DescCNVal:     "Mistral Large 3 2512 是 Mistral 迄今为止最强大的模型，采用稀疏混合专家架构，激活参数达 410 亿（总计 6750 亿），并以 Apache 2.0 许可证发布。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-large-2512" },
		},
		"mistralai/mistral-medium-3": {
			IDVal:         "mistralai/mistral-medium-3",
			NameVal:       "Mistral: Mistral Medium 3",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
			DescCNVal:     "Mistral Medium 3 是一款高性能的企业级语言模型，旨在以显著降低的运营成本提供前沿级能力。该模型在尖端推理与多模态性能之间取得平衡，成本仅为传统大模型的八分之一，适用于专业及工业场景的大规模部署。\n\n该模型在编程、STEM 推理及企业适配等领域表现卓越，支持混合、本地及 VPC 内部署，并针对自定义工作流集成进行了优化。Mistral Medium 3 在准确性方面可与 Claude Sonnet 3.5/3.7、Llama 4 Maverick 和 Command R+ 等更大模型竞争，同时在各类云环境中保持广泛的兼容性。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-medium-3" },
		},
		"mistralai/mistral-medium-3.1": {
			IDVal:         "mistralai/mistral-medium-3.1",
			NameVal:       "Mistral: Mistral Medium 3.1",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3.1 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
			DescCNVal:     "Mistral Medium 3.1 是 Mistral Medium 3 的升级版，是一款高性能企业级语言模型，旨在以显著降低的运营成本提供前沿能力。相比传统大模型，其成本降低 8 倍，同时兼顾顶尖的推理与多模态性能，适用于专业及工业场景的大规模部署。\n\n该模型在编程、STEM 推理和企业适配等领域表现卓越，支持混合部署、本地部署及 VPC 内部署，并针对自定义工作流集成进行了优化。在精度方面可与 Claude Sonnet 3.5/3.7、Llama 4 Maverick 和 Command R+ 等更大模型相媲美，同时保持广泛的云环境兼容性。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-medium-3.1" },
		},
		"mistralai/mistral-nemo": {
			IDVal:         "mistralai/mistral-nemo",
			NameVal:       "Mistral: Mistral Nemo",
			ProviderVal:   "Mistral",
			DescVal:       "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.",
			DescCNVal:     "Mistral 与 NVIDIA 联合开发的 120 亿参数模型，支持 128k token 上下文长度。\n\n该模型支持多语言，包括英语、法语、德语、西班牙语、意大利语、葡萄牙语、中文、日语、韩语、阿拉伯语和印地语。\n\n支持函数调用，并以 Apache 2.0 许可证发布。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-nemo" },
		},
		"mistralai/mistral-saba": {
			IDVal:         "mistralai/mistral-saba",
			NameVal:       "Mistral: Saba",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)",
			DescCNVal:     "Mistral Saba 是一款专为中东和南亚地区设计的 240 亿参数语言模型，在保持高效性能的同时提供准确且符合本地语境的响应。该模型基于精选的区域性数据集训练，支持多种印度本土语言（包括泰米尔语和马拉雅拉姆语）以及阿拉伯语，适用于广泛的区域性和多语言应用场景。更多详情请参阅[此博客文章](https://mistral.ai/en/news/mistral-saba)",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-saba" },
		},
		"mistralai/mistral-small-24b-instruct-2501": {
			IDVal:         "mistralai/mistral-small-24b-instruct-2501",
			NameVal:       "Mistral: Mistral Small 3",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.\n\nThe model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)",
			DescCNVal:     "Mistral Small 3 是一款 240 亿参数的语言模型，针对常见 AI 任务优化了低延迟性能。该模型以 Apache 2.0 许可证发布，提供预训练版和指令微调版，适用于高效的本地部署。\n\n模型在 MMLU 基准测试中达到 81% 的准确率，性能可与 Llama 3.3 70B 和 Qwen 32B 等更大模型竞争，且在同等硬件上运行速度提升三倍。[点击此处阅读模型博客文章。](https://mistral.ai/news/mistral-small-3/)",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-small-24b-instruct-2501" },
		},
		"mistralai/mistral-small-3.1-24b-instruct": {
			IDVal:         "mistralai/mistral-small-3.1-24b-instruct",
			NameVal:       "Mistral: Mistral Small 3.1 24B",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)",
			DescCNVal:     "Mistral Small 3.1 24B Instruct 是 Mistral Small 3（2501）的升级版本，拥有 240 亿参数并具备先进的多模态能力。该模型在文本推理与视觉任务（包括图像分析、编程、数学推理及数十种语言的多语言支持）方面达到业界领先水平。配备高达 128K token 的上下文窗口，并针对高效本地推理进行优化，适用于对话代理、函数调用、长文档理解及隐私敏感型部署等场景。其更新版本为 [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-small-3.1-24b-instruct" },
		},
		"mistralai/mistral-small-3.1-24b-instruct:free": {
			IDVal:         "mistralai/mistral-small-3.1-24b-instruct:free",
			NameVal:       "Mistral: Mistral Small 3.1 24B (free)",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)",
			DescCNVal:     "Mistral Small 3.1 24B Instruct 是 Mistral Small 3（2501）的升级版本，拥有 240 亿参数并具备先进的多模态能力。该模型在文本推理与视觉任务（包括图像分析、编程、数学推理及数十种语言的多语言支持）方面达到业界领先水平。配备高达 128K token 的上下文窗口，并针对高效本地推理进行优化，适用于对话代理、函数调用、长文档理解及隐私敏感型部署等场景。其更新版本为 [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-small-3.1-24b-instruct:free" },
		},
		"mistralai/mistral-small-3.2-24b-instruct": {
			IDVal:         "mistralai/mistral-small-3.2-24b-instruct",
			NameVal:       "Mistral: Mistral Small 3.2 24B",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks.\n\nIt supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).",
			DescCNVal:     "Mistral-Small-3.2-24B-Instruct-2506 是 Mistral 推出的更新版 240 亿参数模型，专为指令遵循、减少重复输出及改进函数调用而优化。相比 3.1 版本，3.2 版在 WildBench 和 Arena Hard 基准上显著提升准确性，有效抑制无限生成问题，并在工具使用和结构化输出任务中取得明显进步。\n\n该模型支持图文输入与结构化输出、函数/工具调用，在编程（HumanEval+、MBPP）、STEM（MMLU、MATH、GPQA）及视觉（ChartQA、DocVQA）等基准测试中表现优异。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-small-3.2-24b-instruct" },
		},
		"mistralai/mistral-small-creative": {
			IDVal:         "mistralai/mistral-small-creative",
			NameVal:       "Mistral: Mistral Small Creative",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral Small Creative is an experimental small model designed for creative writing, narrative generation, roleplay and character-driven dialogue, general-purpose instruction following, and conversational agents.",
			DescCNVal:     "Mistral Small Creative 是一款实验性小型模型，专为创意写作、叙事生成、角色扮演与人物驱动对话、通用指令遵循及对话代理而设计。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-small-creative" },
		},
		"mistralai/mistral-tiny": {
			IDVal:         "mistralai/mistral-tiny",
			NameVal:       "Mistral Tiny",
			ProviderVal:   "Mistral",
			DescVal:       "Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/ministral-8b)\n\nThis model is currently powered by Mistral-7B-v0.2, and incorporates a \"better\" fine-tuning than [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.",
			DescCNVal:     "注意：此模型即将弃用。推荐使用更新的 [Ministral 8B](/mistral/ministral-8b)。\n\n当前该模型基于 Mistral-7B-v0.2 构建，并采用了受社区工作启发的“更优”微调策略，相较于 [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1) 有所提升。适用于对成本敏感但对推理能力要求不高的大批量处理任务。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mistral-tiny" },
		},
		"mistralai/mixtral-8x22b-instruct": {
			IDVal:         "mistralai/mixtral-8x22b-instruct",
			NameVal:       "Mistral: Mixtral 8x22B Instruct",
			ProviderVal:   "Mistral",
			DescVal:       "Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/models/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include:\n- strong math, coding, and reasoning\n- large context length (64k)\n- fluency in English, French, Italian, German, and Spanish\n\nSee benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/).\n#moe",
			DescCNVal:     "Mistral 官方发布的 [Mixtral 8x22B](/models/mistralai/mixtral-8x22b) 指令微调版本。该模型在 141B 总参数中激活 39B 参数，以同等规模实现无与伦比的成本效益。其优势包括：\n- 出色的数学、编程与推理能力\n- 超长上下文长度（64k）\n- 流利支持英语、法语、意大利语、德语和西班牙语\n\n基准测试结果详见发布公告：[此处](https://mistral.ai/news/mixtral-8x22b/)。\n#moe",
			ContextLenVal: 65536,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mixtral-8x22b-instruct" },
		},
		"mistralai/mixtral-8x7b-instruct": {
			IDVal:         "mistralai/mixtral-8x7b-instruct",
			NameVal:       "Mistral: Mixtral 8x7B Instruct",
			ProviderVal:   "Mistral",
			DescVal:       "Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat and instruction use. Incorporates 8 experts (feed-forward networks) for a total of 47 billion parameters.\n\nInstruct model fine-tuned by Mistral. #moe",
			DescCNVal:     "Mixtral 8x7B Instruct 是 Mistral AI 开发的预训练生成式稀疏混合专家（Sparse Mixture of Experts）模型，专为聊天和指令遵循场景设计，包含 8 个专家（前馈网络），总参数量达 470 亿。\n\n该指令微调版本由 Mistral 官方提供。#moe",
			ContextLenVal: 32768,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mixtral-8x7b-instruct" },
		},
		"mistralai/pixtral-12b": {
			IDVal:         "mistralai/pixtral-12b",
			NameVal:       "Mistral: Pixtral 12B",
			ProviderVal:   "Mistral",
			DescVal:       "The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torrent: https://x.com/mistralai/status/1833758285167722836.",
			DescCNVal:     "Mistral AI 推出的首款多模态文本+图像到文本模型。其权重已通过种子文件发布：https://x.com/mistralai/status/1833758285167722836。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "pixtral-12b" },
		},
		"mistralai/pixtral-large-2411": {
			IDVal:         "mistralai/pixtral-large-2411",
			NameVal:       "Mistral: Pixtral Large 2411",
			ProviderVal:   "Mistral",
			DescVal:       "Pixtral Large is a 124B parameter, open-weight, multimodal model built on top of [Mistral Large 2](/mistralai/mistral-large-2411). The model is able to understand documents, charts and natural images.\n\nThe model is available under the Mistral Research License (MRL) for research and educational use, and the Mistral Commercial License for experimentation, testing, and production for commercial purposes.\n\n",
			DescCNVal:     "Pixtral Large 是一个拥有 1240 亿参数的开源权重多模态模型，基于 [Mistral Large 2](/mistralai/mistral-large-2411) 构建，能够理解文档、图表和自然图像。\n\n该模型依据 Mistral 研究许可（MRL）可用于研究与教育用途，并可通过 Mistral 商业许可用于商业目的的实验、测试和生产部署。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "pixtral-large-2411" },
		},
		"mistralai/voxtral-small-24b-2507": {
			IDVal:         "mistralai/voxtral-small-24b-2507",
			NameVal:       "Mistral: Voxtral Small 24B 2507",
			ProviderVal:   "Mistral",
			DescVal:       "Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding. Input audio is priced at $100 per million seconds.",
			DescCNVal:     "Voxtral Small 是 Mistral Small 3 的增强版，在保留业界领先的文本性能的同时，集成了先进的音频输入能力，擅长语音转录、翻译和音频理解。音频输入定价为每百万秒 100 美元。",
			ContextLenVal: 32000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "voxtral-small-24b-2507" },
		},
		"moonshotai/kimi-dev-72b": {
			IDVal:         "moonshotai/kimi-dev-72b",
			NameVal:       "MoonshotAI: Kimi Dev 72B",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue resolution tasks. Based on Qwen2.5-72B, it is optimized using large-scale reinforcement learning that applies code patches in real repositories and validates them via full test suite execution—rewarding only correct, robust completions. The model achieves 60.4% on SWE-bench Verified, setting a new benchmark among open-source models for software bug fixing and code reasoning.",
			DescCNVal:     "Kimi-Dev-72B 是一款面向软件工程和问题修复任务微调的开源大语言模型。基于 Qwen2.5-72B，通过大规模强化学习进行优化：在真实代码仓库中应用代码补丁，并通过完整测试套件验证，仅对正确且鲁棒的补全结果给予奖励。该模型在 SWE-bench Verified 上达到60.4%，在开源模型中树立了软件缺陷修复与代码推理的新标杆。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-dev-72b" },
		},
		"moonshotai/kimi-k2": {
			IDVal:         "moonshotai/kimi-k2",
			NameVal:       "MoonshotAI: Kimi K2 0711",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
			DescCNVal:     "Kimi K2 Instruct 是月之暗面（Moonshot AI）开发的大规模混合专家（MoE）语言模型，总参数量达 1 万亿，每次前向传播激活 320 亿参数。该模型针对智能体能力进行优化，支持高级工具调用、复杂推理与代码合成。Kimi K2 在多项基准测试中表现卓越，尤其在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）及工具使用（Tau2、AceBench）任务中优势显著。模型支持最长 128K 令牌的长上下文推理，并采用包含 MuonClip 优化器的新型训练栈，确保大规模 MoE 模型训练的稳定性。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-k2" },
		},
		"moonshotai/kimi-k2-0905": {
			IDVal:         "moonshotai/kimi-k2-0905",
			NameVal:       "MoonshotAI: Kimi K2 0905",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
			DescCNVal:     "Kimi K2 0905 是 [Kimi K2 0711](moonshotai/kimi-k2) 的九月更新版本，由月之暗面（Moonshot AI）开发的大规模专家混合（MoE）语言模型，总参数量达 1 万亿，每次前向传递激活 320 亿参数。支持最长 256k tokens 的长上下文推理（此前为 128k）。\n\n本次更新提升了智能体编程的准确率与跨框架泛化能力，并增强了前端编程在 Web、3D 等相关任务中的输出美观性与功能性。Kimi K2 针对智能体能力进行了优化，包括高级工具使用、推理与代码合成，在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）和工具使用（Tau2、AceBench）等基准测试中表现优异。该模型采用包含 MuonClip 优化器的新训练栈，以实现稳定的大规模 MoE 训练。",
			ContextLenVal: 262144,
			MaxOutputVal:  262144,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-k2-0905" },
		},
		"moonshotai/kimi-k2-0905:exacto": {
			IDVal:         "moonshotai/kimi-k2-0905:exacto",
			NameVal:       "MoonshotAI: Kimi K2 0905 (exacto)",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\n\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
			DescCNVal:     "Kimi K2 0905 是 [Kimi K2 0711](moonshotai/kimi-k2) 的九月更新版本，由月之暗面（Moonshot AI）开发的大规模专家混合（MoE）语言模型，总参数量达 1 万亿，每次前向传递激活 320 亿参数。支持最长 256k tokens 的长上下文推理（此前为 128k）。\n\n本次更新提升了智能体编程的准确率与跨框架泛化能力，并增强了前端编程在 Web、3D 等相关任务中的输出美观性与功能性。Kimi K2 针对智能体能力进行了优化，包括高级工具使用、推理与代码合成，在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）和工具使用（Tau2、AceBench）等基准测试中表现优异。该模型采用包含 MuonClip 优化器的新训练栈，以实现稳定的大规模 MoE 训练。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-k2-0905:exacto" },
		},
		"moonshotai/kimi-k2-thinking": {
			IDVal:         "moonshotai/kimi-k2-thinking",
			NameVal:       "MoonshotAI: Kimi K2 Thinking",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 series into agentic, long-horizon reasoning. Built on the trillion-parameter Mixture-of-Experts (MoE) architecture introduced in Kimi K2, it activates 32 billion parameters per forward pass and supports 256 k-token context windows. The model is optimized for persistent step-by-step thought, dynamic tool invocation, and complex reasoning workflows that span hundreds of turns. It interleaves step-by-step reasoning with tool use, enabling autonomous research, coding, and writing that can persist for hundreds of sequential actions without drift.\n\nIt sets new open-source benchmarks on HLE, BrowseComp, SWE-Multilingual, and LiveCodeBench, while maintaining stable multi-agent behavior through 200–300 tool calls. Built on a large-scale MoE architecture with MuonClip optimization, it combines strong reasoning depth with high inference efficiency for demanding agentic and analytical tasks.",
			DescCNVal:     "Kimi K2 Thinking 是月之暗面（Moonshot AI）迄今最先进的开源推理模型，将 K2 系列拓展至智能体驱动的长周期推理领域。该模型基于 Kimi K2 引入的万亿参数混合专家（MoE）架构，每次前向传递激活 320 亿参数，支持 256k token 的上下文窗口。模型针对持续逐步推理、动态工具调用及跨越数百轮次的复杂推理工作流进行了优化，能将逐步推理与工具使用交错执行，实现无需漂移的数百步连续自主研究、编程与写作。\n\n该模型在 HLE、BrowseComp、SWE-Multilingual 和 LiveCodeBench 等开源基准上创下新纪录，并能在 200–300 次工具调用中保持稳定的多智能体行为。依托大规模 MoE 架构与 MuonClip 优化，它在高推理深度与高推理效率之间取得平衡，适用于高要求的智能体与分析任务。",
			ContextLenVal: 262144,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-k2-thinking" },
		},
		"moonshotai/kimi-k2:free": {
			IDVal:         "moonshotai/kimi-k2:free",
			NameVal:       "MoonshotAI: Kimi K2 0711 (free)",
			ProviderVal:   "Moonshotai",
			DescVal:       "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.",
			DescCNVal:     "Kimi K2 Instruct 是月之暗面（Moonshot AI）开发的大规模混合专家（MoE）语言模型，总参数量达 1 万亿，每次前向传播激活 320 亿参数。该模型针对智能体能力进行优化，支持高级工具调用、复杂推理与代码合成。Kimi K2 在多项基准测试中表现卓越，尤其在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）及工具使用（Tau2、AceBench）任务中优势显著。模型支持最长 128K 令牌的长上下文推理，并采用包含 MuonClip 优化器的新型训练栈，确保大规模 MoE 模型训练的稳定性。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "kimi-k2:free" },
		},
		"morph/morph-v3-fast": {
			IDVal:         "morph/morph-v3-fast",
			NameVal:       "Morph: Morph V3 Fast",
			ProviderVal:   "Morph",
			DescVal:       "Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy for rapid code transformations.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)",
			DescCNVal:     "Morph 最快的代码编辑应用模型，推理速度约 10,500 tokens/秒，准确率达 96%，适用于快速代码转换。\n\n该模型要求提示格式如下：\n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nMorph 已启用零数据留存策略。更多模型信息请参阅其[文档](https://docs.morphllm.com/quickstart)。",
			ContextLenVal: 81920,
			MaxOutputVal:  38000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "morph-v3-fast" },
		},
		"morph/morph-v3-large": {
			IDVal:         "morph/morph-v3-large",
			NameVal:       "Morph: Morph V3 Large",
			ProviderVal:   "Morph",
			DescVal:       "Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with 98% accuracy for precise code transformations.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Morph. Learn more about this model in their [documentation](https://docs.morphllm.com/quickstart)",
			DescCNVal:     "Morph 高精度代码编辑应用模型，适用于复杂代码修改，推理速度约 4,500 tokens/秒，准确率达 98%，可实现精准代码转换。\n\n该模型要求提示格式如下：\n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nMorph 已启用零数据留存策略。更多模型信息请参阅其[文档](https://docs.morphllm.com/quickstart)。",
			ContextLenVal: 262144,
			MaxOutputVal:  131072,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "morph-v3-large" },
		},
		"neversleep/llama-3.1-lumimaid-8b": {
			IDVal:         "neversleep/llama-3.1-lumimaid-8b",
			NameVal:       "NeverSleep: Lumimaid v0.2 8B",
			ProviderVal:   "Neversleep",
			DescVal:       "Lumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).",
			DescCNVal:     "Lumimaid v0.2 8B 是基于 [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) 微调的模型，相比 Lumimaid v0.1 在数据集方面实现了“巨大飞跃”，并清理了低质量的聊天输出。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-lumimaid-8b" },
		},
		"neversleep/noromaid-20b": {
			IDVal:         "neversleep/noromaid-20b",
			NameVal:       "Noromaid 20B",
			ProviderVal:   "Neversleep",
			DescVal:       "A collab between IkariDev and Undi. This merge is suitable for RP, ERP, and general knowledge.\n\n#merge #uncensored",
			DescCNVal:     "由 IkariDev 与 Undi 联合开发的合并模型，适用于角色扮演（RP）、成人角色扮演（ERP）及通用知识任务。\n\n#merge #uncensored",
			ContextLenVal: 4096,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "noromaid-20b" },
		},
		"nex-agi/deepseek-v3.1-nex-n1": {
			IDVal:         "nex-agi/deepseek-v3.1-nex-n1",
			NameVal:       "Nex AGI: DeepSeek V3.1 Nex N1",
			ProviderVal:   "Nex-Agi",
			DescVal:       "DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series — a post-trained model designed to highlight agent autonomy, tool use, and real-world productivity. \n\nNex-N1 demonstrates competitive performance across all evaluation scenarios, showing particularly strong results in practical coding and HTML generation tasks.",
			DescCNVal:     "DeepSeek V3.1 Nex-N1 是 Nex-N1 系列的旗舰版本——一款经过后训练的模型，旨在突出智能体自主性、工具使用能力及现实生产力。\n\nNex-N1 在所有评估场景中均展现出竞争力，在实际编码和 HTML 生成任务中表现尤为突出。",
			ContextLenVal: 131072,
			MaxOutputVal:  163840,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-v3.1-nex-n1" },
		},
		"nousresearch/deephermes-3-mistral-24b-preview": {
			IDVal:         "nousresearch/deephermes-3-mistral-24b-preview",
			NameVal:       "Nous: DeepHermes 3 Mistral 24B Preview",
			ProviderVal:   "Nous Research",
			DescVal:       "DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.\n\nDeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *\"deep thinking\"* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer. \n\nSystem Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n",
			DescCNVal:     "DeepHermes 3（Mistral 24B 预览版）是由 Nous Research 基于 Mistral-Small-24B 开发的指令微调语言模型，专为聊天、函数调用及高级多轮推理设计。该模型引入双模式系统，可通过特殊系统提示在直观聊天响应与结构化“深度推理”模式间切换。经 R1 蒸馏微调，支持结构化输出（JSON 模式）及面向智能体应用的函数调用语法。\n\nDeepHermes 3 支持**通过系统提示启用推理切换**，允许用户在快速直观响应与深思熟虑的多步推理之间自由转换。当使用以下特定系统指令激活时，模型将进入*“深度思考”*模式——在给出最终答案前，生成包含在 `<think></think>` 标签内的长链思维过程。\n\n系统提示：你是一个深度思考型 AI，可使用极长的思维链深入分析问题，并通过系统化推理过程与自身反复推演，以得出正确解答。请将你的思考与内心独白置于 <think> </think> 标签内，随后提供问题的解决方案或回答。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deephermes-3-mistral-24b-preview" },
		},
		"nousresearch/hermes-2-pro-llama-3-8b": {
			IDVal:         "nousresearch/hermes-2-pro-llama-3-8b",
			NameVal:       "NousResearch: Hermes 2 Pro - Llama-3 8B",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
			DescCNVal:     "Hermes 2 Pro 是 Nous Hermes 2 的升级再训练版本，采用更新并清洗后的 OpenHermes 2.5 数据集，并新增了内部开发的函数调用与 JSON 模式数据集。",
			ContextLenVal: 8192,
			MaxOutputVal:  2048,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-2-pro-llama-3-8b" },
		},
		"nousresearch/hermes-3-llama-3.1-405b": {
			IDVal:         "nousresearch/hermes-3-llama-3.1-405b",
			NameVal:       "Nous: Hermes 3 405B Instruct",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.",
			DescCNVal:     "Hermes 3 是一款通用语言模型，在 Hermes 2 基础上实现了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理能力、多轮对话连贯性、长上下文一致性，以及全方位性能提升。\n\nHermes 3 405B 是基于 Llama-3.1 405B 基础模型进行全参数微调的前沿级别模型，专注于将大语言模型与用户意图对齐，赋予终端用户强大的引导与控制能力。\n\nHermes 3 系列在 Hermes 2 能力集基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手功能，以及增强的代码生成技能。\n\n在通用能力方面，Hermes 3 与 Llama-3.1 Instruct 模型相比具备竞争力，甚至更优，二者各有优势与不足。",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-3-llama-3.1-405b" },
		},
		"nousresearch/hermes-3-llama-3.1-405b:free": {
			IDVal:         "nousresearch/hermes-3-llama-3.1-405b:free",
			NameVal:       "Nous: Hermes 3 405B Instruct (free)",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.",
			DescCNVal:     "Hermes 3 是一款通用语言模型，在 Hermes 2 基础上进行了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理、多轮对话、长上下文连贯性，以及全方位性能提升。\n\nHermes 3 405B 是基于 Llama-3.1 405B 基础模型进行的全参数微调前沿级模型，专注于将大语言模型与用户需求对齐，赋予终端用户强大的引导能力和控制权。\n\nHermes 3 系列在 Hermes 2 能力基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手能力，以及改进的代码生成技能。\n\n在通用能力方面，Hermes 3 与 Llama-3.1 Instruct 模型相比具有竞争力，甚至更优，两者各有优势与不足。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-3-llama-3.1-405b:free" },
		},
		"nousresearch/hermes-3-llama-3.1-70b": {
			IDVal:         "nousresearch/hermes-3-llama-3.1-70b",
			NameVal:       "Nous: Hermes 3 70B Instruct",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/models/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.",
			DescCNVal:     "Hermes 3 是一款通用语言模型，在 [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo) 基础上进行了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理、多轮对话、长上下文连贯性，以及全方位性能提升。\n\nHermes 3 70B 是对 [Llama-3.1 70B 基础模型](/models/meta-llama/llama-3.1-70b-instruct) 进行的具有竞争力（甚至更优）的微调版本，专注于将大语言模型与用户需求对齐，赋予终端用户强大的引导能力和控制权。\n\nHermes 3 系列在 Hermes 2 能力基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手能力，以及改进的代码生成技能。",
			ContextLenVal: 65536,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-3-llama-3.1-70b" },
		},
		"nousresearch/hermes-4-405b": {
			IDVal:         "nousresearch/hermes-4-405b",
			NameVal:       "Nous: Hermes 4 405B",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and released by Nous Research. It introduces a hybrid reasoning mode, where the model can choose to deliberate internally with <think>...</think> traces or respond directly, offering flexibility between speed and depth. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model is instruction-tuned with an expanded post-training corpus (~60B tokens) emphasizing reasoning traces, improving performance in math, code, STEM, and logical reasoning, while retaining broad assistant utility. It also supports structured outputs, including JSON mode, schema adherence, function calling, and tool use. Hermes 4 is trained for steerability, lower refusal rates, and alignment toward neutral, user-directed behavior.",
			DescCNVal:     "Hermes 4 是由 Nous Research 基于 Meta-Llama-3.1-405B 构建的大规模推理模型，引入了混合推理模式：模型可选择通过 <think>...</think> 标记进行内部推理，或直接响应，从而在速度与深度之间灵活权衡。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该模型经过指令微调，使用了约 600 亿 token 的扩展后训练语料，重点强化推理轨迹，在数学、代码、STEM 及逻辑推理方面性能显著提升，同时保留广泛的助手功能。此外，支持结构化输出（包括 JSON 模式、模式遵循、函数调用和工具使用）。Hermes 4 经过专门训练，具备更强的可控性、更低的拒答率，并对齐至中立、用户导向的行为模式。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-4-405b" },
		},
		"nousresearch/hermes-4-70b": {
			IDVal:         "nousresearch/hermes-4-70b",
			NameVal:       "Nous: Hermes 4 70B",
			ProviderVal:   "Nous Research",
			DescVal:       "Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama-3.1-70B. It introduces the same hybrid mode as the larger 405B release, allowing the model to either respond directly or generate explicit <think>...</think> reasoning traces before answering. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThis 70B variant is trained with the expanded post-training corpus (~60B tokens) emphasizing verified reasoning data, leading to improvements in mathematics, coding, STEM, logic, and structured outputs while maintaining general assistant performance. It supports JSON mode, schema adherence, function calling, and tool use, and is designed for greater steerability with reduced refusal rates.",
			DescCNVal:     "Hermes 4 70B 是 Nous Research 基于 Meta-Llama-3.1-70B 开发的混合推理模型，引入了与更大规模 405B 版本相同的混合模式，允许模型直接响应或在回答前生成显式的 <think>...</think> 推理轨迹。用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\n该 70B 版本使用扩展后的后训练语料库（约 600 亿 tokens）进行训练，重点强化经验证的推理数据，从而在数学、编程、STEM、逻辑和结构化输出方面取得显著提升，同时保持通用助手性能。支持 JSON 模式、模式遵循、函数调用和工具使用，具备更强的可控性并降低拒答率。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hermes-4-70b" },
		},
		"nvidia/llama-3.1-nemotron-70b-instruct": {
			IDVal:         "nvidia/llama-3.1-nemotron-70b-instruct",
			NameVal:       "NVIDIA: Llama 3.1 Nemotron 70B Instruct",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).",
			DescCNVal:     "NVIDIA 的 Llama 3.1 Nemotron 70B 是一款旨在生成精准且实用回复的语言模型。基于 [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) 架构并结合人类反馈强化学习（RLHF），在自动对齐基准测试中表现卓越。该模型专为需要高准确性的帮助性和回复生成任务而设计，适用于跨领域的多样化用户查询。\n\n使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-nemotron-70b-instruct" },
		},
		"nvidia/llama-3.1-nemotron-ultra-253b-v1": {
			IDVal:         "nvidia/llama-3.1-nemotron-ultra-253b-v1",
			NameVal:       "NVIDIA: Llama 3.1 Nemotron Ultra 253B v1",
			ProviderVal:   "Nvidia",
			DescVal:       "Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta’s Llama-3.1-405B-Instruct, it has been significantly customized using Neural Architecture Search (NAS), resulting in enhanced efficiency, reduced memory usage, and improved inference latency. The model supports a context length of up to 128K tokens and can operate efficiently on an 8x NVIDIA H100 node.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
			DescCNVal:     "Llama-3.1-Nemotron-Ultra-253B-v1 是一款专为高级推理、人机交互对话、检索增强生成（RAG）及工具调用任务优化的大语言模型（LLM）。该模型基于 Meta 的 Llama-3.1-405B-Instruct，通过神经架构搜索（NAS）进行了深度定制，显著提升了效率、降低了内存占用并改善了推理延迟。模型支持最高 128K token 的上下文长度，并可在 8 块 NVIDIA H100 GPU 节点上高效运行。\n\n注意：必须在系统提示中包含 `detailed thinking on` 才能启用推理功能。更多详情请参阅 [使用建议](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations)。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.1-nemotron-ultra-253b-v1" },
		},
		"nvidia/llama-3.3-nemotron-super-49b-v1.5": {
			IDVal:         "nvidia/llama-3.3-nemotron-super-49b-v1.5",
			NameVal:       "NVIDIA: Llama 3.3 Nemotron Super 49B V1.5",
			ProviderVal:   "Nvidia",
			DescVal:       "Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived from Meta’s Llama-3.3-70B-Instruct with a 128K context. It’s post-trained for agentic workflows (RAG, tool calling) via SFT across math, code, science, and multi-turn chat, followed by multiple RL stages; Reward-aware Preference Optimization (RPO) for alignment, RL with Verifiable Rewards (RLVR) for step-wise reasoning, and iterative DPO to refine tool-use behavior. A distillation-driven Neural Architecture Search (“Puzzle”) replaces some attention blocks and varies FFN widths to shrink memory footprint and improve throughput, enabling single-GPU (H100/H200) deployment while preserving instruction following and CoT quality.\n\nIn internal evaluations (NeMo-Skills, up to 16 runs, temp = 0.6, top_p = 0.95), the model reports strong reasoning/coding results, e.g., MATH500 pass@1 = 97.4, AIME-2024 = 87.5, AIME-2025 = 82.71, GPQA = 71.97, LiveCodeBench (24.10–25.02) = 73.58, and MMLU-Pro (CoT) = 79.53. The model targets practical inference efficiency (high tokens/s, reduced VRAM) with Transformers/vLLM support and explicit “reasoning on/off” modes (chat-first defaults, greedy recommended when disabled). Suitable for building agents, assistants, and long-context retrieval systems where balanced accuracy-to-cost and reliable tool use matter.\n",
			DescCNVal:     "Llama-3.3-Nemotron-Super-49B-v1.5 是一个以英语为中心的 490 亿参数推理/对话模型，基于 Meta 的 Llama-3.3-70B-Instruct 并支持 128K 上下文。该模型通过监督微调（SFT）针对智能体工作流（RAG、工具调用）在数学、代码、科学及多轮对话任务上进行后训练，随后经历多阶段强化学习：基于奖励感知的偏好优化（RPO）用于对齐，基于可验证奖励的强化学习（RLVR）用于逐步推理，以及迭代式 DPO 以优化工具使用行为。通过蒸馏驱动的神经架构搜索（“Puzzle”）替换部分注意力模块并调整前馈网络宽度，显著降低内存占用并提升吞吐量，从而支持单 GPU（H100/H200）部署，同时保留指令遵循能力和思维链（CoT）质量。\n\n在内部评估中（NeMo-Skills，最多 16 次运行，温度=0.6，top_p=0.95），该模型展现出强大的推理与编码能力，例如 MATH500 pass@1 = 97.4、AIME-2024 = 87.5、AIME-2025 = 82.71、GPQA = 71.97、LiveCodeBench（24.10–25.02）= 73.58 以及 MMLU-Pro（CoT）= 79.53。该模型面向实际推理效率（高 tokens/s、低显存占用），支持 Transformers/vLLM，并提供显式的“推理开/关”模式（默认聊天优先，关闭时推荐使用贪心解码）。适用于构建对准确率与成本平衡性及可靠工具使用有要求的智能体、助手和长上下文检索系统。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "llama-3.3-nemotron-super-49b-v1.5" },
		},
		"nvidia/nemotron-3-nano-30b-a3b": {
			IDVal:         "nvidia/nemotron-3-nano-30b-a3b",
			NameVal:       "NVIDIA: Nemotron 3 Nano 30B A3B",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\n\nThe model is fully open with open-weights, datasets and recipes so developers can easily\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\nsecurity.\n\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.",
			DescCNVal:     "NVIDIA Nemotron 3 Nano 30B A3B 是一款面向开发者的小型语言专家混合（MoE）模型，在计算效率与准确性方面表现卓越，适用于构建专用智能体 AI 系统。\n\n该模型完全开源，提供开放权重、数据集及训练方案，便于开发者轻松定制、优化并在自有基础设施上部署，以实现最高级别的隐私与安全。\n\n注意：免费端点的所有提示与输出均会被记录，用于改进提供商的模型及其产品与服务。请勿上传任何个人、机密或其他敏感信息。此为试用版本，不得用于生产环境或关键业务系统。",
			ContextLenVal: 262144,
			MaxOutputVal:  262144,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nemotron-3-nano-30b-a3b" },
		},
		"nvidia/nemotron-3-nano-30b-a3b:free": {
			IDVal:         "nvidia/nemotron-3-nano-30b-a3b:free",
			NameVal:       "NVIDIA: Nemotron 3 Nano 30B A3B (free)",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\n\nThe model is fully open with open-weights, datasets and recipes so developers can easily\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\nsecurity.\n\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.",
			DescCNVal:     "NVIDIA Nemotron 3 Nano 30B A3B 是一款面向开发者的小型语言专家混合（MoE）模型，在计算效率与准确性方面表现卓越，适用于构建专用智能体 AI 系统。\n\n该模型完全开源，提供开放权重、数据集及训练方案，便于开发者轻松定制、优化并在自有基础设施上部署，以实现最高级别的隐私与安全。\n\n注意：免费端点的所有提示与输出均会被记录，用于改进提供商的模型及其产品与服务。请勿上传任何个人、机密或其他敏感信息。此为试用版本，不得用于生产环境或关键业务系统。",
			ContextLenVal: 256000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nemotron-3-nano-30b-a3b:free" },
		},
		"nvidia/nemotron-nano-12b-v2-vl": {
			IDVal:         "nvidia/nemotron-nano-12b-v2-vl",
			NameVal:       "NVIDIA: Nemotron Nano 12B 2 VL",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\n\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\n\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.",
			DescCNVal:     "NVIDIA Nemotron Nano 2 VL 是一款拥有 120 亿参数的开源多模态推理模型，专为视频理解和文档智能设计。该模型采用混合 Transformer-Mamba 架构，结合了 Transformer 级别的准确性与 Mamba 的内存高效序列建模能力，显著提升吞吐量并降低延迟。\n\n该模型支持文本和多图像文档输入，并生成自然语言输出。其训练数据为 NVIDIA 精心构建的高质量合成数据集，针对光学字符识别（OCR）、图表推理和多模态理解进行了优化。\n\nNemotron Nano 2 VL 在 OCRBench v2 上取得领先成果，并在 MMMU、MathVista、AI2D、OCRBench、OCR-Reasoning、ChartQA、DocVQA 和 Video-MME 等基准测试中平均得分约 74，超越此前所有开源视觉语言基线模型。借助高效视频采样（EVS）技术，该模型可处理长视频内容，同时降低推理成本。\n\n模型权重、训练数据和微调方案均以宽松的 NVIDIA 开源许可证发布，并支持在 NeMo、NIM 及主流推理运行时环境中部署。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "nemotron-nano-12b-v2-vl" },
		},
		"nvidia/nemotron-nano-12b-v2-vl:free": {
			IDVal:         "nvidia/nemotron-nano-12b-v2-vl:free",
			NameVal:       "NVIDIA: Nemotron Nano 12B 2 VL (free)",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.\n\nThe model supports inputs of text and multi-image documents, producing natural-language outputs. It is trained on high-quality NVIDIA-curated synthetic datasets optimized for optical-character recognition, chart reasoning, and multimodal comprehension.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores ≈ 74 average across MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, and Video-MME—surpassing prior open VL baselines. With Efficient Video Sampling (EVS), it handles long-form videos while reducing inference cost.\n\nOpen-weights, training data, and fine-tuning recipes are released under a permissive NVIDIA open license, with deployment supported across NeMo, NIM, and major inference runtimes.",
			DescCNVal:     "NVIDIA Nemotron Nano 2 VL 是一款拥有 120 亿参数的开源多模态推理模型，专为视频理解和文档智能设计。该模型采用混合 Transformer-Mamba 架构，结合了 Transformer 级别的准确性与 Mamba 的内存高效序列建模能力，显著提升吞吐量并降低延迟。\n\n该模型支持文本和多图像文档输入，并生成自然语言输出。其训练数据为 NVIDIA 精心构建的高质量合成数据集，针对光学字符识别（OCR）、图表推理和多模态理解进行了优化。\n\nNemotron Nano 2 VL 在 OCRBench v2 上取得领先成果，并在 MMMU、MathVista、AI2D、OCRBench、OCR-Reasoning、ChartQA、DocVQA 和 Video-MME 等基准测试中平均得分约 74，超越此前所有开源视觉语言基线模型。借助高效视频采样（EVS）技术，该模型可处理长视频内容，同时降低推理成本。\n\n模型权重、训练数据和微调方案均以宽松的 NVIDIA 开源许可证发布，并支持在 NeMo、NIM 及主流推理运行时环境中部署。",
			ContextLenVal: 128000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "nemotron-nano-12b-v2-vl:free" },
		},
		"nvidia/nemotron-nano-9b-v2": {
			IDVal:         "nvidia/nemotron-nano-9b-v2",
			NameVal:       "NVIDIA: Nemotron Nano 9B V2",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. \n\nThe model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.",
			DescCNVal:     "NVIDIA-Nemotron-Nano-9B-v2 是 NVIDIA 从零开始训练的大语言模型（LLM），设计为统一处理推理与非推理任务。模型在响应用户查询和任务时，首先生成推理轨迹，再给出最终答案。\n\n其推理能力可通过系统提示词进行控制。若用户希望模型直接提供最终答案而不显示中间推理轨迹，亦可进行相应配置。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nemotron-nano-9b-v2" },
		},
		"nvidia/nemotron-nano-9b-v2:free": {
			IDVal:         "nvidia/nemotron-nano-9b-v2:free",
			NameVal:       "NVIDIA: Nemotron Nano 9B V2 (free)",
			ProviderVal:   "Nvidia",
			DescVal:       "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. \n\nThe model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.",
			DescCNVal:     "NVIDIA-Nemotron-Nano-9B-v2 是 NVIDIA 从零开始训练的大语言模型（LLM），设计为统一处理推理与非推理任务。模型在响应用户查询和任务时，首先生成推理轨迹，再给出最终答案。\n\n其推理能力可通过系统提示词进行控制。若用户希望模型直接提供最终答案而不显示中间推理轨迹，亦可进行相应配置。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "nemotron-nano-9b-v2:free" },
		},
		"openai/chatgpt-4o-latest": {
			IDVal:         "openai/chatgpt-4o-latest",
			NameVal:       "OpenAI: ChatGPT-4o",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of [GPT-4o](/models/openai/gpt-4o) in that it has additional RLHF. It is intended for research and evaluation.\n\nOpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.",
			DescCNVal:     "OpenAI ChatGPT-4o 由 OpenAI 持续更新，始终指向 ChatGPT 当前使用的 GPT-4o 版本。因此，与 API 版本的 [GPT-4o](/models/openai/gpt-4o) 略有不同，它额外应用了 RLHF 微调，主要用于研究与评估。\n\nOpenAI 指出，该模型不适用于生产环境，未来可能被移除或重定向至其他模型。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000005,
			PriceOutVal:   0.000015,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "chatgpt-4o-latest" },
		},
		"openai/gpt-3.5-turbo": {
			IDVal:         "openai/gpt-3.5-turbo",
			NameVal:       "OpenAI: GPT-3.5 Turbo",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\n\nTraining data up to Sep 2021.",
			DescCNVal:     "GPT-3.5 Turbo 是 OpenAI 速度最快的模型，可理解和生成自然语言或代码，专为聊天及传统补全任务优化。\n\n训练数据截止至 2021 年 9 月。",
			ContextLenVal: 16385,
			MaxOutputVal:  4096,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-3.5-turbo" },
		},
		"openai/gpt-3.5-turbo-0613": {
			IDVal:         "openai/gpt-3.5-turbo-0613",
			NameVal:       "OpenAI: GPT-3.5 Turbo (older v0613)",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\n\nTraining data up to Sep 2021.",
			DescCNVal:     "GPT-3.5 Turbo 是 OpenAI 最快的模型，可理解并生成自然语言或代码，专为聊天及传统补全任务优化。\n\n训练数据截止于 2021 年 9 月。",
			ContextLenVal: 4095,
			MaxOutputVal:  4096,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-3.5-turbo-0613" },
		},
		"openai/gpt-3.5-turbo-16k": {
			IDVal:         "openai/gpt-3.5-turbo-16k",
			NameVal:       "OpenAI: GPT-3.5 Turbo 16k",
			ProviderVal:   "OpenAI",
			DescVal:       "This model offers four times the context length of gpt-3.5-turbo, allowing it to support approximately 20 pages of text in a single request at a higher cost. Training data: up to Sep 2021.",
			DescCNVal:     "该模型的上下文长度是 gpt-3.5-turbo 的四倍，单次请求可处理约 20 页文本，但成本更高。训练数据截止至 2021 年 9 月。",
			ContextLenVal: 16385,
			MaxOutputVal:  4096,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-3.5-turbo-16k" },
		},
		"openai/gpt-3.5-turbo-instruct": {
			IDVal:         "openai/gpt-3.5-turbo-instruct",
			NameVal:       "OpenAI: GPT-3.5 Turbo Instruct",
			ProviderVal:   "OpenAI",
			DescVal:       "This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.",
			DescCNVal:     "该模型是 GPT-3.5 Turbo 的变体，专为指令类提示进行调优，未包含面向聊天场景的优化。训练数据截止至 2021 年 9 月。",
			ContextLenVal: 4095,
			MaxOutputVal:  4096,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000002,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-3.5-turbo-instruct" },
		},
		"openai/gpt-4": {
			IDVal:         "openai/gpt-4",
			NameVal:       "OpenAI: GPT-4",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.",
			DescCNVal:     "OpenAI 旗舰模型 GPT-4 是一款大规模多模态语言模型，凭借更广泛的知识储备和更强的推理能力，在解决复杂问题方面比以往模型更为精准。训练数据截止至 2021 年 9 月。",
			ContextLenVal: 8191,
			MaxOutputVal:  4096,
			PriceInVal:    0.000030,
			PriceOutVal:   0.000060,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4" },
		},
		"openai/gpt-4-0314": {
			IDVal:         "openai/gpt-4-0314",
			NameVal:       "OpenAI: GPT-4 (older v0314)",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was supported until June 14. Training data: up to Sep 2021.",
			DescCNVal:     "GPT-4-0314 是 GPT-4 的首个发布版本，上下文长度为 8,192 个 token，支持持续至 6 月 14 日。训练数据截止至 2021 年 9 月。",
			ContextLenVal: 8191,
			MaxOutputVal:  4096,
			PriceInVal:    0.000030,
			PriceOutVal:   0.000060,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4-0314" },
		},
		"openai/gpt-4-1106-preview": {
			IDVal:         "openai/gpt-4-1106-preview",
			NameVal:       "OpenAI: GPT-4 Turbo (older v1106)",
			ProviderVal:   "OpenAI",
			DescVal:       "The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to April 2023.",
			DescCNVal:     "最新的 GPT-4 Turbo 模型，具备视觉能力。视觉请求现已支持 JSON 模式与函数调用。\n\n训练数据截止至 2023 年 4 月。",
			ContextLenVal: 128000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000010,
			PriceOutVal:   0.000030,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4-1106-preview" },
		},
		"openai/gpt-4-turbo": {
			IDVal:         "openai/gpt-4-turbo",
			NameVal:       "OpenAI: GPT-4 Turbo",
			ProviderVal:   "OpenAI",
			DescVal:       "The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to December 2023.",
			DescCNVal:     "OpenAI 的高性能模型，支持 128k 上下文。",
			ContextLenVal: 128000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000010,
			PriceOutVal:   0.000030,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4-turbo", "gpt4t" },
		},
		"openai/gpt-4-turbo-preview": {
			IDVal:         "openai/gpt-4-turbo-preview",
			NameVal:       "OpenAI: GPT-4 Turbo Preview",
			ProviderVal:   "OpenAI",
			DescVal:       "The preview GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Dec 2023.\n\n**Note:** heavily rate limited by OpenAI while in preview.",
			DescCNVal:     "预览版 GPT-4 模型，改进了指令遵循能力，支持 JSON 模式、可复现输出、并行函数调用等功能。训练数据截止于 2023 年 12 月。\n\n**注意**：预览期间受 OpenAI 严格速率限制。",
			ContextLenVal: 128000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000010,
			PriceOutVal:   0.000030,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4-turbo-preview" },
		},
		"openai/gpt-4.1": {
			IDVal:         "openai/gpt-4.1",
			NameVal:       "OpenAI: GPT-4.1",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
			DescCNVal:     "GPT-4.1 是一款旗舰级大语言模型，针对高级指令遵循、现实世界软件工程和长上下文推理进行了优化。支持 100 万 token 的上下文窗口，在编码（SWE-bench Verified 达 54.6%）、指令遵循（IFEval 达 87.4%）和多模态理解等基准测试中均优于 GPT-4o 和 GPT-4.5。该模型专为精确代码差异生成、智能体可靠性以及在大型文档上下文中实现高召回率而调优，非常适合用于智能体、IDE 工具和企业知识检索场景。",
			ContextLenVal: 1047576,
			MaxOutputVal:  32768,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4.1" },
		},
		"openai/gpt-4.1-mini": {
			IDVal:         "openai/gpt-4.1-mini",
			NameVal:       "OpenAI: GPT-4.1 Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.",
			DescCNVal:     "GPT-4.1 Mini 是一款中等规模模型，在显著降低延迟和成本的同时，性能可与 GPT-4o 相媲美。它保留了 100 万 token 的上下文窗口，在困难指令评估中得分为 45.1%，MultiChallenge 得分为 35.8%，IFEval 得分为 84.1%。Mini 在编码能力（如 Aider 多语言差异基准达 31.6%）和视觉理解方面也表现出色，适用于对性能要求严苛的交互式应用。",
			ContextLenVal: 1047576,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4.1-mini" },
		},
		"openai/gpt-4.1-nano": {
			IDVal:         "openai/gpt-4.1-nano",
			NameVal:       "OpenAI: GPT-4.1 Nano",
			ProviderVal:   "OpenAI",
			DescVal:       "For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.",
			DescCNVal:     "对于低延迟任务，GPT-4.1 Nano 是 GPT-4.1 系列中速度最快、成本最低的模型。其体积小巧却性能卓越，配备 100 万 token 上下文窗口，在 MMLU 上得分 80.1%，GPQA 上得分 50.3%，Aider 多语言编码基准上得分 9.8%——甚至高于 GPT-4o Mini。该模型非常适合分类或自动补全等任务。",
			ContextLenVal: 1047576,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4.1-nano" },
		},
		"openai/gpt-4o": {
			IDVal:         "openai/gpt-4o",
			NameVal:       "OpenAI: GPT-4o",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
			DescCNVal:     "GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本与图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。\n\n在与其他模型的基准对比中，该模型曾短暂命名为 [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)。\n\n#multimodal",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o" },
		},
		"openai/gpt-4o-2024-05-13": {
			IDVal:         "openai/gpt-4o-2024-05-13",
			NameVal:       "OpenAI: GPT-4o (2024-05-13)",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
			DescCNVal:     "GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本和图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但推理速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。\n\n在与其他模型的基准测试中，该模型曾短暂使用代号 [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)。\n\n#多模态",
			ContextLenVal: 128000,
			MaxOutputVal:  4096,
			PriceInVal:    0.000005,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-2024-05-13" },
		},
		"openai/gpt-4o-2024-08-06": {
			IDVal:         "openai/gpt-4o-2024-08-06",
			NameVal:       "OpenAI: GPT-4o (2024-08-06)",
			ProviderVal:   "OpenAI",
			DescVal:       "The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability to supply a JSON schema in the respone_format. Read more [here](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)",
			DescCNVal:     "GPT-4o 2024-08-06 版本在结构化输出方面表现更佳，支持在 response_format 中指定 JSON Schema。更多详情请参阅[此处](https://openai.com/index/introducing-structured-outputs-in-the-api/)。\n\nGPT-4o（“o”代表“omni”）是 OpenAI 最新的人工智能模型，支持文本与图像输入并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但推理速度提升一倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面亦有显著增强。\n\n在与其他模型的基准测试中，该模型曾短暂使用代号 [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-2024-08-06" },
		},
		"openai/gpt-4o-2024-11-20": {
			IDVal:         "openai/gpt-4o-2024-11-20",
			NameVal:       "OpenAI: GPT-4o (2024-11-20)",
			ProviderVal:   "OpenAI",
			DescVal:       "The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, engaging, and tailored writing to improve relevance & readability. It’s also better at working with uploaded files, providing deeper insights & more thorough responses.\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.",
			DescCNVal:     "GPT-4o 2024-11-20 版本提升了创意写作能力，生成更自然、生动且高度定制化的文本，显著增强相关性与可读性。同时，在处理上传文件方面表现更优，可提供更深入的洞察和更全面的回答。\n\nGPT-4o（“o”代表“omni”）是 OpenAI 最新一代 AI 模型，支持文本和图像输入并输出文本。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升一倍，成本降低 50%。此外，该版本在非英语语言处理和视觉能力方面也有进一步增强。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-2024-11-20" },
		},
		"openai/gpt-4o-audio-preview": {
			IDVal:         "openai/gpt-4o-audio-preview",
			NameVal:       "OpenAI: GPT-4o Audio",
			ProviderVal:   "OpenAI",
			DescVal:       "The gpt-4o-audio-preview model adds support for audio inputs as prompts. This enhancement allows the model to detect nuances within audio recordings and add depth to generated user experiences. Audio outputs are currently not supported. Audio tokens are priced at $40 per million input and $80 per million output audio tokens.",
			DescCNVal:     "gpt-4o-audio-preview 模型新增对音频输入作为提示的支持。此增强功能使模型能够识别音频录音中的细微差别，从而丰富生成的用户体验。目前暂不支持音频输出。音频 token 的定价为每百万输入音频 token 40 美元。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityAudioIn | ModalityAudioOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-audio-preview" },
		},
		"openai/gpt-4o-mini": {
			IDVal:         "openai/gpt-4o-mini",
			NameVal:       "OpenAI: GPT-4o-mini",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
			DescCNVal:     "GPT-4o mini 是继 [GPT-4 Omni](/models/openai/gpt-4o) 之后 OpenAI 推出的最新模型，支持文本和图像输入，并生成文本输出。\n\n作为其最先进的小型模型，其成本远低于其他近期前沿模型，比 [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo) 便宜 60% 以上，同时保持业界领先的智能水平，性价比显著提升。\n\nGPT-4o mini 在 MMLU 基准测试中取得 82% 的得分，目前在主流聊天偏好排行榜上超越 GPT-4（参见 [常见排行榜](https://arena.lmsys.org/)）。\n\n了解更多详情，请查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)。\n\n#多模态",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-mini" },
		},
		"openai/gpt-4o-mini-2024-07-18": {
			IDVal:         "openai/gpt-4o-mini-2024-07-18",
			NameVal:       "OpenAI: GPT-4o-mini (2024-07-18)",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
			DescCNVal:     "GPT-4o mini 是继 [GPT-4 Omni](/models/openai/gpt-4o) 之后 OpenAI 推出的最新模型，支持文本和图像输入，并生成文本输出。\n\n作为其最先进的小型模型，其成本远低于其他近期前沿模型，比 [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo) 便宜 60% 以上，同时保持业界领先的智能水平，性价比显著提升。\n\nGPT-4o mini 在 MMLU 基准测试中取得 82% 的得分，目前在主流聊天偏好排行榜上超越 GPT-4（参见 [常见排行榜](https://arena.lmsys.org/)）。\n\n了解更多详情，请查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)。\n\n#多模态",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-mini-2024-07-18" },
		},
		"openai/gpt-4o-mini-search-preview": {
			IDVal:         "openai/gpt-4o-mini-search-preview",
			NameVal:       "OpenAI: GPT-4o-mini Search Preview",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
			DescCNVal:     "GPT-4o mini Search Preview 是专用于聊天补全（Chat Completions）中网络搜索的专用模型，经过训练以理解并执行网络搜索查询。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-mini-search-preview" },
		},
		"openai/gpt-4o-search-preview": {
			IDVal:         "openai/gpt-4o-search-preview",
			NameVal:       "OpenAI: GPT-4o Search Preview",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
			DescCNVal:     "GPT-4o Search Preview 是专用于聊天补全（Chat Completions）中网络搜索的专用模型，经过训练以理解并执行网络搜索查询。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o-search-preview" },
		},
		"openai/gpt-4o:extended": {
			IDVal:         "openai/gpt-4o:extended",
			NameVal:       "OpenAI: GPT-4o (extended)",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal",
			DescCNVal:     "GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本与图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。\n\n在与其他模型的基准对比中，该模型曾短暂命名为 [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)。\n\n#multimodal",
			ContextLenVal: 128000,
			MaxOutputVal:  64000,
			PriceInVal:    0.000006,
			PriceOutVal:   0.000018,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-4o:extended" },
		},
		"openai/gpt-5": {
			IDVal:         "openai/gpt-5",
			NameVal:       "OpenAI: GPT-5",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
			DescCNVal:     "GPT-5 是 OpenAI 最先进的模型，在推理能力、代码质量和用户体验方面实现重大提升。该模型针对需要逐步推理、指令遵循以及在高风险场景中保持高准确性的复杂任务进行了优化。支持测试时路由功能和高级提示理解能力，包括用户指定的意图（如“认真思考此问题”）。改进包括减少幻觉和迎合倾向，并在编码、写作及健康相关任务中表现更佳。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5" },
		},
		"openai/gpt-5-chat": {
			IDVal:         "openai/gpt-5-chat",
			NameVal:       "OpenAI: GPT-5 Chat",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.",
			DescCNVal:     "GPT-5 Chat 专为企业级应用设计，支持高级、自然、多模态且具备上下文感知能力的对话。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-chat" },
		},
		"openai/gpt-5-codex": {
			IDVal:         "openai/gpt-5-codex",
			NameVal:       "OpenAI: GPT-5 Codex",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
			DescCNVal:     "GPT-5-Codex 是 GPT-5 的专用版本，针对软件工程和编码工作流进行了优化。它既适用于交互式开发会话，也适用于长时间独立执行复杂工程任务。该模型支持从零构建项目、功能开发、调试、大规模重构和代码审查。相比 GPT-5，Codex 更具可控性，能更严格遵循开发者指令，并生成更简洁、高质量的代码。推理强度可通过 `reasoning.effort` 参数调节。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex 可集成至 CLI、IDE 插件、GitHub 和云任务等开发者环境。它能动态调整推理强度——对小型任务快速响应，对大型项目则可持续运行数小时。该模型经过训练，可执行结构化代码审查，通过推理依赖关系并验证测试行为来发现关键缺陷。它还支持图像或截图等多模态输入用于 UI 开发，并集成工具用于搜索、依赖安装和环境配置。Codex 专为智能体驱动的编码应用而设计。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-codex" },
		},
		"openai/gpt-5-image": {
			IDVal:         "openai/gpt-5-image",
			NameVal:       "OpenAI: GPT-5 Image",
			ProviderVal:   "OpenAI",
			DescVal:       "[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's GPT-5 model with state-of-the-art image generation capabilities. It offers major improvements in reasoning, code quality, and user experience while incorporating GPT Image 1's superior instruction following, text rendering, and detailed image editing.",
			DescCNVal:     "[GPT-5](https://openrouter.ai/openai/gpt-5) Image 将 OpenAI 的 GPT-5 模型与尖端图像生成能力相结合，在推理能力、代码质量和用户体验方面实现显著提升，同时继承了 GPT Image 1 在指令遵循、文本渲染和精细图像编辑方面的卓越表现。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000010,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityImageOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-image" },
		},
		"openai/gpt-5-image-mini": {
			IDVal:         "openai/gpt-5-image-mini",
			NameVal:       "OpenAI: GPT-5 Image Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini), with GPT Image 1 Mini for efficient image generation. This natively multimodal model features superior instruction following, text rendering, and detailed image editing with reduced latency and cost. It excels at high-quality visual creation while maintaining strong text understanding, making it ideal for applications that require both efficient image generation and text processing at scale.",
			DescCNVal:     "GPT-5 Image Mini 结合了由 [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini) 驱动的先进语言能力与 GPT Image 1 Mini 的高效图像生成能力。这一原生多模态模型具备卓越的指令遵循、文本渲染和精细图像编辑能力，同时显著降低延迟与成本。它在高质量视觉内容创作方面表现出色，并保持强大的文本理解能力，非常适合需要大规模高效图像生成与文本处理的应用场景。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityImageOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-image-mini" },
		},
		"openai/gpt-5-mini": {
			IDVal:         "openai/gpt-5-mini",
			NameVal:       "OpenAI: GPT-5 Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.",
			DescCNVal:     "GPT-5 Mini 是 GPT-5 的紧凑版本，专为轻量级推理任务设计。它继承了 GPT-5 的指令遵循与安全对齐优势，同时显著降低延迟和成本。GPT-5 Mini 是 OpenAI o4-mini 模型的继任者。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-mini" },
		},
		"openai/gpt-5-nano": {
			IDVal:         "openai/gpt-5-nano",
			NameVal:       "OpenAI: GPT-5 Nano",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
			DescCNVal:     "GPT-5-Nano 是 GPT-5 系列中最小、最快的变体，专为开发者工具、快速交互和超低延迟环境优化。尽管其推理深度较大型版本有所限制，但仍保留了关键的指令遵循与安全特性。该模型是 GPT-4.1-nano 的继任者，为成本敏感或实时应用场景提供轻量级选择。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-nano" },
		},
		"openai/gpt-5-pro": {
			IDVal:         "openai/gpt-5-pro",
			NameVal:       "OpenAI: GPT-5 Pro",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
			DescCNVal:     "GPT-5 Pro 是 OpenAI 最先进的模型，在推理能力、代码质量和用户体验方面均有重大提升。该模型针对需逐步推理、精准遵循指令及高风险场景下高准确性的复杂任务进行了优化。支持运行时路由功能和高级提示理解能力，包括识别用户指定意图（如“认真思考此问题”）。改进包括降低幻觉与迎合倾向，并在编程、写作及健康相关任务中表现更优。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000015,
			PriceOutVal:   0.000120,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5-pro" },
		},
		"openai/gpt-5.1": {
			IDVal:         "openai/gpt-5.1",
			NameVal:       "OpenAI: GPT-5.1",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems.\n\nBuilt for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5",
			DescCNVal:     "GPT-5.1 是 GPT-5 系列最新的前沿级模型，在通用推理能力、指令遵循性和自然对话风格方面均优于 GPT-5。它采用自适应推理机制，动态分配计算资源：对简单查询快速响应，对复杂任务投入更深推理。模型输出更清晰、更贴近事实的解释，减少术语使用，即使面对技术性或多步骤问题也易于理解。\n\n为广泛任务覆盖而构建，GPT-5.1 在数学、编程和结构化分析负载上持续提升，提供更连贯的长文本回答及更可靠的工具调用能力。同时，其对话对齐经过精细调优，在不牺牲准确性的前提下实现更亲切、直观的回应。GPT-5.1 是 GPT-5 的主要全功能继任者。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.1" },
		},
		"openai/gpt-5.1-chat": {
			IDVal:         "openai/gpt-5.1-chat",
			NameVal:       "OpenAI: GPT-5.1 Chat",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.1 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.\n",
			DescCNVal:     "GPT-5.1 Chat（又名 Instant）是 5.1 系列中轻量、快速的成员，专为低延迟聊天优化，同时保留强大的通用智能。它采用自适应推理机制，仅在处理较难查询时选择性“深入思考”，从而在不影响常规对话速度的前提下提升数学、编程及多步骤任务的准确性。该模型默认更具亲和力和对话感，指令遵循能力更强，短文本推理更稳定。GPT-5.1 Chat 专为高吞吐、交互式工作负载设计，在响应速度与一致性比深度推理更重要的场景中表现出色。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.1-chat" },
		},
		"openai/gpt-5.1-codex": {
			IDVal:         "openai/gpt-5.1-codex",
			NameVal:       "OpenAI: GPT-5.1-Codex",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
			DescCNVal:     "GPT-5.1-Codex 是 GPT-5.1 的专用版本，针对软件工程与编程工作流优化。它既适用于交互式开发会话，也能独立执行复杂的长期工程任务。模型支持从零构建项目、功能开发、调试、大规模重构及代码审查。相比 GPT-5.1，Codex 更具可控性，严格遵循开发者指令，并生成更简洁、高质量的代码。可通过 `reasoning.effort` 参数调节推理强度，详见[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)。\n\nCodex 可集成至 CLI、IDE 插件、GitHub 及云任务等开发者环境，能动态调整推理强度——小任务快速响应，大型项目则可持续运行数小时。模型经过专门训练，可执行结构化代码审查，通过依赖分析与测试验证识别关键缺陷。此外，它还支持 UI 开发所需的多模态输入（如图像或截图），并集成工具用于搜索、依赖安装及环境配置。Codex 专为智能体编程应用而设计。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.1-codex" },
		},
		"openai/gpt-5.1-codex-max": {
			IDVal:         "openai/gpt-5.1-codex-max",
			NameVal:       "OpenAI: GPT-5.1-Codex-Max",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.1-Codex-Max is OpenAI’s latest agentic coding model, designed for long-running, high-context software development tasks. It is based on an updated version of the 5.1 reasoning stack and trained on agentic workflows spanning software engineering, mathematics, and research. \nGPT-5.1-Codex-Max delivers faster performance, improved reasoning, and higher token efficiency across the development lifecycle. ",
			DescCNVal:     "GPT-5.1-Codex-Max 是 OpenAI 最新的智能体编程模型，专为长时间运行、高上下文的软件开发任务而设计。该模型基于更新版的 5.1 推理架构，并在涵盖软件工程、数学和科研领域的智能体工作流上进行训练。\nGPT-5.1-Codex-Max 在整个开发生命周期中提供更快的性能、更强的推理能力以及更高的 Token 利用效率。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000010,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.1-codex-max" },
		},
		"openai/gpt-5.1-codex-mini": {
			IDVal:         "openai/gpt-5.1-codex-mini",
			NameVal:       "OpenAI: GPT-5.1-Codex-Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex",
			DescCNVal:     "GPT-5.1-Codex-Mini 是 GPT-5.1-Codex 的更小、更快版本。",
			ContextLenVal: 400000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.1-codex-mini" },
		},
		"openai/gpt-5.2": {
			IDVal:         "openai/gpt-5.2",
			NameVal:       "OpenAI: GPT-5.2",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long context perfomance compared to GPT-5.1. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks.\n\nBuilt for broad task coverage, GPT-5.2 delivers consistent gains across math, coding, sciende, and tool calling workloads, with more coherent long-form answers and improved tool-use reliability.",
			DescCNVal:     "GPT-5.2 是 GPT-5 系列中最新的前沿级模型，在智能体能力和长上下文性能方面较 GPT-5.1 有显著提升。该模型采用自适应推理机制，动态分配计算资源，对简单查询快速响应，同时对复杂任务投入更深层次的处理。\n\nGPT-5.2 面向广泛任务场景设计，在数学、编程、科学及工具调用等负载上均实现一致性能提升，并能生成更具连贯性的长篇回答，同时提高工具使用的可靠性。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000014,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.2" },
		},
		"openai/gpt-5.2-chat": {
			IDVal:         "openai/gpt-5.2-chat",
			NameVal:       "OpenAI: GPT-5.2 Chat",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.2 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.",
			DescCNVal:     "GPT-5.2 Chat（又称 Instant）是 GPT-5.2 系列中轻量、高速的成员，专为低延迟对话优化，同时保留强大的通用智能。该模型采用自适应推理机制，仅在面对较难查询时才启动深度思考，从而在不拖慢常规对话的前提下提升数学、编程及多步骤任务的准确性。默认设置下，模型语气更亲切自然，指令遵循能力更强，短程推理也更加稳定。GPT-5.2 Chat 专为高吞吐、交互式工作负载设计，在响应速度与一致性比深度推理更为关键的场景中表现优异。",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000014,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.2-chat" },
		},
		"openai/gpt-5.2-codex": {
			IDVal:         "openai/gpt-5.2-codex",
			NameVal:       "OpenAI: GPT-5.2-Codex",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.2-Codex is an upgraded version of GPT-5.1-Codex optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1-Codex, 5.2-Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
			DescCNVal:     "GPT-5.2-Codex 是 GPT-5.1-Codex 的升级版，针对软件工程与编程工作流进行了优化，适用于交互式开发会话及长时间独立执行复杂工程任务。该模型支持从零构建项目、功能开发、调试、大规模重构及代码审查。相比 GPT-5.1-Codex，5.2-Codex 更具可控性，严格遵循开发者指令，并生成更简洁、高质量的代码。可通过 `reasoning.effort` 参数调节推理强度。详见[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)。\n\nCodex 可集成至 CLI、IDE 插件、GitHub 及云任务等开发者环境，能动态调整推理强度——小任务快速响应，大型项目则可持续运行数小时。该模型经过训练，可执行结构化代码审查，通过分析依赖关系并对照测试验证行为，识别关键缺陷。此外，它还支持 UI 开发所需的图像或多模态输入，并集成工具用于搜索、依赖安装及环境配置。Codex 专为智能体编程应用场景而设计。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000014,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.2-codex" },
		},
		"openai/gpt-5.2-pro": {
			IDVal:         "openai/gpt-5.2-pro",
			NameVal:       "OpenAI: GPT-5.2 Pro",
			ProviderVal:   "OpenAI",
			DescVal:       "GPT-5.2 Pro is OpenAI’s most advanced model, offering major improvements in agentic coding and long context performance over GPT-5 Pro. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
			DescCNVal:     "GPT-5.2 Pro 是 OpenAI 最先进的模型，在智能体编程和长上下文性能方面相较 GPT-5 Pro 有重大提升。该模型专为需要逐步推理、精准指令遵循及高可靠性输出的高风险应用场景而优化。它支持运行时路由功能和高级提示理解能力，包括识别用户指定意图（如“对此深入思考”）。改进之处包括显著降低幻觉与迎合倾向，并在编程、写作及健康相关任务中表现更佳。",
			ContextLenVal: 400000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000021,
			PriceOutVal:   0.000168,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-5.2-pro" },
		},
		"openai/gpt-audio": {
			IDVal:         "openai/gpt-audio",
			NameVal:       "OpenAI: GPT Audio",
			ProviderVal:   "OpenAI",
			DescVal:       "The gpt-audio model is OpenAI's first generally available audio model. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Audio is priced at $32 per million input tokens and $64 per million output tokens.",
			DescCNVal:     "",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000010,
			Features:      CapJsonMode | ModalityAudioIn | ModalityAudioOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-audio" },
		},
		"openai/gpt-audio-mini": {
			IDVal:         "openai/gpt-audio-mini",
			NameVal:       "OpenAI: GPT Audio Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "A cost-efficient version of GPT Audio. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Input is priced at $0.60 per million tokens and output is priced at $2.40 per million tokens.",
			DescCNVal:     "",
			ContextLenVal: 128000,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      CapJsonMode | ModalityAudioIn | ModalityAudioOut | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-audio-mini" },
		},
		"openai/gpt-oss-120b": {
			IDVal:         "openai/gpt-oss-120b",
			NameVal:       "OpenAI: gpt-oss-120b",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
			DescCNVal:     "gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-120b" },
		},
		"openai/gpt-oss-120b:exacto": {
			IDVal:         "openai/gpt-oss-120b:exacto",
			NameVal:       "OpenAI: gpt-oss-120b (exacto)",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
			DescCNVal:     "gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-120b:exacto" },
		},
		"openai/gpt-oss-120b:free": {
			IDVal:         "openai/gpt-oss-120b:free",
			NameVal:       "OpenAI: gpt-oss-120b (free)",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
			DescCNVal:     "gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-120b:free" },
		},
		"openai/gpt-oss-20b": {
			IDVal:         "openai/gpt-oss-20b",
			NameVal:       "OpenAI: gpt-oss-20b",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
			DescCNVal:     "gpt-oss-20b 是 OpenAI 在 Apache 2.0 许可下发布的开源权重、210 亿参数模型。采用混合专家（MoE）架构，每次前向传递激活 36 亿参数，针对低延迟推理和消费级或单 GPU 硬件部署进行了优化。该模型基于 OpenAI 的 Harmony 响应格式训练，支持推理层级配置、微调以及智能体能力，包括函数调用、工具使用和结构化输出。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-20b" },
		},
		"openai/gpt-oss-20b:free": {
			IDVal:         "openai/gpt-oss-20b:free",
			NameVal:       "OpenAI: gpt-oss-20b (free)",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
			DescCNVal:     "gpt-oss-20b 是 OpenAI 在 Apache 2.0 许可下发布的开源权重、210 亿参数模型。采用混合专家（MoE）架构，每次前向传递激活 36 亿参数，针对低延迟推理和消费级或单 GPU 硬件部署进行了优化。该模型基于 OpenAI 的 Harmony 响应格式训练，支持推理层级配置、微调以及智能体能力，包括函数调用、工具使用和结构化输出。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-20b:free" },
		},
		"openai/gpt-oss-safeguard-20b": {
			IDVal:         "openai/gpt-oss-safeguard-20b",
			NameVal:       "OpenAI: gpt-oss-safeguard-20b",
			ProviderVal:   "OpenAI",
			DescVal:       "gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weight, 21B-parameter Mixture-of-Experts (MoE) model offers lower latency for safety tasks like content classification, LLM filtering, and trust & safety labeling.\n\nLearn more about this model in OpenAI's gpt-oss-safeguard [user guide](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide).",
			DescCNVal:     "gpt-oss-safeguard-20b 是 OpenAI 基于 gpt-oss-20b 构建的安全推理模型。这款开源权重、210 亿参数的混合专家（MoE）模型在内容分类、大模型过滤及信任与安全标注等安全任务上具备更低延迟。\n\n更多关于此模型的信息，请参阅 OpenAI 的 gpt-oss-safeguard [用户指南](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide)。",
			ContextLenVal: 131072,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "gpt-oss-safeguard-20b" },
		},
		"openai/o1": {
			IDVal:         "openai/o1",
			NameVal:       "OpenAI: o1",
			ProviderVal:   "OpenAI",
			DescVal:       "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. \n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n",
			DescCNVal:     "o1 是 OpenAI 最新且最强的模型系列，设计上会花更多时间进行思考后再响应。该系列模型通过大规模强化学习训练，采用思维链（Chain of Thought）方式进行推理。\n\no1 模型针对数学、科学、编程及其他 STEM 领域任务进行了优化，在物理、化学和生物学等领域的基准测试中持续展现出博士级准确率。更多信息请参阅[发布公告](https://openai.com/o1)。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000015,
			PriceOutVal:   0.000060,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o1" },
		},
		"openai/o1-pro": {
			IDVal:         "openai/o1-pro",
			NameVal:       "OpenAI: o1-pro",
			ProviderVal:   "OpenAI",
			DescVal:       "The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.",
			DescCNVal:     "o1 系列模型通过强化学习训练，能够在回答前进行思考并执行复杂推理。o1-pro 模型投入更多计算资源进行深度思考，从而持续提供更优质的答案。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000150,
			PriceOutVal:   0.000600,
			Features:      CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o1-pro" },
		},
		"openai/o3": {
			IDVal:         "openai/o3",
			NameVal:       "OpenAI: o3",
			ProviderVal:   "OpenAI",
			DescVal:       "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. ",
			DescCNVal:     "o3 是一款在多个领域表现均衡且强大的模型，在数学、科学、编程和视觉推理任务中树立了新标杆。同时，它在技术写作和指令遵循方面也极为出色。适用于需结合文本、代码和图像进行多步骤分析的问题求解。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o3" },
		},
		"openai/o3-deep-research": {
			IDVal:         "openai/o3-deep-research",
			NameVal:       "OpenAI: o3 Deep Research",
			ProviderVal:   "OpenAI",
			DescVal:       "o3-deep-research is OpenAI's advanced model for deep research, designed to tackle complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.",
			DescCNVal:     "o3-deep-research 是 OpenAI 专为深度研究设计的高级模型，旨在处理复杂的多步骤研究任务。\n\n注意：此模型始终使用“web_search”工具，会产生额外费用。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000010,
			PriceOutVal:   0.000040,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o3-deep-research" },
		},
		"openai/o3-mini": {
			IDVal:         "openai/o3-mini",
			NameVal:       "OpenAI: o3 Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to \"high\", \"medium\", or \"low\" to control the thinking time of the model. The default is \"medium\". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to \"high\".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.",
			DescCNVal:     "OpenAI o3-mini 是一款高性价比语言模型，专为 STEM 推理任务优化，在科学、数学和编程领域表现尤为突出。\n\n该模型支持 `reasoning_effort` 参数，可设为“high”、“medium”或“low”，以控制模型的思考时间，默认值为“medium”。OpenRouter 还提供模型别名 `openai/o3-mini-high`，默认将该参数设为“high”。\n\n模型具备三级可调推理强度，并支持函数调用、结构化输出和流式响应等关键开发者功能，但不包含视觉处理能力。\n\n相比前代模型，o3-mini 实现显著改进：专家评测者在 56% 的情况下更偏好其回答，且在复杂问题上重大错误减少 39%。在中等推理强度设置下，o3-mini 在 AIME 和 GPQA 等高难度推理评测中性能媲美更大的 o1 模型，同时保持更低延迟与成本。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o3-mini" },
		},
		"openai/o3-mini-high": {
			IDVal:         "openai/o3-mini-high",
			NameVal:       "OpenAI: o3 Mini High",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. \n\no3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.",
			DescCNVal:     "OpenAI o3-mini-high 与 [o3-mini](/openai/o3-mini) 为同一模型，仅将 reasoning_effort 参数设为 high。\n\no3-mini 是一款高性价比的语言模型，专为 STEM 推理任务优化，在科学、数学和编程领域表现尤为突出。该模型提供三种可调节的推理强度级别，并支持函数调用、结构化输出和流式响应等关键开发者功能，但不包含视觉处理能力。\n\n相比前代模型，o3-mini 表现显著提升：专家评测者在 56% 的情况下更偏好其回答，并在复杂问题上观察到重大错误减少 39%。在中等推理强度设置下，o3-mini 在 AIME 和 GPQA 等高难度推理评测中达到与更大规模的 o1 模型相当的性能，同时保持更低的延迟和成本。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o3-mini-high" },
		},
		"openai/o3-pro": {
			IDVal:         "openai/o3-pro",
			NameVal:       "OpenAI: o3 Pro",
			ProviderVal:   "OpenAI",
			DescVal:       "The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers.\n\nNote that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations",
			DescCNVal:     "o 系列模型通过强化学习训练，在回答前进行思考，以执行复杂推理。o3-pro 模型投入更多算力进行深度思考，从而持续提供更优答案。\n\n注意：使用此模型需自带密钥（BYOK）。设置地址：https://openrouter.ai/settings/integrations",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000020,
			PriceOutVal:   0.000080,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o3-pro" },
		},
		"openai/o4-mini": {
			IDVal:         "openai/o4-mini",
			NameVal:       "OpenAI: o4 Mini",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
			DescCNVal:     "OpenAI o4-mini 是 o 系列中一款紧凑型推理模型，在保持强大多模态和智能体能力的同时，针对快速、高性价比的性能进行了优化。该模型支持工具调用，在 AIME（配合 Python 达 99.5%）和 SWE-bench 等基准测试中展现出具有竞争力的推理与编码能力，超越前代 o3-mini，甚至在某些领域接近 o3。\n\n尽管模型规模较小，o4-mini 在 STEM 任务、视觉问题求解（如 MathVista、MMMU）和代码编辑方面仍具备高准确率。特别适用于对延迟或成本敏感的高吞吐场景。得益于其高效架构和精细化的强化学习训练，o4-mini 能够串联工具、生成结构化输出，并在极短时间内（通常不到一分钟）完成多步骤任务。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o4-mini" },
		},
		"openai/o4-mini-deep-research": {
			IDVal:         "openai/o4-mini-deep-research",
			NameVal:       "OpenAI: o4 Mini Deep Research",
			ProviderVal:   "OpenAI",
			DescVal:       "o4-mini-deep-research is OpenAI's faster, more affordable deep research model—ideal for tackling complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.",
			DescCNVal:     "o4-mini-deep-research 是 OpenAI 推出的速度更快、成本更低的深度研究模型，非常适合处理复杂的多步骤研究任务。\n\n注意：此模型始终使用“web_search”工具，会产生额外费用。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o4-mini-deep-research" },
		},
		"openai/o4-mini-high": {
			IDVal:         "openai/o4-mini-high",
			NameVal:       "OpenAI: o4 Mini High",
			ProviderVal:   "OpenAI",
			DescVal:       "OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. \n\nOpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
			DescCNVal:     "OpenAI o4-mini-high 与 [o4-mini](/openai/o4-mini) 为同一模型，仅将 reasoning_effort 参数设为 high。\n\nOpenAI o4-mini 是 o 系列中一款紧凑型推理模型，在保持强大多模态和智能体能力的同时，针对快速、高性价比的性能进行了优化。该模型支持工具调用，在 AIME（配合 Python 达 99.5%）和 SWE-bench 等基准测试中展现出具有竞争力的推理与编码能力，超越前代 o3-mini，甚至在某些领域接近 o3。\n\n尽管模型规模较小，o4-mini 在 STEM 任务、视觉问题求解（如 MathVista、MMMU）和代码编辑方面仍具备高准确率。特别适用于对延迟或成本敏感的高吞吐场景。得益于其高效架构和精细化的强化学习训练，o4-mini 能够串联工具、生成结构化输出，并在极短时间内（通常不到一分钟）完成多步骤任务。",
			ContextLenVal: 200000,
			MaxOutputVal:  100000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityFileIn | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "o4-mini-high" },
		},
		"opengvlab/internvl3-78b": {
			IDVal:         "opengvlab/internvl3-78b",
			NameVal:       "OpenGVLab: InternVL3 78B",
			ProviderVal:   "Opengvlab",
			DescVal:       "The InternVL3 series is an advanced multimodal large language model (MLLM). Compared to InternVL 2.5, InternVL3 demonstrates stronger multimodal perception and reasoning capabilities. \n\nIn addition, InternVL3 is benchmarked against the Qwen2.5 Chat models, whose pre-trained base models serve as the initialization for its language component. Benefiting from Native Multimodal Pre-Training, the InternVL3 series surpasses the Qwen2.5 series in overall text performance.",
			DescCNVal:     "InternVL3 系列是先进的多模态大语言模型（MLLM）。相较于 InternVL 2.5，InternVL3 展现出更强的多模态感知与推理能力。\n\n此外，InternVL3 在基准测试中对标 Qwen2.5 Chat 模型，其语言模块以 Qwen2.5 预训练基础模型为初始化起点。得益于原生多模态预训练（Native Multimodal Pre-Training），InternVL3 系列在整体文本性能上超越 Qwen2.5 系列。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "internvl3-78b" },
		},
		"openrouter/auto": {
			IDVal:         "openrouter/auto",
			NameVal:       "Auto Router",
			ProviderVal:   "Openrouter",
			DescVal:       "Your prompt will be processed by a meta-model and routed to one of dozens of models (see below), optimizing for the best possible output.\n\nTo see which model was used, visit [Activity](/activity), or read the `model` attribute of the response. Your response will be priced at the same rate as the routed model.\n\nLearn more, including how to customize the models for routing, in our [docs](/docs/guides/routing/routers/auto-router).\n\nRequests will be routed to the following models:\n- [openai/gpt-5.1](/openai/gpt-5.1)\n- [openai/gpt-5](/openai/gpt-5)\n- [openai/gpt-5-mini](/openai/gpt-5-mini)\n- [openai/gpt-5-nano](/openai/gpt-5-nano)\n- [openai/gpt-4.1](/openai/gpt-4.1)\n- [openai/gpt-4.1-mini](/openai/gpt-4.1-mini)\n- [openai/gpt-4.1-nano](/openai/gpt-4.1-nano)\n- [openai/gpt-4o](/openai/gpt-4o)\n- [openai/gpt-4o-2024-05-13](/openai/gpt-4o-2024-05-13)\n- [openai/gpt-4o-2024-08-06](/openai/gpt-4o-2024-08-06)\n- [openai/gpt-4o-2024-11-20](/openai/gpt-4o-2024-11-20)\n- [openai/gpt-4o-mini](/openai/gpt-4o-mini)\n- [openai/gpt-4o-mini-2024-07-18](/openai/gpt-4o-mini-2024-07-18)\n- [openai/gpt-4-turbo](/openai/gpt-4-turbo)\n- [openai/gpt-4-turbo-preview](/openai/gpt-4-turbo-preview)\n- [openai/gpt-4-1106-preview](/openai/gpt-4-1106-preview)\n- [openai/gpt-4](/openai/gpt-4)\n- [openai/gpt-3.5-turbo](/openai/gpt-3.5-turbo)\n- [openai/gpt-oss-120b](/openai/gpt-oss-120b)\n- [anthropic/claude-opus-4.5](/anthropic/claude-opus-4.5)\n- [anthropic/claude-opus-4.1](/anthropic/claude-opus-4.1)\n- [anthropic/claude-opus-4](/anthropic/claude-opus-4)\n- [anthropic/claude-sonnet-4.5](/anthropic/claude-sonnet-4.5)\n- [anthropic/claude-sonnet-4](/anthropic/claude-sonnet-4)\n- [anthropic/claude-3.7-sonnet](/anthropic/claude-3.7-sonnet)\n- [anthropic/claude-haiku-4.5](/anthropic/claude-haiku-4.5)\n- [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku)\n- [anthropic/claude-3-haiku](/anthropic/claude-3-haiku)\n- [google/gemini-3-pro-preview](/google/gemini-3-pro-preview)\n- [google/gemini-2.5-pro](/google/gemini-2.5-pro)\n- [google/gemini-2.0-flash-001](/google/gemini-2.0-flash-001)\n- [google/gemini-2.5-flash](/google/gemini-2.5-flash)\n- [mistralai/mistral-large](/mistralai/mistral-large)\n- [mistralai/mistral-large-2407](/mistralai/mistral-large-2407)\n- [mistralai/mistral-large-2411](/mistralai/mistral-large-2411)\n- [mistralai/mistral-medium-3.1](/mistralai/mistral-medium-3.1)\n- [mistralai/mistral-nemo](/mistralai/mistral-nemo)\n- [mistralai/mistral-7b-instruct](/mistralai/mistral-7b-instruct)\n- [mistralai/mixtral-8x7b-instruct](/mistralai/mixtral-8x7b-instruct)\n- [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct)\n- [mistralai/codestral-2508](/mistralai/codestral-2508)\n- [x-ai/grok-4](/x-ai/grok-4)\n- [x-ai/grok-3](/x-ai/grok-3)\n- [x-ai/grok-3-mini](/x-ai/grok-3-mini)\n- [deepseek/deepseek-r1](/deepseek/deepseek-r1)\n- [meta-llama/llama-3.3-70b-instruct](/meta-llama/llama-3.3-70b-instruct)\n- [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct)\n- [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct)\n- [meta-llama/llama-3.1-8b-instruct](/meta-llama/llama-3.1-8b-instruct)\n- [meta-llama/llama-3-70b-instruct](/meta-llama/llama-3-70b-instruct)\n- [meta-llama/llama-3-8b-instruct](/meta-llama/llama-3-8b-instruct)\n- [qwen/qwen3-235b-a22b](/qwen/qwen3-235b-a22b)\n- [qwen/qwen3-32b](/qwen/qwen3-32b)\n- [qwen/qwen3-14b](/qwen/qwen3-14b)\n- [cohere/command-r-plus-08-2024](/cohere/command-r-plus-08-2024)\n- [cohere/command-r-08-2024](/cohere/command-r-08-2024)\n- [moonshotai/kimi-k2-thinking](/moonshotai/kimi-k2-thinking)\n- [perplexity/sonar](/perplexity/sonar)",
			DescCNVal:     "您的提示将由元模型处理，并自动路由至以下数十种模型之一（见下文），以优化输出质量。\n\n要查看实际使用的模型，请访问 [Activity](/activity)，或读取响应中的 `model` 字段。计费将按所路由模型的标准费率执行。\n\n更多详情（包括如何自定义路由模型）请参阅我们的 [文档](/docs/guides/routing/routers/auto-router)。\n\n请求可能被路由至以下模型：\n- [openai/gpt-5.1](/openai/gpt-5.1)\n- [openai/gpt-5](/openai/gpt-5)\n- [openai/gpt-5-mini](/openai/gpt-5-mini)\n- [openai/gpt-5-nano](/openai/gpt-5-nano)\n- [openai/gpt-4.1](/openai/gpt-4.1)\n- [openai/gpt-4.1-mini](/openai/gpt-4.1-mini)\n- [openai/gpt-4.1-nano](/openai/gpt-4.1-nano)\n- [openai/gpt-4o](/openai/gpt-4o)\n- [openai/gpt-4o-2024-05-13](/openai/gpt-4o-2024-05-13)\n- [openai/gpt-4o-2024-08-06](/openai/gpt-4o-2024-08-06)\n- [openai/gpt-4o-2024-11-20](/openai/gpt-4o-2024-11-20)\n- [openai/gpt-4o-mini](/openai/gpt-4o-mini)\n- [openai/gpt-4o-mini-2024-07-18](/openai/gpt-4o-mini-2024-07-18)\n- [openai/gpt-4-turbo](/openai/gpt-4-turbo)\n- [openai/gpt-4-turbo-preview](/openai/gpt-4-turbo-preview)\n- [openai/gpt-4-1106-preview](/openai/gpt-4-1106-preview)\n- [openai/gpt-4](/openai/gpt-4)\n- [openai/gpt-3.5-turbo](/openai/gpt-3.5-turbo)\n- [openai/gpt-oss-120b](/openai/gpt-oss-120b)\n- [anthropic/claude-opus-4.5](/anthropic/claude-opus-4.5)\n- [anthropic/claude-opus-4.1](/anthropic/claude-opus-4.1)\n- [anthropic/claude-opus-4](/anthropic/claude-opus-4)\n- [anthropic/claude-sonnet-4.5](/anthropic/claude-sonnet-4.5)\n- [anthropic/claude-sonnet-4](/anthropic/claude-sonnet-4)\n- [anthropic/claude-3.7-sonnet](/anthropic/claude-3.7-sonnet)\n- [anthropic/claude-haiku-4.5](/anthropic/claude-haiku-4.5)\n- [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku)\n- [anthropic/claude-3-haiku](/anthropic/claude-3-haiku)\n- [google/gemini-3-pro-preview](/google/gemini-3-pro-preview)\n- [google/gemini-2.5-pro](/google/gemini-2.5-pro)\n- [google/gemini-2.0-flash-001](/google/gemini-2.0-flash-001)\n- [google/gemini-2.5-flash](/google/gemini-2.5-flash)\n- [mistralai/mistral-large](/mistralai/mistral-large)\n- [mistralai/mistral-large-2407](/mistralai/mistral-large-2407)\n- [mistralai/mistral-large-2411](/mistralai/mistral-large-2411)\n- [mistralai/mistral-medium-3.1](/mistralai/mistral-medium-3.1)\n- [mistralai/mistral-nemo](/mistralai/mistral-nemo)\n- [mistralai/mistral-7b-instruct](/mistralai/mistral-7b-instruct)\n- [mistralai/mixtral-8x7b-instruct](/mistralai/mixtral-8x7b-instruct)\n- [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct)\n- [mistralai/codestral-2508](/mistralai/codestral-2508)\n- [x-ai/grok-4](/x-ai/grok-4)\n- [x-ai/grok-3](/x-ai/grok-3)\n- [x-ai/grok-3-mini](/x-ai/grok-3-mini)\n- [deepseek/deepseek-r1](/deepseek/deepseek-r1)\n- [meta-llama/llama-3.3-70b-instruct](/meta-llama/llama-3.3-70b-instruct)\n- [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct)\n- [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct)\n- [meta-llama/llama-3.1-8b-instruct](/meta-llama/llama-3.1-8b-instruct)\n- [meta-llama/llama-3-70b-instruct](/meta-llama/llama-3-70b-instruct)\n- [meta-llama/llama-3-8b-instruct](/meta-llama/llama-3-8b-instruct)\n- [qwen/qwen3-235b-a22b](/qwen/qwen3-235b-a22b)\n- [qwen/qwen3-32b](/qwen/qwen3-32b)\n- [qwen/qwen3-14b](/qwen/qwen3-14b)\n- [cohere/command-r-plus-08-2024](/cohere/command-r-plus-08-2024)\n- [cohere/command-r-08-2024](/cohere/command-r-08-2024)\n- [moonshotai/kimi-k2-thinking](/moonshotai/kimi-k2-thinking)\n- [perplexity/sonar](/perplexity/sonar)",
			ContextLenVal: 2000000,
			MaxOutputVal:  0,
			PriceInVal:    -1.000000,
			PriceOutVal:   -1.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "auto" },
		},
		"openrouter/bodybuilder": {
			IDVal:         "openrouter/bodybuilder",
			NameVal:       "Body Builder (beta)",
			ProviderVal:   "Openrouter",
			DescVal:       "Transform your natural language requests into structured OpenRouter API request objects. Describe what you want to accomplish with AI models, and Body Builder will construct the appropriate API calls. Example: \"count to 10 using gemini and opus.\"\n\nThis is useful for creating multi-model requests, custom model routers, or programmatic generation of API calls from human descriptions.\n\n**BETA NOTICE**: Body Builder is in beta, and currently free. Pricing and functionality may change in the future.",
			DescCNVal:     "将您的自然语言请求转换为结构化的 OpenRouter API 请求对象。只需描述您希望 AI 模型完成的任务，Body Builder 即可自动生成相应的 API 调用。例如：“使用 gemini 和 opus 从 1 数到 10。”\n\n此功能适用于创建多模型请求、自定义模型路由，或根据人类描述程序化生成 API 调用。\n\n**Beta 公告**：Body Builder 目前处于 Beta 阶段，免费使用。未来定价与功能可能发生变化。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    -1.000000,
			PriceOutVal:   -1.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "bodybuilder" },
		},
		"perplexity/sonar": {
			IDVal:         "perplexity/sonar",
			NameVal:       "Perplexity: Sonar",
			ProviderVal:   "Perplexity",
			DescVal:       "Sonar is lightweight, affordable, fast, and simple to use — now featuring citations and the ability to customize sources. It is designed for companies seeking to integrate lightweight question-and-answer features optimized for speed.",
			DescCNVal:     "Sonar 轻量、经济、快速且易于使用——现已支持引用来源并可自定义信息源。该模型专为希望集成轻量级、高速问答功能的企业设计。",
			ContextLenVal: 127072,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sonar" },
		},
		"perplexity/sonar-deep-research": {
			IDVal:         "perplexity/sonar-deep-research",
			NameVal:       "Perplexity: Sonar Deep Research",
			ProviderVal:   "Perplexity",
			DescVal:       "Sonar Deep Research is a research-focused model designed for multi-step retrieval, synthesis, and reasoning across complex topics. It autonomously searches, reads, and evaluates sources, refining its approach as it gathers information. This enables comprehensive report generation across domains like finance, technology, health, and current events.\n\nNotes on Pricing ([Source](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)) \n- Input tokens comprise of Prompt tokens (user prompt) + Citation tokens (these are processed tokens from running searches)\n- Deep Research runs multiple searches to conduct exhaustive research. Searches are priced at $5/1000 searches. A request that does 30 searches will cost $0.15 in this step.\n- Reasoning is a distinct step in Deep Research since it does extensive automated reasoning through all the material it gathers during its research phase. Reasoning tokens here are a bit different than the CoTs in the answer - these are tokens that we use to reason through the research material prior to generating the outputs via the CoTs. Reasoning tokens are priced at $3/1M tokens",
			DescCNVal:     "Sonar Deep Research 是一款专注于研究的模型，专为跨复杂主题的多步骤检索、信息整合与推理而设计。它能自主搜索、阅读并评估信息源，并在收集信息过程中不断优化策略，从而在金融、科技、健康和时事等领域生成全面的研究报告。\n\n定价说明（[来源](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)）：\n- 输入 token 包含提示 token（用户输入）和引用 token（执行搜索后处理的 token）\n- Deep Research 会执行多次搜索以完成深度研究，搜索费用为每 1000 次搜索 5 美元。例如，一次包含 30 次搜索的请求将产生 0.15 美元的搜索费用\n- 推理是 Deep Research 中的一个独立步骤，模型会对研究阶段收集的所有材料进行大量自动化推理。此处的推理 token 与最终答案中的思维链（CoT）不同，指的是在生成输出前用于分析研究材料的内部推理 token，定价为每百万 token 3 美元",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sonar-deep-research" },
		},
		"perplexity/sonar-pro": {
			IDVal:         "perplexity/sonar-pro",
			NameVal:       "Perplexity: Sonar Pro",
			ProviderVal:   "Perplexity",
			DescVal:       "Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\nFor enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions. ",
			DescCNVal:     "注意：Sonar Pro 的定价已包含 Perplexity 搜索费用。详见[此处说明](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\n面向需要更高级功能的企业用户，Sonar Pro API 可处理深入的多步骤查询，并具备更强的扩展性，例如平均每轮搜索可提供的引用数量是普通 Sonar 的两倍。此外，凭借更大的上下文窗口，它能够处理更长、更复杂的搜索请求及后续追问。",
			ContextLenVal: 200000,
			MaxOutputVal:  8000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sonar-pro" },
		},
		"perplexity/sonar-pro-search": {
			IDVal:         "perplexity/sonar-pro-search",
			NameVal:       "Perplexity: Sonar Pro Search",
			ProviderVal:   "Perplexity",
			DescVal:       "Exclusively available on the OpenRouter API, Sonar Pro's new Pro Search mode is Perplexity's most advanced agentic search system. It is designed for deeper reasoning and analysis. Pricing is based on tokens plus $18 per thousand requests. This model powers the Pro Search mode on the Perplexity platform.\n\nSonar Pro Search adds autonomous, multi-step reasoning to Sonar Pro. So, instead of just one query + synthesis, it plans and executes entire research workflows using tools.",
			DescCNVal:     "Sonar Pro 的全新 Pro Search 模式仅在 OpenRouter API 上提供，是 Perplexity 最先进的智能体搜索系统，专为深度推理与分析而设计。定价基于 token 数量外加每千次请求 18 美元。该模型驱动 Perplexity 平台上的 Pro Search 模式。\n\nSonar Pro Search 在 Sonar Pro 基础上增加了自主多步推理能力，不再局限于单次查询与结果整合，而是能够规划并执行完整的研究工作流，调用多种工具完成任务。",
			ContextLenVal: 200000,
			MaxOutputVal:  8000,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sonar-pro-search" },
		},
		"perplexity/sonar-reasoning-pro": {
			IDVal:         "perplexity/sonar-reasoning-pro",
			NameVal:       "Perplexity: Sonar Reasoning Pro",
			ProviderVal:   "Perplexity",
			DescVal:       "Note: Sonar Pro pricing includes Perplexity search pricing. See [details here](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\nSonar Reasoning Pro is a premier reasoning model powered by DeepSeek R1 with Chain of Thought (CoT). Designed for advanced use cases, it supports in-depth, multi-step queries with a larger context window and can surface more citations per search, enabling more comprehensive and extensible responses.",
			DescCNVal:     "注意：Sonar Pro 的定价已包含 Perplexity 搜索费用。详见[此处说明](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)\n\nSonar Reasoning Pro 是一款由 DeepSeek R1 驱动、采用思维链（Chain of Thought, CoT）技术的高端推理模型。专为高级应用场景设计，支持深入的多步骤查询，拥有更大的上下文窗口，并可在每次搜索中返回更多引用，从而生成更全面、更具扩展性的回答。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000008,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sonar-reasoning-pro" },
		},
		"prime-intellect/intellect-3": {
			IDVal:         "prime-intellect/intellect-3",
			NameVal:       "Prime Intellect: INTELLECT-3",
			ProviderVal:   "Prime-Intellect",
			DescVal:       "INTELLECT-3 is a 106B-parameter Mixture-of-Experts model (12B active) post-trained from GLM-4.5-Air-Base using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL). It offers state-of-the-art performance for its size across math, code, science, and general reasoning, consistently outperforming many larger frontier models. Designed for strong multi-step problem solving, it maintains high accuracy on structured tasks while remaining efficient at inference thanks to its MoE architecture.",
			DescCNVal:     "INTELLECT-3 是一款1060亿参数的混合专家（MoE）模型（激活参数约120亿），基于 GLM-4.5-Air-Base 经过监督微调（SFT）及大规模强化学习（RL）后训练而成。在数学、代码、科学及通用推理等任务上，其单位参数性能达到业界领先水平，持续超越众多更大规模的前沿模型。该模型专为强大多步问题求解设计，在结构化任务中保持高准确性，同时凭借 MoE 架构实现高效的推理性能。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "intellect-3" },
		},
		"qwen/qwen-2.5-72b-instruct": {
			IDVal:         "qwen/qwen-2.5-72b-instruct",
			NameVal:       "Qwen2.5 72B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
			DescCNVal:     "通义千问 2.5 72B 指令微调版。",
			ContextLenVal: 32768,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-2.5-72b", "qwen2.5", "qwen-2.5-72b-instruct" },
		},
		"qwen/qwen-2.5-7b-instruct": {
			IDVal:         "qwen/qwen-2.5-7b-instruct",
			NameVal:       "Qwen: Qwen2.5 7B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
			DescCNVal:     "Qwen2.5 7B 是通义千问大语言模型系列的最新版本，在 Qwen2 基础上带来以下改进：\n\n- 凭借领域专家模型，在知识储备、代码及数学能力方面显著增强。\n\n- 指令遵循能力大幅提升，支持生成长文本（超过 8K tokens），更好地理解结构化数据（如表格），并能高效生成结构化输出（尤其是 JSON）。对系统提示的多样性更具鲁棒性，显著提升聊天机器人的角色扮演和条件设定能力。\n\n- 支持最长 128K tokens 的上下文，最大生成长度达 8K tokens。\n\n- 支持 29 种以上语言，包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\n\n使用本模型需遵守 [通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-2.5-7b-instruct" },
		},
		"qwen/qwen-2.5-coder-32b-instruct": {
			IDVal:         "qwen/qwen-2.5-coder-32b-instruct",
			NameVal:       "Qwen2.5 Coder 32B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).",
			DescCNVal:     "Qwen2.5-Coder 是最新一代专注于代码的 Qwen 大语言模型系列（前身为 CodeQwen）。相较于 CodeQwen1.5，Qwen2.5-Coder 带来了以下改进：\n\n- 在**代码生成**、**代码推理**和**代码修复**方面显著提升；\n- 为实际应用场景（如**代码智能体**）提供了更全面的基础，不仅强化了编码能力，还保持了在数学和通用能力方面的优势。\n\n更多评测结果详见 [Qwen 2.5 Coder 博客](https://qwenlm.github.io/blog/qwen2.5-coder-family/)。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-2.5-coder-32b-instruct" },
		},
		"qwen/qwen-2.5-vl-7b-instruct": {
			IDVal:         "qwen/qwen-2.5-vl-7b-instruct",
			NameVal:       "Qwen: Qwen2.5-VL 7B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
			DescCNVal:     "Qwen2.5 VL 7B 是通义千问团队推出的多模态大语言模型，具备以下关键增强特性：\n\n- 多分辨率与比例图像的 SOTA 理解能力：Qwen2.5-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中达到业界领先水平。\n\n- 支持 20 分钟以上视频理解：可高质量完成基于长视频的问答、对话、内容创作等任务。\n\n- 可操作手机、机器人等设备的智能体：凭借强大的复杂推理与决策能力，Qwen2.5-VL 可集成至手机、机器人等设备，根据视觉环境与文本指令实现自动化操作。\n\n- 多语言支持：除英语和中文外，现支持识别图像内多种语言文字，包括主流欧洲语言、日语、韩语、阿拉伯语、越南语等，服务全球用户。\n\n更多详情请参见[博客文章](https://qwenlm.github.io/blog/qwen2-vl/)和[GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用本模型需遵守[通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-2.5-vl-7b-instruct" },
		},
		"qwen/qwen-2.5-vl-7b-instruct:free": {
			IDVal:         "qwen/qwen-2.5-vl-7b-instruct:free",
			NameVal:       "Qwen: Qwen2.5-VL 7B Instruct (free)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).",
			DescCNVal:     "Qwen2.5 VL 7B 是通义千问团队推出的多模态大语言模型，具备以下关键增强特性：\n\n- 多分辨率与比例图像的 SOTA 理解能力：Qwen2.5-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中达到业界领先水平。\n\n- 支持 20 分钟以上视频理解：可高质量完成基于长视频的问答、对话、内容创作等任务。\n\n- 可操作手机、机器人等设备的智能体：凭借强大的复杂推理与决策能力，Qwen2.5-VL 可集成至手机、机器人等设备，根据视觉环境与文本指令实现自动化操作。\n\n- 多语言支持：除英语和中文外，现支持识别图像内多种语言文字，包括主流欧洲语言、日语、韩语、阿拉伯语、越南语等，服务全球用户。\n\n更多详情请参见[博客文章](https://qwenlm.github.io/blog/qwen2-vl/)和[GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用本模型需遵守[通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-2.5-vl-7b-instruct:free" },
		},
		"qwen/qwen-max": {
			IDVal:         "qwen/qwen-max",
			NameVal:       "Qwen: Qwen-Max ",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.",
			DescCNVal:     "Qwen-Max 基于 Qwen2.5 构建，是 [Qwen 系列模型](/qwen) 中推理性能最强的版本，尤其擅长复杂的多步骤任务。该大规模 MoE 模型在超过 20 万亿 token 上完成预训练，并进一步通过精选的监督微调（SFT）与人类反馈强化学习（RLHF）方法进行后训练。具体参数量未公开。",
			ContextLenVal: 32768,
			MaxOutputVal:  8192,
			PriceInVal:    0.000002,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-max" },
		},
		"qwen/qwen-plus": {
			IDVal:         "qwen/qwen-plus",
			NameVal:       "Qwen: Qwen-Plus",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination.",
			DescCNVal:     "Qwen-Plus 基于 Qwen2.5 基础模型，上下文长度达 131K，兼顾性能、速度与成本。",
			ContextLenVal: 131072,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-plus" },
		},
		"qwen/qwen-plus-2025-07-28": {
			IDVal:         "qwen/qwen-plus-2025-07-28",
			NameVal:       "Qwen: Qwen Plus 0728",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.",
			DescCNVal:     "Qwen Plus 0728 基于 Qwen3 基础模型，是一款支持百万上下文的混合推理模型，在性能、速度与成本之间取得均衡。",
			ContextLenVal: 1000000,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-plus-2025-07-28" },
		},
		"qwen/qwen-plus-2025-07-28:thinking": {
			IDVal:         "qwen/qwen-plus-2025-07-28:thinking",
			NameVal:       "Qwen: Qwen Plus 0728 (thinking)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.",
			DescCNVal:     "Qwen Plus 0728 基于 Qwen3 基础模型，是一款支持百万上下文的混合推理模型，在性能、速度与成本之间取得均衡。",
			ContextLenVal: 1000000,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000004,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-plus-2025-07-28:thinking" },
		},
		"qwen/qwen-turbo": {
			IDVal:         "qwen/qwen-turbo",
			NameVal:       "Qwen: Qwen-Turbo",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.",
			DescCNVal:     "Qwen-Turbo 基于 Qwen2.5，上下文长度达 1M，具备高速度与低成本特性，适用于简单任务。",
			ContextLenVal: 1000000,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-turbo" },
		},
		"qwen/qwen-vl-max": {
			IDVal:         "qwen/qwen-vl-max",
			NameVal:       "Qwen: Qwen VL Max",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.\n",
			DescCNVal:     "Qwen VL Max 是一款视觉理解模型，上下文长度达 7500 个 token，在更广泛的复杂任务中提供卓越性能。",
			ContextLenVal: 131072,
			MaxOutputVal:  8192,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-vl-max" },
		},
		"qwen/qwen-vl-plus": {
			IDVal:         "qwen/qwen-vl-plus",
			NameVal:       "Qwen: Qwen VL Plus",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.\n",
			DescCNVal:     "Qwen 增强版大视觉语言模型，显著提升了细节识别与文本识别能力，支持高达百万像素级的超高清分辨率及极端宽高比的图像输入，在各类视觉任务中均展现出显著性能优势。",
			ContextLenVal: 7500,
			MaxOutputVal:  1500,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen-vl-plus" },
		},
		"qwen/qwen2.5-coder-7b-instruct": {
			IDVal:         "qwen/qwen2.5-coder-7b-instruct",
			NameVal:       "Qwen: Qwen2.5 Coder 7B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model optimized for code-related tasks such as code generation, reasoning, and bug fixing. Based on the Qwen2.5 architecture, it incorporates enhancements like RoPE, SwiGLU, RMSNorm, and GQA attention with support for up to 128K tokens using YaRN-based extrapolation. It is trained on a large corpus of source code, synthetic data, and text-code grounding, providing robust performance across programming languages and agentic coding workflows.\n\nThis model is part of the Qwen2.5-Coder family and offers strong compatibility with tools like vLLM for efficient deployment. Released under the Apache 2.0 license.",
			DescCNVal:     "Qwen2.5-Coder-7B-Instruct 是一款拥有 70 亿参数的指令微调语言模型，专为代码生成、推理和调试等任务优化。基于 Qwen2.5 架构，集成了 RoPE、SwiGLU、RMSNorm 和 GQA 注意力机制，并通过 YaRN 插值技术支持高达 128K token 的上下文长度。该模型在大规模源代码、合成数据及文本-代码对齐语料上训练，可在多种编程语言和智能体编码工作流中提供稳健性能。\n\n作为 Qwen2.5-Coder 系列的一员，该模型与 vLLM 等高效部署工具高度兼容，并以 Apache 2.0 许可证发布。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen2.5-coder-7b-instruct" },
		},
		"qwen/qwen2.5-vl-32b-instruct": {
			IDVal:         "qwen/qwen2.5-vl-32b-instruct",
			NameVal:       "Qwen: Qwen2.5 VL 32B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5-VL-32B is a multimodal vision-language model fine-tuned through reinforcement learning for enhanced mathematical reasoning, structured outputs, and visual problem-solving capabilities. It excels at visual analysis tasks, including object recognition, textual interpretation within images, and precise event localization in extended videos. Qwen2.5-VL-32B demonstrates state-of-the-art performance across multimodal benchmarks such as MMMU, MathVista, and VideoMME, while maintaining strong reasoning and clarity in text-based tasks like MMLU, mathematical problem-solving, and code generation.",
			DescCNVal:     "Qwen2.5-VL-32B 是一款通过强化学习微调的多模态视觉-语言模型，显著增强了数学推理、结构化输出及视觉问题求解能力。该模型在视觉分析任务中表现卓越，包括物体识别、图像内文本解析以及长视频中精确事件定位。在 MMMU、MathVista 和 VideoMME 等多模态基准测试中达到业界领先水平，同时在 MMLU、数学问题求解和代码生成等纯文本任务中也展现出强大的推理能力与清晰度。",
			ContextLenVal: 16384,
			MaxOutputVal:  16384,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen2.5-vl-32b-instruct" },
		},
		"qwen/qwen2.5-vl-72b-instruct": {
			IDVal:         "qwen/qwen2.5-vl-72b-instruct",
			NameVal:       "Qwen: Qwen2.5 VL 72B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen2.5-VL is proficient in recognizing common objects such as flowers, birds, fish, and insects. It is also highly capable of analyzing texts, charts, icons, graphics, and layouts within images.",
			DescCNVal:     "Qwen2.5-VL 擅长识别花卉、鸟类、鱼类和昆虫等常见物体，同时能高效分析图像中的文字、图表、图标、图形及版式布局。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen2.5-vl-72b-instruct" },
		},
		"qwen/qwen3-14b": {
			IDVal:         "qwen/qwen3-14b",
			NameVal:       "Qwen: Qwen3 14B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, programming, and logical inference, and a \"non-thinking\" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling.",
			DescCNVal:     "Qwen3-14B 是 Qwen3 系列中的稠密因果语言模型，参数量达 148 亿，兼顾复杂推理与高效对话。该模型支持在“思考”模式（适用于数学、编程和逻辑推理等任务）与“非思考”模式（适用于通用对话）之间无缝切换。模型经过指令遵循、智能体工具调用、创意写作及 100 多种语言和方言的多语言任务微调，原生支持 32K token 上下文，并可通过基于 YaRN 的扩展技术将上下文长度提升至 131K tokens。",
			ContextLenVal: 40960,
			MaxOutputVal:  40960,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-14b" },
		},
		"qwen/qwen3-235b-a22b": {
			IDVal:         "qwen/qwen3-235b-a22b",
			NameVal:       "Qwen: Qwen3 235B A22B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a \"thinking\" mode for complex reasoning, math, and code tasks, and a \"non-thinking\" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling.",
			DescCNVal:     "Qwen3-235B-A22B 是千问推出的 2350 亿参数稀疏专家混合（MoE）模型，每次前向传播激活 220 亿参数。该模型支持在“思考”模式（用于复杂推理、数学和代码任务）与“非思考”模式（用于高效通用对话）之间无缝切换，展现出强大的推理能力、多语言支持（覆盖 100 多种语言和方言）、高级指令遵循能力以及智能体工具调用功能。模型原生支持 32K token 上下文窗口，并可通过基于 YaRN 的扩展技术将上下文长度延伸至 131K tokens。",
			ContextLenVal: 40960,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-235b-a22b" },
		},
		"qwen/qwen3-235b-a22b-2507": {
			IDVal:         "qwen/qwen3-235b-a22b-2507",
			NameVal:       "Qwen: Qwen3 235B A22B Instruct 2507",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
			DescCNVal:     "Qwen3-235B-A22B-Instruct-2507 是基于 Qwen3-235B 架构的多语言指令微调混合专家语言模型，每次前向传播激活 220 亿参数。该模型针对通用文本生成任务优化，涵盖指令遵循、逻辑推理、数学、编程及工具调用等能力。模型原生支持 262K 令牌上下文长度，且未启用“思考模式”（即无 <think> 区块）。\n\n相较于基础版本，此版本在知识覆盖广度、长上下文推理、编程基准及开放式任务对齐方面均有显著提升。其在多语言理解、数学推理（如 AIME、HMMT）以及 Arena-Hard、WritingBench 等对齐评估中表现尤为突出。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-235b-a22b-2507" },
		},
		"qwen/qwen3-235b-a22b-thinking-2507": {
			IDVal:         "qwen/qwen3-235b-a22b-thinking-2507",
			NameVal:       "Qwen: Qwen3 235B A22B Thinking 2507",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass and natively supports up to 262,144 tokens of context. This \"thinking-only\" variant enhances structured logical reasoning, mathematics, science, and long-form generation, showing strong benchmark performance across AIME, SuperGPQA, LiveCodeBench, and MMLU-Redux. It enforces a special reasoning mode (</think>) and is designed for high-token outputs (up to 81,920 tokens) in challenging domains.\n\nThe model is instruction-tuned and excels at step-by-step reasoning, tool use, agentic workflows, and multilingual tasks. This release represents the most capable open-source variant in the Qwen3-235B series, surpassing many closed models in structured reasoning use cases.",
			DescCNVal:     "Qwen3-235B-A22B-Thinking-2507 是一款高性能、开源权重的混合专家（MoE）语言模型，专为复杂推理任务优化。每次前向传播激活其 2350 亿参数中的 220 亿，并原生支持最多 262,144 个上下文 token。该“纯推理”变体在结构化逻辑推理、数学、科学及长文本生成方面表现卓越，在 AIME、SuperGPQA、LiveCodeBench 和 MMLU-Redux 等基准测试中成绩优异。模型强制启用特殊推理模式（</think>），并针对高 token 输出（最高达 81,920 个 token）的挑战性领域进行优化。\n\n该模型经过指令微调，在逐步推理、工具调用、智能体工作流及多语言任务方面表现出色。此版本是 Qwen3-235B 系列中最强大的开源变体，在结构化推理应用场景中超越众多闭源模型。",
			ContextLenVal: 262144,
			MaxOutputVal:  262144,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-235b-a22b-thinking-2507" },
		},
		"qwen/qwen3-30b-a3b": {
			IDVal:         "qwen/qwen3-30b-a3b",
			NameVal:       "Qwen: Qwen3 30B A3B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.\n\nSignificantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models.",
			DescCNVal:     "Qwen3 是千问大语言模型系列的最新一代，同时提供稠密架构与专家混合（MoE）架构，在推理能力、多语言支持和高级智能体任务方面表现卓越。其独特之处在于可无缝切换“思考”模式（用于复杂推理）与“非思考”模式（用于高效对话），确保多功能、高质量的性能表现。\n\n相比 QwQ 和 Qwen2.5 等前代模型，Qwen3 在数学、编程、常识推理、创意写作和交互式对话等方面显著提升。Qwen3-30B-A3B 变体包含 305 亿总参数（每任务激活 33 亿参数）、48 层网络结构、128 个专家（每任务激活 8 个），并借助 YaRN 技术支持高达 131K token 的上下文长度，为开源模型树立了新标杆。",
			ContextLenVal: 40960,
			MaxOutputVal:  40960,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-30b-a3b" },
		},
		"qwen/qwen3-30b-a3b-instruct-2507": {
			IDVal:         "qwen/qwen3-30b-a3b-instruct-2507",
			NameVal:       "Qwen: Qwen3 30B A3B Instruct 2507",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quality instruction following, multilingual understanding, and agentic tool use. Post-trained on instruction data, it demonstrates competitive performance across reasoning (AIME, ZebraLogic), coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, WritingBench) benchmarks. It outperforms its non-instruct variant on subjective and open-ended tasks while retaining strong factual and coding performance.",
			DescCNVal:     "Qwen3-30B-A3B-Instruct-2507 是千问推出的 305 亿参数混合专家（MoE）语言模型，每次推理激活 33 亿参数。该模型运行于非推理模式，专为高质量指令遵循、多语言理解及智能体工具调用而设计。经指令数据后训练，其在推理（AIME、ZebraLogic）、代码（MultiPL-E、LiveCodeBench）和对齐（IFEval、WritingBench）等基准测试中表现优异。相比非指令微调版本，该模型在主观性和开放式任务上表现更佳，同时保持强大的事实性和代码能力。",
			ContextLenVal: 262144,
			MaxOutputVal:  262144,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-30b-a3b-instruct-2507" },
		},
		"qwen/qwen3-30b-a3b-thinking-2507": {
			IDVal:         "qwen/qwen3-30b-a3b-thinking-2507",
			NameVal:       "Qwen: Qwen3 30B A3B Thinking 2507",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning model optimized for complex tasks requiring extended multi-step thinking. The model is designed specifically for “thinking mode,” where internal reasoning traces are separated from final answers.\n\nCompared to earlier Qwen3-30B releases, this version improves performance across logical reasoning, mathematics, science, coding, and multilingual benchmarks. It also demonstrates stronger instruction following, tool use, and alignment with human preferences. With higher reasoning efficiency and extended output budgets, it is best suited for advanced research, competitive problem solving, and agentic applications requiring structured long-context reasoning.",
			DescCNVal:     "Qwen3-30B-A3B-Thinking-2507 是一款 300 亿参数的专家混合（MoE）推理模型，专为需要扩展多步思考的复杂任务而优化。该模型专为“思考模式”设计，将内部推理轨迹与最终答案分离。\n\n相较于早期 Qwen3-30B 版本，此版本在逻辑推理、数学、科学、编程及多语言基准测试中均有性能提升，同时展现出更强的指令遵循能力、工具使用能力以及与人类偏好的对齐度。凭借更高的推理效率和更长的输出预算，该模型特别适用于高级研究、竞赛级问题求解以及需要结构化长上下文推理的智能体应用。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-30b-a3b-thinking-2507" },
		},
		"qwen/qwen3-32b": {
			IDVal:         "qwen/qwen3-32b",
			NameVal:       "Qwen: Qwen3 32B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a \"thinking\" mode for tasks like math, coding, and logical inference, and a \"non-thinking\" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. ",
			DescCNVal:     "Qwen3-32B 是 Qwen3 系列中的稠密因果语言模型，参数量达 328 亿，专为复杂推理与高效对话双重目标优化。该模型支持在“思考”模式（用于数学、编程和逻辑推理等任务）与“非思考”模式（用于更快速的通用对话）之间无缝切换。模型在指令遵循、智能体工具调用、创意写作及 100 多种语言和方言的多语言任务中均表现出色，原生支持 32K token 上下文，并可通过基于 YaRN 的扩展技术将上下文长度提升至 131K tokens。",
			ContextLenVal: 40960,
			MaxOutputVal:  40960,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-32b" },
		},
		"qwen/qwen3-4b:free": {
			IDVal:         "qwen/qwen3-4b:free",
			NameVal:       "Qwen: Qwen3 4B (free)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.",
			DescCNVal:     "Qwen3-4B 是 Qwen3 系列中的 40 亿参数稠密语言模型，兼顾通用任务与高推理强度任务。该模型引入双模架构——“思考”模式与“非思考”模式，可动态切换于高精度逻辑推理与高效对话生成之间，非常适合多轮对话、指令遵循及复杂智能体工作流场景。",
			ContextLenVal: 40960,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-4b:free" },
		},
		"qwen/qwen3-8b": {
			IDVal:         "qwen/qwen3-8b",
			NameVal:       "Qwen: Qwen3 8B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between \"thinking\" mode for math, coding, and logical inference, and \"non-thinking\" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling.",
			DescCNVal:     "Qwen3-8B 是 Qwen3 系列中的稠密因果语言模型，参数量达 82 亿，兼顾高推理负载任务与高效对话。该模型支持在“思考”模式（用于数学、编程和逻辑推理）与“非思考”模式（用于通用对话）之间无缝切换。模型经过指令遵循、智能体集成、创意写作及 100 多种语言和方言的多语言任务微调，原生支持 32K token 上下文窗口，并可通过 YaRN 扩展技术将上下文长度提升至 131K tokens。",
			ContextLenVal: 32000,
			MaxOutputVal:  8192,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-8b" },
		},
		"qwen/qwen3-coder": {
			IDVal:         "qwen/qwen3-coder",
			NameVal:       "Qwen: Qwen3 Coder 480B A35B",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
			DescCNVal:     "Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。\n\n阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。",
			ContextLenVal: 262144,
			MaxOutputVal:  262144,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder" },
		},
		"qwen/qwen3-coder-30b-a3b-instruct": {
			IDVal:         "qwen/qwen3-coder-30b-a3b-instruct",
			NameVal:       "Qwen: Qwen3 Coder 30B A3B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\n\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. ",
			DescCNVal:     "Qwen3-Coder-30B-A3B-Instruct 是一款 305 亿参数的混合专家（MoE）模型，包含 128 个专家（每次前向传播激活 8 个），专为高级代码生成、仓库级理解及智能体工具调用而设计。基于 Qwen3 架构，原生支持 256K token 上下文长度（通过 Yarn 可扩展至 1M），在函数调用、浏览器操作和结构化代码补全等任务中表现强劲。\n\n该模型针对无“推理模式”的指令遵循进行优化，并与 OpenAI 兼容的工具调用格式良好集成。",
			ContextLenVal: 160000,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder-30b-a3b-instruct" },
		},
		"qwen/qwen3-coder-flash": {
			IDVal:         "qwen/qwen3-coder-flash",
			NameVal:       "Qwen: Qwen3 Coder Flash",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.",
			DescCNVal:     "Qwen3 Coder Flash 是阿里巴巴推出的 Qwen3 Coder Plus 的快速且高性价比版本。该模型是一款强大的编程智能体，专注于通过工具调用与环境交互实现自主编程，兼具卓越的编码能力与通用任务处理能力。",
			ContextLenVal: 128000,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder-flash" },
		},
		"qwen/qwen3-coder-plus": {
			IDVal:         "qwen/qwen3-coder-plus",
			NameVal:       "Qwen: Qwen3 Coder Plus",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder 480B A35B. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.",
			DescCNVal:     "Qwen3 Coder Plus 是阿里巴巴基于开源 Qwen3 Coder 480B A35B 打造的专有版本，是一款强大的编码智能体模型，专注于通过工具调用和环境交互实现自主编程，兼具卓越的编码能力与通用任务处理能力。",
			ContextLenVal: 128000,
			MaxOutputVal:  65536,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000005,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder-plus" },
		},
		"qwen/qwen3-coder:exacto": {
			IDVal:         "qwen/qwen3-coder:exacto",
			NameVal:       "Qwen: Qwen3 Coder 480B A35B (exacto)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
			DescCNVal:     "Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。\n\n阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。",
			ContextLenVal: 262144,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder:exacto" },
		},
		"qwen/qwen3-coder:free": {
			IDVal:         "qwen/qwen3-coder:free",
			NameVal:       "Qwen: Qwen3 Coder 480B A35B (free)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\n\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
			DescCNVal:     "Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。\n\n阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。",
			ContextLenVal: 262000,
			MaxOutputVal:  262000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-coder:free" },
		},
		"qwen/qwen3-max": {
			IDVal:         "qwen/qwen3-max",
			NameVal:       "Qwen: Qwen3 Max",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Max is an updated release built on the Qwen3 series, offering major improvements in reasoning, instruction following, multilingual support, and long-tail knowledge coverage compared to the January 2025 version. It delivers higher accuracy in math, coding, logic, and science tasks, follows complex instructions in Chinese and English more reliably, reduces hallucinations, and produces higher-quality responses for open-ended Q&A, writing, and conversation. The model supports over 100 languages with stronger translation and commonsense reasoning, and is optimized for retrieval-augmented generation (RAG) and tool calling, though it does not include a dedicated “thinking” mode.",
			DescCNVal:     "Qwen3-Max 是 Qwen3 系列的最新版本，相较 2025 年 1 月版，在推理能力、指令遵循、多语言支持及长尾知识覆盖方面均有显著提升。它在数学、编程、逻辑和科学任务中精度更高，能更可靠地理解中英文复杂指令，减少幻觉现象，并在开放式问答、写作和对话中生成更高质量的回答。该模型支持 100 多种语言，具备更强的翻译能力和常识推理能力，并针对检索增强生成（RAG）和工具调用进行了优化，但未包含专用的“思考”模式。",
			ContextLenVal: 256000,
			MaxOutputVal:  32768,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000006,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-max" },
		},
		"qwen/qwen3-next-80b-a3b-instruct": {
			IDVal:         "qwen/qwen3-next-80b-a3b-instruct",
			NameVal:       "Qwen: Qwen3 Next 80B A3B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.",
			DescCNVal:     "Qwen3-Next-80B-A3B-Instruct 是 Qwen3-Next 系列中的指令微调对话模型，专为快速、稳定响应而优化，不输出“思考”痕迹。该模型面向复杂任务场景，涵盖推理、代码生成、知识问答及多语言应用，同时在对齐性与格式规范方面保持稳健。相较早期 Qwen3 指令模型，本版本聚焦于超长输入与多轮对话下的更高吞吐量与稳定性，特别适用于需一致最终答案而非显式思维链的 RAG、工具调用及智能体工作流。\n\n模型采用高效扩展的训练与解码策略，提升参数效率与推理速度，并在广泛的公开基准测试中验证：在多个类别上达到或接近更大规模 Qwen3 系统的水平，同时显著优于早期中等规模基线。该模型最适合用于生产环境中需要确定性、严格遵循指令输出的通用助手、代码辅助及长上下文任务求解场景。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-next-80b-a3b-instruct" },
		},
		"qwen/qwen3-next-80b-a3b-instruct:free": {
			IDVal:         "qwen/qwen3-next-80b-a3b-instruct:free",
			NameVal:       "Qwen: Qwen3 Next 80B A3B Instruct (free)",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.",
			DescCNVal:     "Qwen3-Next-80B-A3B-Instruct 是 Qwen3-Next 系列中的指令微调对话模型，专为快速、稳定响应而优化，不输出“思考”痕迹。该模型面向复杂任务场景，涵盖推理、代码生成、知识问答及多语言应用，同时在对齐性与格式规范方面保持稳健。相较早期 Qwen3 指令模型，本版本聚焦于超长输入与多轮对话下的更高吞吐量与稳定性，特别适用于需一致最终答案而非显式思维链的 RAG、工具调用及智能体工作流。\n\n模型采用高效扩展的训练与解码策略，提升参数效率与推理速度，并在广泛的公开基准测试中验证：在多个类别上达到或接近更大规模 Qwen3 系统的水平，同时显著优于早期中等规模基线。该模型最适合用于生产环境中需要确定性、严格遵循指令输出的通用助手、代码辅助及长上下文任务求解场景。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-next-80b-a3b-instruct:free" },
		},
		"qwen/qwen3-next-80b-a3b-thinking": {
			IDVal:         "qwen/qwen3-next-80b-a3b-thinking",
			NameVal:       "Qwen: Qwen3 Next 80B A3B Thinking",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code synthesis/debugging, logic, and agentic planning, and reports strong results across knowledge, reasoning, coding, alignment, and multilingual evaluations. Compared with prior Qwen3 variants, it emphasizes stability under long chains of thought and efficient scaling during inference, and it is tuned to follow complex instructions while reducing repetitive or off-task behavior.\n\nThe model is suitable for agent frameworks and tool use (function calling), retrieval-heavy workflows, and standardized benchmarking where step-by-step solutions are required. It supports long, detailed completions and leverages throughput-oriented techniques (e.g., multi-token prediction) for faster generation. Note that it operates in thinking-only mode.",
			DescCNVal:     "Qwen3-Next-80B-A3B-Thinking 是 Qwen3-Next 系列中以推理优先的对话模型，默认输出结构化的“思考”轨迹。该模型专为高难度多步问题设计，包括数学证明、代码生成/调试、逻辑推理及智能体规划，在知识、推理、编程、对齐性与多语言评估中均表现强劲。相较早期 Qwen3 变体，本模型强调长思维链下的稳定性与推理阶段的高效扩展能力，并经过调优以更好遵循复杂指令，减少重复或偏离任务的行为。\n\n该模型适用于智能体框架与工具调用（函数调用）、重度检索工作流及需逐步解答的标准基准测试场景。支持生成长篇、详尽的回答，并采用面向吞吐量的技术（如多token预测）加速生成。请注意，该模型仅运行于纯思考模式。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-next-80b-a3b-thinking" },
		},
		"qwen/qwen3-vl-235b-a22b-instruct": {
			IDVal:         "qwen/qwen3-vl-235b-a22b-instruct",
			NameVal:       "Qwen: Qwen3 VL 235B A22B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.",
			DescCNVal:     "Qwen3-VL-235B-A22B Instruct 是一款开源多模态模型，融合了强大的文本生成能力与对图像和视频的视觉理解能力。Instruct 版本面向通用视觉-语言应用场景（如视觉问答、文档解析、图表/表格提取、多语言 OCR）。该系列强调鲁棒感知能力（识别多样化的现实与合成类别）、空间理解能力（2D/3D 定位）以及长篇幅视觉理解能力，在公开多模态基准测试中，其感知与推理性能均具竞争力。\n\n除分析能力外，Qwen3-VL 还支持智能体交互与工具调用：可在多图像、多轮对话中执行复杂指令；将文本对齐至视频时间轴以实现精确时序查询；并可操作 GUI 元素完成自动化任务。该模型还支持可视化编码工作流——将草图或原型转化为代码，并辅助 UI 调试，同时保持与旗舰版 Qwen3 语言模型相当的纯文本性能。因此，Qwen3-VL 适用于涵盖文档 AI、多语言 OCR、软件/UI 辅助、空间/具身任务及视觉-语言智能体研究等生产场景。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-235b-a22b-instruct" },
		},
		"qwen/qwen3-vl-235b-a22b-thinking": {
			IDVal:         "qwen/qwen3-vl-235b-a22b-thinking",
			NameVal:       "Qwen: Qwen3 VL 235B A22B Thinking",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-235B-A22B Thinking is a multimodal model that unifies strong text generation with visual understanding across images and video. The Thinking model is optimized for multimodal reasoning in STEM and math. The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows, turning sketches or mockups into code and assisting with UI debugging, while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.",
			DescCNVal:     "Qwen3-VL-235B-A22B Thinking 是一款多模态模型，融合了强大的文本生成能力与对图像和视频的视觉理解能力。Thinking 版本专为 STEM 和数学领域的多模态推理优化。该系列强调鲁棒感知能力（识别多样化的现实与合成类别）、空间理解能力（2D/3D 定位）以及长篇幅视觉理解能力，在公开多模态基准测试中，其感知与推理性能均具竞争力。\n\n除分析能力外，Qwen3-VL 还支持智能体交互与工具调用：可在多图像、多轮对话中执行复杂指令；将文本对齐至视频时间轴以实现精确时序查询；并可操作 GUI 元素完成自动化任务。该模型还支持可视化编码工作流——将草图或原型转化为代码，并辅助 UI 调试，同时保持与旗舰版 Qwen3 语言模型相当的纯文本性能。因此，Qwen3-VL 适用于涵盖文档 AI、多语言 OCR、软件/UI 辅助、空间/具身任务及视觉-语言智能体研究等生产场景。",
			ContextLenVal: 131072,
			MaxOutputVal:  32768,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000004,
			Features:      ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-235b-a22b-thinking" },
		},
		"qwen/qwen3-vl-30b-a3b-instruct": {
			IDVal:         "qwen/qwen3-vl-30b-a3b-instruct",
			NameVal:       "Qwen: Qwen3 VL 30B A3B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-30B-A3B-Instruct is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Instruct variant optimizes instruction-following for general multimodal tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.",
			DescCNVal:     "Qwen3-VL-30B-A3B-Instruct 是一款多模态模型，融合强大的文本生成能力与对图像和视频的视觉理解能力。其 Instruct 变体针对通用多模态任务的指令遵循能力进行了优化。该模型在真实/合成类别感知、2D/3D 空间定位及长篇视觉理解方面表现出色，在多模态基准测试中成绩优异。在智能体应用中，可处理多图像多轮指令、视频时间轴对齐、GUI 自动化，以及从草图到调试完成 UI 的可视化编程。其文本性能与旗舰 Qwen3 系列相当，适用于文档 AI、OCR、UI 辅助、空间任务及智能体研究。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-30b-a3b-instruct" },
		},
		"qwen/qwen3-vl-30b-a3b-thinking": {
			IDVal:         "qwen/qwen3-vl-30b-a3b-thinking",
			NameVal:       "Qwen: Qwen3 VL 30B A3B Thinking",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-30B-A3B-Thinking is a multimodal model that unifies strong text generation with visual understanding for images and videos. Its Thinking variant enhances reasoning in STEM, math, and complex tasks. It excels in perception of real-world/synthetic categories, 2D/3D spatial grounding, and long-form visual comprehension, achieving competitive multimodal benchmark results. For agentic use, it handles multi-image multi-turn instructions, video timeline alignments, GUI automation, and visual coding from sketches to debugged UI. Text performance matches flagship Qwen3 models, suiting document AI, OCR, UI assistance, spatial tasks, and agent research.",
			DescCNVal:     "Qwen3-VL-30B-A3B-Thinking 是一款多模态模型，融合强大的文本生成能力与对图像和视频的视觉理解能力。其 Thinking 变体强化了在 STEM、数学及复杂任务中的推理能力。该模型在真实/合成类别感知、2D/3D 空间定位及长篇视觉理解方面表现出色，在多模态基准测试中成绩优异。在智能体应用中，可处理多图像多轮指令、视频时间轴对齐、GUI 自动化，以及从草图到调试完成 UI 的可视化编程。其文本性能与旗舰 Qwen3 系列相当，适用于文档 AI、OCR、UI 辅助、空间任务及智能体研究。",
			ContextLenVal: 131072,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-30b-a3b-thinking" },
		},
		"qwen/qwen3-vl-32b-instruct": {
			IDVal:         "qwen/qwen3-vl-32b-instruct",
			NameVal:       "Qwen: Qwen3 VL 32B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-32B-Instruct is a large-scale multimodal vision-language model designed for high-precision understanding and reasoning across text, images, and video. With 32 billion parameters, it combines deep visual perception with advanced text comprehension, enabling fine-grained spatial reasoning, document and scene analysis, and long-horizon video understanding.Robust OCR in 32 languages, and enhanced multimodal fusion through Interleaved-MRoPE and DeepStack architectures. Optimized for agentic interaction and visual tool use, Qwen3-VL-32B delivers state-of-the-art performance for complex real-world multimodal tasks.",
			DescCNVal:     "Qwen3-VL-32B-Instruct 是一款大规模多模态视觉语言模型，专为文本、图像和视频的高精度理解与推理而设计。该模型拥有 320 亿参数，融合深度视觉感知与先进文本理解能力，支持细粒度空间推理、文档与场景分析以及长周期视频理解。支持 32 种语言的鲁棒 OCR，并通过 Interleaved-MRoPE 与 DeepStack 架构实现增强的多模态融合。该模型针对智能体交互和视觉工具调用进行了优化，在复杂现实世界的多模态任务中达到业界领先水平。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-32b-instruct" },
		},
		"qwen/qwen3-vl-8b-instruct": {
			IDVal:         "qwen/qwen3-vl-8b-instruct",
			NameVal:       "Qwen: Qwen3 VL 8B Instruct",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization.\n\nThe model supports a native 256K-token context window, extensible to 1M tokens, and handles both static and dynamic media inputs for tasks like document parsing, visual question answering, spatial reasoning, and GUI control. It achieves text understanding comparable to leading LLMs while expanding OCR coverage to 32 languages and enhancing robustness under varied visual conditions.",
			DescCNVal:     "Qwen3-VL-8B-Instruct 是 Qwen3-VL 系列中的多模态视觉语言模型，专为跨文本、图像和视频的高保真理解与推理而构建。该模型采用改进的多模态融合技术，包括用于长时序推理的 Interleaved-MRoPE、用于细粒度视觉-文本对齐的 DeepStack，以及用于精确事件定位的文本-时间戳对齐机制。\n\n该模型原生支持 256K token 上下文窗口，可扩展至 1M tokens，能够处理静态与动态媒体输入，适用于文档解析、视觉问答、空间推理和 GUI 控制等任务。其文本理解能力媲美主流大语言模型，OCR 支持语言扩展至 32 种，并在多样化视觉条件下展现出更强鲁棒性。",
			ContextLenVal: 131072,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-8b-instruct" },
		},
		"qwen/qwen3-vl-8b-thinking": {
			IDVal:         "qwen/qwen3-vl-8b-thinking",
			NameVal:       "Qwen: Qwen3 VL 8B Thinking",
			ProviderVal:   "Qwen",
			DescVal:       "Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designed for advanced visual and textual reasoning across complex scenes, documents, and temporal sequences. It integrates enhanced multimodal alignment and long-context processing (native 256K, expandable to 1M tokens) for tasks such as scientific visual analysis, causal inference, and mathematical reasoning over image or video inputs.\n\nCompared to the Instruct edition, the Thinking version introduces deeper visual-language fusion and deliberate reasoning pathways that improve performance on long-chain logic tasks, STEM problem-solving, and multi-step video understanding. It achieves stronger temporal grounding via Interleaved-MRoPE and timestamp-aware embeddings, while maintaining robust OCR, multilingual comprehension, and text generation on par with large text-only LLMs.",
			DescCNVal:     "Qwen3-VL-8B-Thinking 是 Qwen3-VL-8B 多模态模型的推理优化版本，专为复杂场景、文档和时序序列中的高级视觉与文本推理而设计。该模型集成增强的多模态对齐机制与长上下文处理能力（原生 256K，可扩展至 1M tokens），适用于科学视觉分析、因果推断以及基于图像或视频输入的数学推理等任务。\n\n相比 Instruct 版本，Thinking 版本引入了更深层次的视觉-语言融合与深思熟虑的推理路径，在长链逻辑任务、STEM 问题求解和多步视频理解方面表现更优。通过 Interleaved-MRoPE 和时间戳感知嵌入，该模型实现了更强的时间定位能力，同时保持与大型纯文本 LLM 相当的 OCR、多语言理解及文本生成能力。",
			ContextLenVal: 256000,
			MaxOutputVal:  32768,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwen3-vl-8b-thinking" },
		},
		"qwen/qwq-32b": {
			IDVal:         "qwen/qwq-32b",
			NameVal:       "Qwen: QwQ 32B",
			ProviderVal:   "Qwen",
			DescVal:       "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
			DescCNVal:     "QwQ 是通义千问（Qwen）系列中的推理专用模型。相较于传统的指令微调模型，具备思考与推理能力的 QwQ 在下游任务（尤其是难题）上性能显著提升。QwQ-32B 是其中的中等规模推理模型，在性能上可与当前领先的推理模型（如 DeepSeek-R1、o1-mini）相媲美。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "qwq-32b" },
		},
		"raifle/sorcererlm-8x22b": {
			IDVal:         "raifle/sorcererlm-8x22b",
			NameVal:       "SorcererLM 8x22B",
			ProviderVal:   "Raifle",
			DescVal:       "SorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b).\n\n- Advanced reasoning and emotional intelligence for engaging and immersive interactions\n- Vivid writing capabilities enriched with spatial and contextual awareness\n- Enhanced narrative depth, promoting creative and dynamic storytelling",
			DescCNVal:     "SorcererLM 是一款先进的角色扮演与叙事模型，基于 [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) 采用低秩 16 位 LoRA 微调而成。\n\n- 具备高级推理与情感智能，可实现引人入胜的沉浸式交互；\n- 写作风格生动，融合空间感知与上下文意识；\n- 叙事深度增强，支持富有创意且动态变化的故事创作。",
			ContextLenVal: 16000,
			MaxOutputVal:  0,
			PriceInVal:    0.000005,
			PriceOutVal:   0.000005,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "sorcererlm-8x22b" },
		},
		"relace/relace-apply-3": {
			IDVal:         "relace/relace-apply-3",
			NameVal:       "Relace: Relace Apply 3",
			ProviderVal:   "Relace",
			DescVal:       "Relace Apply 3 is a specialized code-patching LLM that merges AI-suggested edits straight into your source files. It can apply updates from GPT-4o, Claude, and others into your files at 10,000 tokens/sec on average.\n\nThe model requires the prompt to be in the following format: \n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nZero Data Retention is enabled for Relace. Learn more about this model in their [documentation](https://docs.relace.ai/api-reference/instant-apply/apply)",
			DescCNVal:     "Relace Apply 3 是一款专用的代码修补大语言模型，可直接将 AI 建议的编辑内容合并到源文件中。它平均以每秒 10,000 个 token 的速度将 GPT-4o、Claude 等模型的更新应用到您的文件中。\n\n该模型要求提示格式如下：\n<instruction>{instruction}</instruction>\n<code>{initial_code}</code>\n<update>{edit_snippet}</update>\n\nRelace 已启用零数据留存策略。更多详情请参阅其[文档](https://docs.relace.ai/api-reference/instant-apply/apply)。",
			ContextLenVal: 256000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "relace-apply-3" },
		},
		"relace/relace-search": {
			IDVal:         "relace/relace-search",
			NameVal:       "Relace: Relace Search",
			ProviderVal:   "Relace",
			DescVal:       "The relace-search model uses 4-12 `view_file` and `grep` tools in parallel to explore a codebase and return relevant files to the user request. \n\nIn contrast to RAG, relace-search performs agentic multi-step reasoning to produce highly precise results 4x faster than any frontier model. It's designed to serve as a subagent that passes its findings to an \"oracle\" coding agent, who orchestrates/performs the rest of the coding task.\n\nTo use relace-search you need to build an appropriate agent harness, and parse the response for relevant information to hand off to the oracle. Read more about it in the [Relace documentation](https://docs.relace.ai/docs/fast-agentic-search/agent).",
			DescCNVal:     "relace-search 模型通过并行调用 4–12 个 `view_file` 和 `grep` 工具来探索代码库，并返回与用户请求相关的文件。\n\n与 RAG 不同，relace-search 采用智能体多步推理机制，生成高度精准的结果，速度比任何前沿模型快 4 倍。该模型设计为子智能体，将其发现传递给“预言机”编程智能体，由后者协调并执行后续编码任务。\n\n使用 relace-search 需构建合适的智能体框架，并解析其响应以提取相关信息交予预言机。更多详情请参阅 [Relace 文档](https://docs.relace.ai/docs/fast-agentic-search/agent)。",
			ContextLenVal: 256000,
			MaxOutputVal:  128000,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "relace-search" },
		},
		"sao10k/l3-euryale-70b": {
			IDVal:         "sao10k/l3-euryale-70b",
			NameVal:       "Sao10k: Llama 3 Euryale 70B v2.1",
			ProviderVal:   "Sao10k",
			DescVal:       "Euryale 70B v2.1 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k).\n\n- Better prompt adherence.\n- Better anatomy / spatial awareness.\n- Adapts much better to unique and custom formatting / reply formats.\n- Very creative, lots of unique swipes.\n- Is not restrictive during roleplays.",
			DescCNVal:     "Euryale 70B v2.1 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型。\n\n- 更强的提示遵循能力\n- 更佳的人体结构与空间感知能力\n- 对独特及自定义格式/回复样式的适应性显著提升\n- 极具创造力，提供大量新颖表达\n- 在角色扮演过程中无过多限制",
			ContextLenVal: 8192,
			MaxOutputVal:  8192,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "l3-euryale-70b" },
		},
		"sao10k/l3-lunaris-8b": {
			IDVal:         "sao10k/l3-lunaris-8b",
			NameVal:       "Sao10K: Llama 3 8B Lunaris",
			ProviderVal:   "Sao10k",
			DescVal:       "Lunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge of multiple models, designed to balance creativity with improved logic and general knowledge.\n\nCreated by [Sao10k](https://huggingface.co/Sao10k), this model aims to offer an improved experience over Stheno v3.2, with enhanced creativity and logical reasoning.\n\nFor best results, use with Llama 3 Instruct context template, temperature 1.4, and min_p 0.1.",
			DescCNVal:     "Lunaris 8B 是一款基于 Llama 3 的多功能通用及角色扮演模型，通过战略性融合多个模型，在创造力、逻辑推理与通用知识之间取得良好平衡。\n\n由 [Sao10k](https://huggingface.co/Sao10k) 开发，旨在提供优于 Stheno v3.2 的体验，显著增强创造力与逻辑推理能力。\n\n为获得最佳效果，建议配合 Llama 3 Instruct 上下文模板使用，温度（temperature）设为 1.4，min_p 设为 0.1。",
			ContextLenVal: 8192,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "l3-lunaris-8b" },
		},
		"sao10k/l3.1-70b-hanami-x1": {
			IDVal:         "sao10k/l3.1-70b-hanami-x1",
			NameVal:       "Sao10K: Llama 3.1 70B Hanami x1",
			ProviderVal:   "Sao10k",
			DescVal:       "This is [Sao10K](/sao10k)'s experiment over [Euryale v2.2](/sao10k/l3.1-euryale-70b).",
			DescCNVal:     "这是 [Sao10K](/sao10k) 在 [Euryale v2.2](/sao10k/l3.1-euryale-70b) 基础上开展的实验。",
			ContextLenVal: 16000,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000003,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "l3.1-70b-hanami-x1" },
		},
		"sao10k/l3.1-euryale-70b": {
			IDVal:         "sao10k/l3.1-euryale-70b",
			NameVal:       "Sao10K: Llama 3.1 Euryale 70B v2.2",
			ProviderVal:   "Sao10k",
			DescVal:       "Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b).",
			DescCNVal:     "Euryale L3.1 70B v2.2 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型，是 [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b) 的继任版本。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "l3.1-euryale-70b" },
		},
		"sao10k/l3.3-euryale-70b": {
			IDVal:         "sao10k/l3.3-euryale-70b",
			NameVal:       "Sao10K: Llama 3.3 Euryale 70B",
			ProviderVal:   "Sao10k",
			DescVal:       "Euryale L3.3 70B is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b).",
			DescCNVal:     "Euryale L3.3 70B 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型，是 [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b) 的继任版本。",
			ContextLenVal: 131072,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "l3.3-euryale-70b" },
		},
		"stepfun-ai/step3": {
			IDVal:         "stepfun-ai/step3",
			NameVal:       "StepFun: Step3",
			ProviderVal:   "Stepfun-Ai",
			DescVal:       "Step3 is a cutting-edge multimodal reasoning model—built on a Mixture-of-Experts architecture with 321B total parameters and 38B active. It is designed end-to-end to minimize decoding costs while delivering top-tier performance in vision–language reasoning. Through the co-design of Multi-Matrix Factorization Attention (MFA) and Attention-FFN Disaggregation (AFD), Step3 maintains exceptional efficiency across both flagship and low-end accelerators.",
			DescCNVal:     "Step3 是一款前沿的多模态推理模型，采用专家混合架构，总参数量达 3210 亿，每次前向传递激活 380 亿参数。该模型端到端设计，旨在最小化解码成本的同时，在视觉–语言推理任务中实现顶尖性能。通过多矩阵分解注意力（MFA）与注意力–前馈网络解耦（AFD）的协同设计，Step3 在旗舰级及低端加速器上均保持卓越效率。",
			ContextLenVal: 65536,
			MaxOutputVal:  65536,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "step3" },
		},
		"switchpoint/router": {
			IDVal:         "switchpoint/router",
			NameVal:       "Switchpoint Router",
			ProviderVal:   "Switchpoint",
			DescVal:       "Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an ever-evolving library. \n\nAs the world of LLMs advances, our router gets smarter, ensuring you always benefit from the industry's newest models without changing your workflow.\n\nThis model is configured for a simple, flat rate per response here on OpenRouter. It's powered by the full routing engine from [Switchpoint AI](https://www.switchpoint.dev).",
			DescCNVal:     "Switchpoint AI 的路由模型可即时分析您的请求，并将其动态分发至不断演进的 AI 模型库中最优选项。\n\n随着大语言模型领域的持续进步，我们的路由系统亦同步进化，确保您无需更改现有工作流即可始终受益于业界最新模型。\n\n该模型在 OpenRouter 平台上按每条响应收取统一费率，由 [Switchpoint AI](https://www.switchpoint.dev) 完整路由引擎驱动。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000003,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "router" },
		},
		"tencent/hunyuan-a13b-instruct": {
			IDVal:         "tencent/hunyuan-a13b-instruct",
			NameVal:       "Tencent: Hunyuan A13B Instruct",
			ProviderVal:   "Tencent",
			DescVal:       "Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers competitive benchmark performance across mathematics, science, coding, and multi-turn reasoning tasks, while maintaining high inference efficiency via Grouped Query Attention (GQA) and quantization support (FP8, GPTQ, etc.).",
			DescCNVal:     "Hunyuan-A13B 是腾讯开发的 130 亿激活参数混合专家（MoE）语言模型，总参数量达 800 亿，支持思维链（Chain-of-Thought）推理。该模型在数学、科学、编程及多轮推理等任务的基准测试中表现优异，同时通过分组查询注意力（GQA）机制和量化支持（FP8、GPTQ 等）实现高效推理。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "hunyuan-a13b-instruct" },
		},
		"thedrummer/cydonia-24b-v4.1": {
			IDVal:         "thedrummer/cydonia-24b-v4.1",
			NameVal:       "TheDrummer: Cydonia 24B V4.1",
			ProviderVal:   "Thedrummer",
			DescVal:       "Uncensored and creative writing model based on Mistral Small 3.2 24B with good recall, prompt adherence, and intelligence.",
			DescCNVal:     "基于 Mistral Small 3.2 24B 构建的无审查创意写作模型，具备良好的记忆能力、提示遵循性与智能水平。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "cydonia-24b-v4.1" },
		},
		"thedrummer/rocinante-12b": {
			IDVal:         "thedrummer/rocinante-12b",
			NameVal:       "TheDrummer: Rocinante 12B",
			ProviderVal:   "Thedrummer",
			DescVal:       "Rocinante 12B is designed for engaging storytelling and rich prose.\n\nEarly testers have reported:\n- Expanded vocabulary with unique and expressive word choices\n- Enhanced creativity for vivid narratives\n- Adventure-filled and captivating stories",
			DescCNVal:     "Rocinante 12B 专为引人入胜的故事叙述和丰富文采而设计。\n\n早期测试者反馈：\n- 词汇量更广，用词独特且富有表现力\n- 创造力显著增强，可生成生动叙事\n- 故事情节充满冒险且引人入胜",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "rocinante-12b" },
		},
		"thedrummer/skyfall-36b-v2": {
			IDVal:         "thedrummer/skyfall-36b-v2",
			NameVal:       "TheDrummer: Skyfall 36B V2",
			ProviderVal:   "Thedrummer",
			DescVal:       "Skyfall 36B v2 is an enhanced iteration of Mistral Small 2501, specifically fine-tuned for improved creativity, nuanced writing, role-playing, and coherent storytelling.",
			DescCNVal:     "Skyfall 36B v2 是 Mistral Small 2501 的增强版本，经过专门微调，显著提升了创造力、细腻文风、角色扮演能力及连贯叙事表现。",
			ContextLenVal: 32768,
			MaxOutputVal:  32768,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000001,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "skyfall-36b-v2" },
		},
		"thedrummer/unslopnemo-12b": {
			IDVal:         "thedrummer/unslopnemo-12b",
			NameVal:       "TheDrummer: UnslopNemo 12B",
			ProviderVal:   "Thedrummer",
			DescVal:       "UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios.",
			DescCNVal:     "UnslopNemo v4.1 是 Rocinante 创作者推出的最新模型，专为冒险题材写作和角色扮演场景设计。",
			ContextLenVal: 32768,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "unslopnemo-12b" },
		},
		"tngtech/deepseek-r1t-chimera": {
			IDVal:         "tngtech/deepseek-r1t-chimera",
			NameVal:       "TNG: DeepSeek R1T Chimera",
			ProviderVal:   "Tngtech",
			DescVal:       "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
			DescCNVal:     "DeepSeek-R1T-Chimera 通过融合 DeepSeek-R1 与 DeepSeek-V3（0324）构建而成，兼具 R1 的推理能力和 V3 的 token 效率优势。该模型基于 DeepSeek-MoE Transformer 架构，针对通用文本生成任务进行了优化。\n\n模型通过合并两个源模型的预训练权重，在推理能力、效率和指令遵循任务之间实现性能平衡。采用 MIT 许可证发布，适用于研究及商业用途。",
			ContextLenVal: 163840,
			MaxOutputVal:  163840,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1t-chimera" },
		},
		"tngtech/deepseek-r1t-chimera:free": {
			IDVal:         "tngtech/deepseek-r1t-chimera:free",
			NameVal:       "TNG: DeepSeek R1T Chimera (free)",
			ProviderVal:   "Tngtech",
			DescVal:       "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
			DescCNVal:     "DeepSeek-R1T-Chimera 通过融合 DeepSeek-R1 与 DeepSeek-V3（0324）构建而成，结合了 R1 的推理能力与 V3 的 Token 效率优化优势。该模型基于 DeepSeek-MoE Transformer 架构，专为通用文本生成任务优化。\n\n模型通过合并两个源模型的预训练权重，在推理能力、效率和指令遵循任务之间实现性能平衡。本模型采用 MIT 许可证发布，适用于研究及商业用途。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1t-chimera:free" },
		},
		"tngtech/deepseek-r1t2-chimera": {
			IDVal:         "tngtech/deepseek-r1t2-chimera",
			NameVal:       "TNG: DeepSeek R1T2 Chimera",
			ProviderVal:   "Tngtech",
			DescVal:       "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
			DescCNVal:     "DeepSeek-TNG-R1T2-Chimera 是 TNG Tech 推出的第二代 Chimera 模型，是一款 6710 亿参数的混合专家文本生成模型，由 DeepSeek-AI 的 R1-0528、R1 和 V3-0324 三个检查点通过专家集成（Assembly-of-Experts）融合而成。三亲本设计在 vLLM 下推理速度较原始 R1 提升约 20%，较 R1-0528 提升超 2 倍，在成本与智能之间取得良好平衡。该检查点标准使用支持最长 60k tokens 上下文（实测可达约 130k），并保持一致的 <think> token 行为，适用于长上下文分析、对话及其他开放式生成任务。",
			ContextLenVal: 163840,
			MaxOutputVal:  163840,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1t2-chimera" },
		},
		"tngtech/deepseek-r1t2-chimera:free": {
			IDVal:         "tngtech/deepseek-r1t2-chimera:free",
			NameVal:       "TNG: DeepSeek R1T2 Chimera (free)",
			ProviderVal:   "Tngtech",
			DescVal:       "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
			DescCNVal:     "DeepSeek-TNG-R1T2-Chimera 是 TNG Tech 推出的第二代 Chimera 模型，是一款 6710 亿参数的混合专家文本生成模型，由 DeepSeek-AI 的 R1-0528、R1 和 V3-0324 三个检查点通过专家集成（Assembly-of-Experts）融合而成。三亲本设计在 vLLM 下推理速度较原始 R1 提升约 20%，较 R1-0528 提升超 2 倍，在成本与智能之间取得良好平衡。该检查点标准使用支持最长 60k tokens 上下文（实测可达约 130k），并保持一致的 <think> token 行为，适用于长上下文分析、对话及其他开放式生成任务。",
			ContextLenVal: 163840,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "deepseek-r1t2-chimera:free" },
		},
		"tngtech/tng-r1t-chimera": {
			IDVal:         "tngtech/tng-r1t-chimera",
			NameVal:       "TNG: R1T Chimera",
			ProviderVal:   "Tngtech",
			DescVal:       "TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter.\n\nCharacteristics and improvements include:\n\nWe think that it has a creative and pleasant personality.\nIt has a preliminary EQ-Bench3 value of about 1305.\nIt is quite a bit more intelligent than the original, albeit a slightly slower.\nIt is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated.\nTool calling is much improved.\n\nTNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).",
			DescCNVal:     "TNG-R1T-Chimera 是一款实验性大语言模型，擅长创意叙事与角色互动。它是2025年4月发布的原始 TNG/DeepSeek-R1T-Chimera 模型的衍生版本，仅通过 Chutes 和 OpenRouter 平台提供。\n\n主要特性与改进包括：\n\n- 具备富有创意且令人愉悦的个性；\n- 初步 EQ-Bench3 评分为约1305；\n- 智能水平显著高于原始版本，尽管推理速度略有下降；\n- “思考令牌”一致性大幅提升，即推理块与答案块边界清晰明确；\n- 工具调用能力显著增强。\n\n模型开发者 TNG Tech 要求用户遵循微软为其“MAI-DS-R1” DeepSeek 基础模型制定的审慎使用指南。相关指南可在 Hugging Face 获取（https://huggingface.co/microsoft/MAI-DS-R1）。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "tng-r1t-chimera" },
		},
		"tngtech/tng-r1t-chimera:free": {
			IDVal:         "tngtech/tng-r1t-chimera:free",
			NameVal:       "TNG: R1T Chimera (free)",
			ProviderVal:   "Tngtech",
			DescVal:       "TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter.\n\nCharacteristics and improvements include:\n\nWe think that it has a creative and pleasant personality.\nIt has a preliminary EQ-Bench3 value of about 1305.\nIt is quite a bit more intelligent than the original, albeit a slightly slower.\nIt is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated.\nTool calling is much improved.\n\nTNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).",
			DescCNVal:     "TNG-R1T-Chimera 是一款实验性大语言模型，擅长创意叙事与角色互动。它是2025年4月发布的原始 TNG/DeepSeek-R1T-Chimera 模型的衍生版本，仅通过 Chutes 和 OpenRouter 平台提供。\n\n主要特性与改进包括：\n\n- 具备富有创意且令人愉悦的个性；\n- 初步 EQ-Bench3 评分为约1305；\n- 智能水平显著高于原始版本，尽管推理速度略有下降；\n- “思考令牌”一致性大幅提升，即推理块与答案块边界清晰明确；\n- 工具调用能力显著增强。\n\n模型开发者 TNG Tech 要求用户遵循微软为其“MAI-DS-R1” DeepSeek 基础模型制定的审慎使用指南。相关指南可在 Hugging Face 获取（https://huggingface.co/microsoft/MAI-DS-R1）。",
			ContextLenVal: 163840,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "tng-r1t-chimera:free" },
		},
		"undi95/remm-slerp-l2-13b": {
			IDVal:         "undi95/remm-slerp-l2-13b",
			NameVal:       "ReMM SLERP 13B",
			ProviderVal:   "Undi95",
			DescVal:       "A recreation trial of the original MythoMax-L2-B13 but with updated models. #merge",
			DescCNVal:     "基于更新模型对原始 MythoMax-L2-B13 的复现尝试。#merge",
			ContextLenVal: 6144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "remm-slerp-l2-13b" },
		},
		"x-ai/grok-3": {
			IDVal:         "x-ai/grok-3",
			NameVal:       "xAI: Grok 3",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\n",
			DescCNVal:     "Grok 3 是 xAI 推出的最新模型，作为其旗舰产品，在数据提取、编程和文本摘要等企业级应用场景中表现出色。在金融、医疗、法律和科学领域具备深厚的专业知识。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-3" },
		},
		"x-ai/grok-3-beta": {
			IDVal:         "x-ai/grok-3-beta",
			NameVal:       "xAI: Grok 3 Beta",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
			DescCNVal:     "Grok 3 是 xAI 最新推出的旗舰模型，在企业级应用场景（如数据提取、编程和文本摘要）中表现卓越，并在金融、医疗、法律和科学等领域具备深厚的领域知识。\n\n该模型在 GPQA、LCB 和 MMLU-Pro 等结构化任务与基准测试中表现优异，即使在高推理模式下也显著优于 Grok 3 Mini。\n\n注意：此模型提供两个 xAI 接入端点。默认情况下，使用该模型将始终路由至基础端点。若需使用高速端点，可添加 `provider: { sort: throughput }` 以按吞吐量优先排序。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-3-beta" },
		},
		"x-ai/grok-3-mini": {
			IDVal:         "x-ai/grok-3-mini",
			NameVal:       "xAI: Grok 3 Mini",
			ProviderVal:   "X-Ai",
			DescVal:       "A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.",
			DescCNVal:     "一款轻量级模型，在响应前会进行思考。速度快、智能高效，适用于无需深厚领域知识的逻辑类任务。原始思考轨迹可供访问。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-3-mini" },
		},
		"x-ai/grok-3-mini-beta": {
			IDVal:         "x-ai/grok-3-mini-beta",
			NameVal:       "xAI: Grok 3 Mini Beta",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t demand extensive domain knowledge, and shines in math-specific and quantitative use cases, such as solving challenging puzzles or math problems.\n\nTransparent \"thinking\" traces accessible. Defaults to low reasoning, can boost with setting `reasoning: { effort: \"high\" }`\n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
			DescCNVal:     "Grok 3 Mini 是一款轻量级、低开销的推理模型。与传统模型即时生成答案不同，Grok 3 Mini 会在响应前进行思考，特别适合无需深厚领域知识但需强推理能力的任务，在数学及量化场景（如解决高难度谜题或数学问题）中表现突出。\n\n支持透明的“思考”过程追踪。默认采用低推理强度，可通过设置 `reasoning: { effort: \"high\" }` 提升推理强度。\n\n注意：此模型提供两个 xAI 接入端点。默认情况下，使用该模型将始终路由至基础端点。若需使用高速端点，可添加 `provider: { sort: throughput }` 以按吞吐量优先排序。",
			ContextLenVal: 131072,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-3-mini-beta" },
		},
		"x-ai/grok-4": {
			IDVal:         "x-ai/grok-4",
			NameVal:       "xAI: Grok 4",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)",
			DescCNVal:     "Grok 4 是 xAI 最新推出的推理模型，上下文窗口达 256K 令牌。支持并行工具调用、结构化输出，以及图像与文本双模态输入。请注意：该模型的推理能力不可关闭、不可调节，也无法指定推理强度。当单次请求总令牌数超过 128K 时，计费标准将上调。更多详情请参阅 [xAI 官方文档](https://docs.x.ai/docs/models/grok-4-0709)。",
			ContextLenVal: 256000,
			MaxOutputVal:  0,
			PriceInVal:    0.000003,
			PriceOutVal:   0.000015,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-4" },
		},
		"x-ai/grok-4-fast": {
			IDVal:         "x-ai/grok-4-fast",
			NameVal:       "xAI: Grok 4 Fast",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast).\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
			DescCNVal:     "Grok 4 Fast 是 xAI 最新推出的多模态模型，具备业界领先的性价比，并支持200万token上下文窗口。该模型提供两种模式：非推理模式与推理模式。更多详情请参阅 xAI 官方[新闻公告](http://x.ai/news/grok-4-fast)。\n\n用户可通过 API 中的 `reasoning` 参数的 `enabled` 字段启用或禁用推理功能。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
			ContextLenVal: 2000000,
			MaxOutputVal:  30000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-4-fast" },
		},
		"x-ai/grok-4.1-fast": {
			IDVal:         "x-ai/grok-4.1-fast",
			NameVal:       "xAI: Grok 4.1 Fast",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window.\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
			DescCNVal:     "Grok 4.1 Fast 是 xAI 最佳的智能体工具调用模型，在客服支持与深度研究等真实场景中表现卓越，支持 200 万 token 上下文窗口。\n\n可通过 API 中的 `reasoning` `enabled` 参数启用或禁用推理功能。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
			ContextLenVal: 2000000,
			MaxOutputVal:  30000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-4.1-fast" },
		},
		"x-ai/grok-code-fast-1": {
			IDVal:         "x-ai/grok-code-fast-1",
			NameVal:       "xAI: Grok Code Fast 1",
			ProviderVal:   "X-Ai",
			DescVal:       "Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows.",
			DescCNVal:     "Grok Code Fast 1 是一款高效经济的推理模型，在智能体编程任务中表现卓越。其响应中包含可见的推理轨迹，便于开发者引导 Grok Code 实现高质量工作流。",
			ContextLenVal: 256000,
			MaxOutputVal:  10000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "grok-code-fast-1" },
		},
		"xiaomi/mimo-v2-flash": {
			IDVal:         "xiaomi/mimo-v2-flash",
			NameVal:       "Xiaomi: MiMo-V2-Flash",
			ProviderVal:   "Xiaomi",
			DescVal:       "MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).",
			DescCNVal:     "MiMo-V2-Flash 是小米开发的开源基础语言模型，采用混合专家（Mixture-of-Experts）架构，总参数量达 3090 亿，激活参数量为 150 亿，并采用混合注意力机制。MiMo-V2-Flash 支持混合思维开关与 256K 上下文窗口，在推理、编程及智能体场景中表现卓越。在 SWE-bench Verified 与 SWE-bench Multilingual 基准测试中，MiMo-V2-Flash 均位列全球开源模型榜首，性能媲美 Claude Sonnet 4.5，成本却仅为后者的约 3.5%。\n\n用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)。",
			ContextLenVal: 262144,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mimo-v2-flash" },
		},
		"xiaomi/mimo-v2-flash:free": {
			IDVal:         "xiaomi/mimo-v2-flash:free",
			NameVal:       "Xiaomi: MiMo-V2-Flash (free)",
			ProviderVal:   "Xiaomi",
			DescVal:       "MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).",
			DescCNVal:     "MiMo-V2-Flash 是小米开发的开源基础语言模型，采用混合专家（Mixture-of-Experts）架构，总参数量达 3090 亿，激活参数量为 150 亿，并采用混合注意力机制。MiMo-V2-Flash 支持混合思维开关与 256K 上下文窗口，在推理、编程及智能体场景中表现卓越。在 SWE-bench Verified 与 SWE-bench Multilingual 基准测试中，MiMo-V2-Flash 均位列全球开源模型榜首，性能媲美 Claude Sonnet 4.5，成本却仅为后者的约 3.5%。\n\n用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)。",
			ContextLenVal: 262144,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "mimo-v2-flash:free" },
		},
		"z-ai/glm-4-32b": {
			IDVal:         "z-ai/glm-4-32b",
			NameVal:       "Z.AI: GLM 4 32B ",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM 4 32B is a cost-effective foundation language model.\n\nIt can efficiently perform complex tasks and has significantly enhanced capabilities in tool use, online search, and code-related intelligent tasks.\n\nIt is made by the same lab behind the thudm models.",
			DescCNVal:     "GLM-4-32B 是一款高性价比的基础语言模型。\n\n可高效执行复杂任务，在工具调用、在线搜索及代码相关智能任务方面能力显著增强。\n\n由 thudm 模型背后的同一实验室研发。",
			ContextLenVal: 128000,
			MaxOutputVal:  0,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4-32b" },
		},
		"z-ai/glm-4.5": {
			IDVal:         "z-ai/glm-4.5",
			NameVal:       "Z.AI: GLM 4.5",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leverages a Mixture-of-Experts (MoE) architecture and supports a context length of up to 128k tokens. GLM-4.5 delivers significantly enhanced capabilities in reasoning, code generation, and agent alignment. It supports a hybrid inference mode with two options, a \"thinking mode\" designed for complex reasoning and tool use, and a \"non-thinking mode\" optimized for instant responses. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "GLM-4.5 是我们最新推出的旗舰基础模型，专为智能体应用打造。采用混合专家（MoE）架构，支持最高 128k token 的上下文长度。GLM-4.5 在推理、代码生成和智能体对齐方面能力显著提升。支持混合推理模式：一种为复杂推理和工具调用设计的“推理模式”，另一种为即时响应优化的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 131072,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.5" },
		},
		"z-ai/glm-4.5-air": {
			IDVal:         "z-ai/glm-4.5-air",
			NameVal:       "Z.AI: GLM 4.5 Air",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "GLM-4.5-Air 是我们最新旗舰模型系列的轻量级变体，同样专为以智能体为中心的应用场景设计。与 GLM-4.5 类似，它也采用混合专家（MoE）架构，但参数规模更为紧凑。GLM-4.5-Air 同样支持混合推理模式：提供用于高级推理和工具调用的“推理模式”，以及用于实时交互的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.5-air" },
		},
		"z-ai/glm-4.5-air:free": {
			IDVal:         "z-ai/glm-4.5-air:free",
			NameVal:       "Z.AI: GLM 4.5 Air (free)",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "GLM-4.5-Air 是我们最新旗舰模型系列的轻量级变体，同样专为以智能体为中心的应用场景设计。与 GLM-4.5 类似，它也采用混合专家（MoE）架构，但参数规模更为紧凑。GLM-4.5-Air 同样支持混合推理模式：提供用于高级推理和工具调用的“推理模式”，以及用于实时交互的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 131072,
			MaxOutputVal:  96000,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.5-air:free" },
		},
		"z-ai/glm-4.5v": {
			IDVal:         "z-ai/glm-4.5v",
			NameVal:       "Z.AI: GLM 4.5V",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.5V is a vision-language foundation model for multimodal agent applications. Built on a Mixture-of-Experts (MoE) architecture with 106B parameters and 12B activated parameters, it achieves state-of-the-art results in video understanding, image Q&A, OCR, and document parsing, with strong gains in front-end web coding, grounding, and spatial reasoning. It offers a hybrid inference mode: a \"thinking mode\" for deep reasoning and a \"non-thinking mode\" for fast responses. Reasoning behavior can be toggled via the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			DescCNVal:     "GLM-4.5V 是一款面向多模态智能体应用的视觉语言基础模型。基于混合专家（MoE）架构，总参数量达 1060 亿，每 token 激活 120 亿参数，在视频理解、图像问答、OCR 和文档解析等任务上达到业界领先水平，并在前端网页编码、事实依据和空间推理方面取得显著提升。模型提供混合推理模式：“思考模式”用于深度推理，“非思考模式”用于快速响应。推理行为可通过 `reasoning` `enabled` 布尔值切换。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
			ContextLenVal: 65536,
			MaxOutputVal:  16384,
			PriceInVal:    0.000001,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.5v" },
		},
		"z-ai/glm-4.6": {
			IDVal:         "z-ai/glm-4.6",
			NameVal:       "Z.AI: GLM 4.6",
			ProviderVal:   "Z-Ai",
			DescVal:       "Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.",
			DescCNVal:     "相比 GLM-4.5，本代模型带来多项关键改进：\n\n更长上下文窗口：上下文窗口从 128K 扩展至 200K tokens，使模型能够处理更复杂的智能体任务。\n卓越编码性能：在代码基准测试中得分更高，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等实际应用中表现更佳，包括生成视觉效果更精美的前端页面。\n高级推理能力：GLM-4.6 的推理性能显著提升，支持推理过程中的工具调用，整体能力更强。\n更强大的智能体：在工具调用型和基于搜索的智能体任务中表现更优，并能更有效地集成到智能体框架中。\n精炼的写作能力：在风格与可读性方面更符合人类偏好，在角色扮演场景中表现更自然。",
			ContextLenVal: 202752,
			MaxOutputVal:  65536,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.6" },
		},
		"z-ai/glm-4.6:exacto": {
			IDVal:         "z-ai/glm-4.6:exacto",
			NameVal:       "Z.AI: GLM 4.6 (exacto)",
			ProviderVal:   "Z-Ai",
			DescVal:       "Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.",
			DescCNVal:     "相比 GLM-4.5，本代模型带来多项关键改进：\n\n更长上下文窗口：上下文窗口从 128K 扩展至 200K tokens，使模型能够处理更复杂的智能体任务。\n卓越编码性能：在代码基准测试中得分更高，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等实际应用中表现更佳，包括生成视觉效果更精美的前端页面。\n高级推理能力：GLM-4.6 的推理性能显著提升，支持推理过程中的工具调用，整体能力更强。\n更强大的智能体：在工具调用型和基于搜索的智能体任务中表现更优，并能更有效地集成到智能体框架中。\n精炼的写作能力：在风格与可读性方面更符合人类偏好，在角色扮演场景中表现更自然。",
			ContextLenVal: 204800,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.6:exacto" },
		},
		"z-ai/glm-4.6v": {
			IDVal:         "z-ai/glm-4.6v",
			NameVal:       "Z.AI: GLM 4.6V",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.6V is a large multimodal model designed for high-fidelity visual understanding and long-context reasoning across images, documents, and mixed media. It supports up to 128K tokens, processes complex page layouts and charts directly as visual inputs, and integrates native multimodal function calling to connect perception with downstream tool execution. The model also enables interleaved image-text generation and UI reconstruction workflows, including screenshot-to-HTML synthesis and iterative visual editing.",
			DescCNVal:     "GLM-4.6V 是一款大型多模态模型，专为高保真视觉理解及跨图像、文档和混合媒体的长上下文推理而设计。该模型支持高达 128K tokens，可直接将复杂页面布局和图表作为视觉输入进行处理，并集成原生多模态函数调用，实现感知与下游工具执行的无缝衔接。此外，该模型还支持交错式图文生成与 UI 重建工作流，包括截图转 HTML 合成及迭代式视觉编辑。",
			ContextLenVal: 131072,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000001,
			Features:      CapFunctionCall | CapJsonMode | ModalityImageIn | ModalityTextIn | ModalityTextOut | ModalityVideoIn,
			AliasList:     []string{ "glm-4.6v" },
		},
		"z-ai/glm-4.7": {
			IDVal:         "z-ai/glm-4.7",
			NameVal:       "Z.AI: GLM 4.7",
			ProviderVal:   "Z-Ai",
			DescVal:       "GLM-4.7 is Z.AI’s latest flagship model, featuring upgrades in two key areas: enhanced programming capabilities and more stable multi-step reasoning/execution. It demonstrates significant improvements in executing complex agent tasks while delivering more natural conversational experiences and superior front-end aesthetics.",
			DescCNVal:     "GLM-4.7 是 Z.AI 最新旗舰模型，在两大核心领域实现升级：增强的编程能力与更稳定的多步推理/执行能力。该模型在执行复杂智能体任务方面表现显著提升，同时提供更自然的对话体验与更出色的前端交互效果。",
			ContextLenVal: 202752,
			MaxOutputVal:  65535,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000002,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.7" },
		},
		"z-ai/glm-4.7-flash": {
			IDVal:         "z-ai/glm-4.7-flash",
			NameVal:       "Z.AI: GLM 4.7 Flash",
			ProviderVal:   "Z-Ai",
			DescVal:       "As a 30B-class SOTA model, GLM-4.7-Flash offers a new option that balances performance and efficiency. It is further optimized for agentic coding use cases, strengthening coding capabilities, long-horizon task planning, and tool collaboration, and has achieved leading performance among open-source models of the same size on several current public benchmark leaderboards.",
			DescCNVal:     "",
			ContextLenVal: 200000,
			MaxOutputVal:  131072,
			PriceInVal:    0.000000,
			PriceOutVal:   0.000000,
			Features:      CapFunctionCall | CapJsonMode | ModalityTextIn | ModalityTextOut,
			AliasList:     []string{ "glm-4.7-flash" },
		},
	}

	aliasIndex = map[string]string{
		"aion-1.0": "aion-labs/aion-1.0",
		"aion-1.0-mini": "aion-labs/aion-1.0-mini",
		"aion-rp-llama-3.1-8b": "aion-labs/aion-rp-llama-3.1-8b",
		"auto": "openrouter/auto",
		"bodybuilder": "openrouter/bodybuilder",
		"chatgpt-4o-latest": "openai/chatgpt-4o-latest",
		"claude-3-haiku": "anthropic/claude-3-haiku",
		"claude-3.5-haiku": "anthropic/claude-3.5-haiku",
		"claude-3.5-sonnet": "anthropic/claude-3.5-sonnet",
		"claude-3.7-sonnet": "anthropic/claude-3.7-sonnet",
		"claude-3.7-sonnet:thinking": "anthropic/claude-3.7-sonnet:thinking",
		"claude-haiku-4.5": "anthropic/claude-haiku-4.5",
		"claude-opus-4": "anthropic/claude-opus-4",
		"claude-opus-4.1": "anthropic/claude-opus-4.1",
		"claude-opus-4.5": "anthropic/claude-opus-4.5",
		"claude-sonnet-4": "anthropic/claude-sonnet-4",
		"claude-sonnet-4.5": "anthropic/claude-sonnet-4.5",
		"codellama-7b-instruct-solidity": "alfredpros/codellama-7b-instruct-solidity",
		"coder-large": "arcee-ai/coder-large",
		"codestral-2508": "mistralai/codestral-2508",
		"cogito-v2-preview-llama-109b-moe": "deepcogito/cogito-v2-preview-llama-109b-moe",
		"cogito-v2-preview-llama-405b": "deepcogito/cogito-v2-preview-llama-405b",
		"cogito-v2-preview-llama-70b": "deepcogito/cogito-v2-preview-llama-70b",
		"cogito-v2.1-671b": "deepcogito/cogito-v2.1-671b",
		"command-a": "cohere/command-a",
		"command-r-08-2024": "cohere/command-r-08-2024",
		"command-r-plus-08-2024": "cohere/command-r-plus-08-2024",
		"command-r7b-12-2024": "cohere/command-r7b-12-2024",
		"cydonia-24b-v4.1": "thedrummer/cydonia-24b-v4.1",
		"deephermes-3-mistral-24b-preview": "nousresearch/deephermes-3-mistral-24b-preview",
		"deepseek-chat": "deepseek/deepseek-chat",
		"deepseek-chat-v3-0324": "deepseek/deepseek-chat-v3-0324",
		"deepseek-chat-v3.1": "deepseek/deepseek-chat-v3.1",
		"deepseek-r1": "deepseek/deepseek-r1",
		"deepseek-r1-0528": "deepseek/deepseek-r1-0528",
		"deepseek-r1-0528:free": "deepseek/deepseek-r1-0528:free",
		"deepseek-r1-distill-llama-70b": "deepseek/deepseek-r1-distill-llama-70b",
		"deepseek-r1-distill-qwen-32b": "deepseek/deepseek-r1-distill-qwen-32b",
		"deepseek-r1t-chimera": "tngtech/deepseek-r1t-chimera",
		"deepseek-r1t-chimera:free": "tngtech/deepseek-r1t-chimera:free",
		"deepseek-r1t2-chimera": "tngtech/deepseek-r1t2-chimera",
		"deepseek-r1t2-chimera:free": "tngtech/deepseek-r1t2-chimera:free",
		"deepseek-v3.1-nex-n1": "nex-agi/deepseek-v3.1-nex-n1",
		"deepseek-v3.1-terminus": "deepseek/deepseek-v3.1-terminus",
		"deepseek-v3.1-terminus:exacto": "deepseek/deepseek-v3.1-terminus:exacto",
		"deepseek-v3.2": "deepseek/deepseek-v3.2",
		"deepseek-v3.2-exp": "deepseek/deepseek-v3.2-exp",
		"deepseek-v3.2-speciale": "deepseek/deepseek-v3.2-speciale",
		"devstral-2512": "mistralai/devstral-2512",
		"devstral-2512:free": "mistralai/devstral-2512:free",
		"devstral-medium": "mistralai/devstral-medium",
		"devstral-small": "mistralai/devstral-small",
		"dolphin-mistral-24b-venice-edition:free": "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
		"ernie-4.5-21b-a3b": "baidu/ernie-4.5-21b-a3b",
		"ernie-4.5-21b-a3b-thinking": "baidu/ernie-4.5-21b-a3b-thinking",
		"ernie-4.5-300b-a47b": "baidu/ernie-4.5-300b-a47b",
		"ernie-4.5-vl-28b-a3b": "baidu/ernie-4.5-vl-28b-a3b",
		"ernie-4.5-vl-424b-a47b": "baidu/ernie-4.5-vl-424b-a47b",
		"gemini-2.0-flash-001": "google/gemini-2.0-flash-001",
		"gemini-2.0-flash-exp:free": "google/gemini-2.0-flash-exp:free",
		"gemini-2.0-flash-lite-001": "google/gemini-2.0-flash-lite-001",
		"gemini-2.5-flash": "google/gemini-2.5-flash",
		"gemini-2.5-flash-image": "google/gemini-2.5-flash-image",
		"gemini-2.5-flash-lite": "google/gemini-2.5-flash-lite",
		"gemini-2.5-flash-lite-preview-09-2025": "google/gemini-2.5-flash-lite-preview-09-2025",
		"gemini-2.5-flash-preview-09-2025": "google/gemini-2.5-flash-preview-09-2025",
		"gemini-2.5-pro": "google/gemini-2.5-pro",
		"gemini-2.5-pro-preview": "google/gemini-2.5-pro-preview",
		"gemini-2.5-pro-preview-05-06": "google/gemini-2.5-pro-preview-05-06",
		"gemini-3-flash-preview": "google/gemini-3-flash-preview",
		"gemini-3-pro-image-preview": "google/gemini-3-pro-image-preview",
		"gemini-3-pro-preview": "google/gemini-3-pro-preview",
		"gemma-2-27b-it": "google/gemma-2-27b-it",
		"gemma-2-9b-it": "google/gemma-2-9b-it",
		"gemma-3-12b-it": "google/gemma-3-12b-it",
		"gemma-3-12b-it:free": "google/gemma-3-12b-it:free",
		"gemma-3-27b-it": "google/gemma-3-27b-it",
		"gemma-3-27b-it:free": "google/gemma-3-27b-it:free",
		"gemma-3-4b-it": "google/gemma-3-4b-it",
		"gemma-3-4b-it:free": "google/gemma-3-4b-it:free",
		"gemma-3n-e2b-it:free": "google/gemma-3n-e2b-it:free",
		"gemma-3n-e4b-it": "google/gemma-3n-e4b-it",
		"gemma-3n-e4b-it:free": "google/gemma-3n-e4b-it:free",
		"glm-4-32b": "z-ai/glm-4-32b",
		"glm-4.5": "z-ai/glm-4.5",
		"glm-4.5-air": "z-ai/glm-4.5-air",
		"glm-4.5-air:free": "z-ai/glm-4.5-air:free",
		"glm-4.5v": "z-ai/glm-4.5v",
		"glm-4.6": "z-ai/glm-4.6",
		"glm-4.6:exacto": "z-ai/glm-4.6:exacto",
		"glm-4.6v": "z-ai/glm-4.6v",
		"glm-4.7": "z-ai/glm-4.7",
		"glm-4.7-flash": "z-ai/glm-4.7-flash",
		"goliath-120b": "alpindale/goliath-120b",
		"gpt-3.5-turbo": "openai/gpt-3.5-turbo",
		"gpt-3.5-turbo-0613": "openai/gpt-3.5-turbo-0613",
		"gpt-3.5-turbo-16k": "openai/gpt-3.5-turbo-16k",
		"gpt-3.5-turbo-instruct": "openai/gpt-3.5-turbo-instruct",
		"gpt-4": "openai/gpt-4",
		"gpt-4-0314": "openai/gpt-4-0314",
		"gpt-4-1106-preview": "openai/gpt-4-1106-preview",
		"gpt-4-turbo": "openai/gpt-4-turbo",
		"gpt-4-turbo-preview": "openai/gpt-4-turbo-preview",
		"gpt-4.1": "openai/gpt-4.1",
		"gpt-4.1-mini": "openai/gpt-4.1-mini",
		"gpt-4.1-nano": "openai/gpt-4.1-nano",
		"gpt-4o": "openai/gpt-4o",
		"gpt-4o-2024-05-13": "openai/gpt-4o-2024-05-13",
		"gpt-4o-2024-08-06": "openai/gpt-4o-2024-08-06",
		"gpt-4o-2024-11-20": "openai/gpt-4o-2024-11-20",
		"gpt-4o-audio-preview": "openai/gpt-4o-audio-preview",
		"gpt-4o-mini": "openai/gpt-4o-mini",
		"gpt-4o-mini-2024-07-18": "openai/gpt-4o-mini-2024-07-18",
		"gpt-4o-mini-search-preview": "openai/gpt-4o-mini-search-preview",
		"gpt-4o-search-preview": "openai/gpt-4o-search-preview",
		"gpt-4o:extended": "openai/gpt-4o:extended",
		"gpt-5": "openai/gpt-5",
		"gpt-5-chat": "openai/gpt-5-chat",
		"gpt-5-codex": "openai/gpt-5-codex",
		"gpt-5-image": "openai/gpt-5-image",
		"gpt-5-image-mini": "openai/gpt-5-image-mini",
		"gpt-5-mini": "openai/gpt-5-mini",
		"gpt-5-nano": "openai/gpt-5-nano",
		"gpt-5-pro": "openai/gpt-5-pro",
		"gpt-5.1": "openai/gpt-5.1",
		"gpt-5.1-chat": "openai/gpt-5.1-chat",
		"gpt-5.1-codex": "openai/gpt-5.1-codex",
		"gpt-5.1-codex-max": "openai/gpt-5.1-codex-max",
		"gpt-5.1-codex-mini": "openai/gpt-5.1-codex-mini",
		"gpt-5.2": "openai/gpt-5.2",
		"gpt-5.2-chat": "openai/gpt-5.2-chat",
		"gpt-5.2-codex": "openai/gpt-5.2-codex",
		"gpt-5.2-pro": "openai/gpt-5.2-pro",
		"gpt-audio": "openai/gpt-audio",
		"gpt-audio-mini": "openai/gpt-audio-mini",
		"gpt-oss-120b": "openai/gpt-oss-120b",
		"gpt-oss-120b:exacto": "openai/gpt-oss-120b:exacto",
		"gpt-oss-120b:free": "openai/gpt-oss-120b:free",
		"gpt-oss-20b": "openai/gpt-oss-20b",
		"gpt-oss-20b:free": "openai/gpt-oss-20b:free",
		"gpt-oss-safeguard-20b": "openai/gpt-oss-safeguard-20b",
		"gpt4t": "openai/gpt-4-turbo",
		"granite-4.0-h-micro": "ibm-granite/granite-4.0-h-micro",
		"grok-3": "x-ai/grok-3",
		"grok-3-beta": "x-ai/grok-3-beta",
		"grok-3-mini": "x-ai/grok-3-mini",
		"grok-3-mini-beta": "x-ai/grok-3-mini-beta",
		"grok-4": "x-ai/grok-4",
		"grok-4-fast": "x-ai/grok-4-fast",
		"grok-4.1-fast": "x-ai/grok-4.1-fast",
		"grok-code-fast-1": "x-ai/grok-code-fast-1",
		"hermes-2-pro-llama-3-8b": "nousresearch/hermes-2-pro-llama-3-8b",
		"hermes-3-llama-3.1-405b": "nousresearch/hermes-3-llama-3.1-405b",
		"hermes-3-llama-3.1-405b:free": "nousresearch/hermes-3-llama-3.1-405b:free",
		"hermes-3-llama-3.1-70b": "nousresearch/hermes-3-llama-3.1-70b",
		"hermes-4-405b": "nousresearch/hermes-4-405b",
		"hermes-4-70b": "nousresearch/hermes-4-70b",
		"hunyuan-a13b-instruct": "tencent/hunyuan-a13b-instruct",
		"inflection-3-pi": "inflection/inflection-3-pi",
		"inflection-3-productivity": "inflection/inflection-3-productivity",
		"intellect-3": "prime-intellect/intellect-3",
		"internvl3-78b": "opengvlab/internvl3-78b",
		"jamba-large-1.7": "ai21/jamba-large-1.7",
		"jamba-mini-1.7": "ai21/jamba-mini-1.7",
		"kat-coder-pro": "kwaipilot/kat-coder-pro",
		"kimi-dev-72b": "moonshotai/kimi-dev-72b",
		"kimi-k2": "moonshotai/kimi-k2",
		"kimi-k2-0905": "moonshotai/kimi-k2-0905",
		"kimi-k2-0905:exacto": "moonshotai/kimi-k2-0905:exacto",
		"kimi-k2-thinking": "moonshotai/kimi-k2-thinking",
		"kimi-k2:free": "moonshotai/kimi-k2:free",
		"l3-euryale-70b": "sao10k/l3-euryale-70b",
		"l3-lunaris-8b": "sao10k/l3-lunaris-8b",
		"l3.1-70b-hanami-x1": "sao10k/l3.1-70b-hanami-x1",
		"l3.1-euryale-70b": "sao10k/l3.1-euryale-70b",
		"l3.3-euryale-70b": "sao10k/l3.3-euryale-70b",
		"lfm-2.2-6b": "liquid/lfm-2.2-6b",
		"lfm2-8b-a1b": "liquid/lfm2-8b-a1b",
		"llama-3-70b-instruct": "meta-llama/llama-3-70b-instruct",
		"llama-3-8b-instruct": "meta-llama/llama-3-8b-instruct",
		"llama-3.1-405b": "meta-llama/llama-3.1-405b",
		"llama-3.1-405b-instruct": "meta-llama/llama-3.1-405b-instruct",
		"llama-3.1-405b-instruct:free": "meta-llama/llama-3.1-405b-instruct:free",
		"llama-3.1-70b-instruct": "meta-llama/llama-3.1-70b-instruct",
		"llama-3.1-8b-instruct": "meta-llama/llama-3.1-8b-instruct",
		"llama-3.1-lumimaid-8b": "neversleep/llama-3.1-lumimaid-8b",
		"llama-3.1-nemotron-70b-instruct": "nvidia/llama-3.1-nemotron-70b-instruct",
		"llama-3.1-nemotron-ultra-253b-v1": "nvidia/llama-3.1-nemotron-ultra-253b-v1",
		"llama-3.2-11b-vision-instruct": "meta-llama/llama-3.2-11b-vision-instruct",
		"llama-3.2-1b-instruct": "meta-llama/llama-3.2-1b-instruct",
		"llama-3.2-3b-instruct": "meta-llama/llama-3.2-3b-instruct",
		"llama-3.2-3b-instruct:free": "meta-llama/llama-3.2-3b-instruct:free",
		"llama-3.3-70b-instruct": "meta-llama/llama-3.3-70b-instruct",
		"llama-3.3-70b-instruct:free": "meta-llama/llama-3.3-70b-instruct:free",
		"llama-3.3-nemotron-super-49b-v1.5": "nvidia/llama-3.3-nemotron-super-49b-v1.5",
		"llama-4-maverick": "meta-llama/llama-4-maverick",
		"llama-4-scout": "meta-llama/llama-4-scout",
		"llama-guard-2-8b": "meta-llama/llama-guard-2-8b",
		"llama-guard-3-8b": "meta-llama/llama-guard-3-8b",
		"llama-guard-4-12b": "meta-llama/llama-guard-4-12b",
		"llemma_7b": "eleutherai/llemma_7b",
		"longcat-flash-chat": "meituan/longcat-flash-chat",
		"maestro-reasoning": "arcee-ai/maestro-reasoning",
		"magnum-v4-72b": "anthracite-org/magnum-v4-72b",
		"mercury": "inception/mercury",
		"mercury-coder": "inception/mercury-coder",
		"mimo-v2-flash": "xiaomi/mimo-v2-flash",
		"mimo-v2-flash:free": "xiaomi/mimo-v2-flash:free",
		"minimax-01": "minimax/minimax-01",
		"minimax-m1": "minimax/minimax-m1",
		"minimax-m2": "minimax/minimax-m2",
		"minimax-m2.1": "minimax/minimax-m2.1",
		"ministral-14b-2512": "mistralai/ministral-14b-2512",
		"ministral-3b": "mistralai/ministral-3b",
		"ministral-3b-2512": "mistralai/ministral-3b-2512",
		"ministral-8b": "mistralai/ministral-8b",
		"ministral-8b-2512": "mistralai/ministral-8b-2512",
		"mistral-7b-instruct": "mistralai/mistral-7b-instruct",
		"mistral-7b-instruct-v0.1": "mistralai/mistral-7b-instruct-v0.1",
		"mistral-7b-instruct-v0.2": "mistralai/mistral-7b-instruct-v0.2",
		"mistral-7b-instruct-v0.3": "mistralai/mistral-7b-instruct-v0.3",
		"mistral-large": "mistralai/mistral-large",
		"mistral-large-2407": "mistralai/mistral-large-2407",
		"mistral-large-2411": "mistralai/mistral-large-2411",
		"mistral-large-2512": "mistralai/mistral-large-2512",
		"mistral-medium-3": "mistralai/mistral-medium-3",
		"mistral-medium-3.1": "mistralai/mistral-medium-3.1",
		"mistral-nemo": "mistralai/mistral-nemo",
		"mistral-saba": "mistralai/mistral-saba",
		"mistral-small-24b-instruct-2501": "mistralai/mistral-small-24b-instruct-2501",
		"mistral-small-3.1-24b-instruct": "mistralai/mistral-small-3.1-24b-instruct",
		"mistral-small-3.1-24b-instruct:free": "mistralai/mistral-small-3.1-24b-instruct:free",
		"mistral-small-3.2-24b-instruct": "mistralai/mistral-small-3.2-24b-instruct",
		"mistral-small-creative": "mistralai/mistral-small-creative",
		"mistral-tiny": "mistralai/mistral-tiny",
		"mixtral-8x22b-instruct": "mistralai/mixtral-8x22b-instruct",
		"mixtral-8x7b-instruct": "mistralai/mixtral-8x7b-instruct",
		"molmo-2-8b:free": "allenai/molmo-2-8b:free",
		"morph-v3-fast": "morph/morph-v3-fast",
		"morph-v3-large": "morph/morph-v3-large",
		"mythomax-l2-13b": "gryphe/mythomax-l2-13b",
		"nemotron-3-nano-30b-a3b": "nvidia/nemotron-3-nano-30b-a3b",
		"nemotron-3-nano-30b-a3b:free": "nvidia/nemotron-3-nano-30b-a3b:free",
		"nemotron-nano-12b-v2-vl": "nvidia/nemotron-nano-12b-v2-vl",
		"nemotron-nano-12b-v2-vl:free": "nvidia/nemotron-nano-12b-v2-vl:free",
		"nemotron-nano-9b-v2": "nvidia/nemotron-nano-9b-v2",
		"nemotron-nano-9b-v2:free": "nvidia/nemotron-nano-9b-v2:free",
		"noromaid-20b": "neversleep/noromaid-20b",
		"nova-2-lite-v1": "amazon/nova-2-lite-v1",
		"nova-lite-v1": "amazon/nova-lite-v1",
		"nova-micro-v1": "amazon/nova-micro-v1",
		"nova-premier-v1": "amazon/nova-premier-v1",
		"nova-pro-v1": "amazon/nova-pro-v1",
		"o1": "openai/o1",
		"o1-pro": "openai/o1-pro",
		"o3": "openai/o3",
		"o3-deep-research": "openai/o3-deep-research",
		"o3-mini": "openai/o3-mini",
		"o3-mini-high": "openai/o3-mini-high",
		"o3-pro": "openai/o3-pro",
		"o4-mini": "openai/o4-mini",
		"o4-mini-deep-research": "openai/o4-mini-deep-research",
		"o4-mini-high": "openai/o4-mini-high",
		"olmo-2-0325-32b-instruct": "allenai/olmo-2-0325-32b-instruct",
		"olmo-3-32b-think": "allenai/olmo-3-32b-think",
		"olmo-3-7b-instruct": "allenai/olmo-3-7b-instruct",
		"olmo-3-7b-think": "allenai/olmo-3-7b-think",
		"olmo-3.1-32b-instruct": "allenai/olmo-3.1-32b-instruct",
		"olmo-3.1-32b-think": "allenai/olmo-3.1-32b-think",
		"opus-4.5": "anthropic/claude-opus-4.5",
		"phi-4": "microsoft/phi-4",
		"pixtral-12b": "mistralai/pixtral-12b",
		"pixtral-large-2411": "mistralai/pixtral-large-2411",
		"qwen-2.5-72b": "qwen/qwen-2.5-72b-instruct",
		"qwen-2.5-72b-instruct": "qwen/qwen-2.5-72b-instruct",
		"qwen-2.5-7b-instruct": "qwen/qwen-2.5-7b-instruct",
		"qwen-2.5-coder-32b-instruct": "qwen/qwen-2.5-coder-32b-instruct",
		"qwen-2.5-vl-7b-instruct": "qwen/qwen-2.5-vl-7b-instruct",
		"qwen-2.5-vl-7b-instruct:free": "qwen/qwen-2.5-vl-7b-instruct:free",
		"qwen-max": "qwen/qwen-max",
		"qwen-plus": "qwen/qwen-plus",
		"qwen-plus-2025-07-28": "qwen/qwen-plus-2025-07-28",
		"qwen-plus-2025-07-28:thinking": "qwen/qwen-plus-2025-07-28:thinking",
		"qwen-turbo": "qwen/qwen-turbo",
		"qwen-vl-max": "qwen/qwen-vl-max",
		"qwen-vl-plus": "qwen/qwen-vl-plus",
		"qwen2.5": "qwen/qwen-2.5-72b-instruct",
		"qwen2.5-coder-7b-instruct": "qwen/qwen2.5-coder-7b-instruct",
		"qwen2.5-vl-32b-instruct": "qwen/qwen2.5-vl-32b-instruct",
		"qwen2.5-vl-72b-instruct": "qwen/qwen2.5-vl-72b-instruct",
		"qwen3-14b": "qwen/qwen3-14b",
		"qwen3-235b-a22b": "qwen/qwen3-235b-a22b",
		"qwen3-235b-a22b-2507": "qwen/qwen3-235b-a22b-2507",
		"qwen3-235b-a22b-thinking-2507": "qwen/qwen3-235b-a22b-thinking-2507",
		"qwen3-30b-a3b": "qwen/qwen3-30b-a3b",
		"qwen3-30b-a3b-instruct-2507": "qwen/qwen3-30b-a3b-instruct-2507",
		"qwen3-30b-a3b-thinking-2507": "qwen/qwen3-30b-a3b-thinking-2507",
		"qwen3-32b": "qwen/qwen3-32b",
		"qwen3-4b:free": "qwen/qwen3-4b:free",
		"qwen3-8b": "qwen/qwen3-8b",
		"qwen3-coder": "qwen/qwen3-coder",
		"qwen3-coder-30b-a3b-instruct": "qwen/qwen3-coder-30b-a3b-instruct",
		"qwen3-coder-flash": "qwen/qwen3-coder-flash",
		"qwen3-coder-plus": "qwen/qwen3-coder-plus",
		"qwen3-coder:exacto": "qwen/qwen3-coder:exacto",
		"qwen3-coder:free": "qwen/qwen3-coder:free",
		"qwen3-max": "qwen/qwen3-max",
		"qwen3-next-80b-a3b-instruct": "qwen/qwen3-next-80b-a3b-instruct",
		"qwen3-next-80b-a3b-instruct:free": "qwen/qwen3-next-80b-a3b-instruct:free",
		"qwen3-next-80b-a3b-thinking": "qwen/qwen3-next-80b-a3b-thinking",
		"qwen3-vl-235b-a22b-instruct": "qwen/qwen3-vl-235b-a22b-instruct",
		"qwen3-vl-235b-a22b-thinking": "qwen/qwen3-vl-235b-a22b-thinking",
		"qwen3-vl-30b-a3b-instruct": "qwen/qwen3-vl-30b-a3b-instruct",
		"qwen3-vl-30b-a3b-thinking": "qwen/qwen3-vl-30b-a3b-thinking",
		"qwen3-vl-32b-instruct": "qwen/qwen3-vl-32b-instruct",
		"qwen3-vl-8b-instruct": "qwen/qwen3-vl-8b-instruct",
		"qwen3-vl-8b-thinking": "qwen/qwen3-vl-8b-thinking",
		"qwq-32b": "qwen/qwq-32b",
		"relace-apply-3": "relace/relace-apply-3",
		"relace-search": "relace/relace-search",
		"remm-slerp-l2-13b": "undi95/remm-slerp-l2-13b",
		"rnj-1-instruct": "essentialai/rnj-1-instruct",
		"rocinante-12b": "thedrummer/rocinante-12b",
		"router": "switchpoint/router",
		"seed-1.6": "bytedance-seed/seed-1.6",
		"seed-1.6-flash": "bytedance-seed/seed-1.6-flash",
		"skyfall-36b-v2": "thedrummer/skyfall-36b-v2",
		"sonar": "perplexity/sonar",
		"sonar-deep-research": "perplexity/sonar-deep-research",
		"sonar-pro": "perplexity/sonar-pro",
		"sonar-pro-search": "perplexity/sonar-pro-search",
		"sonar-reasoning-pro": "perplexity/sonar-reasoning-pro",
		"sorcererlm-8x22b": "raifle/sorcererlm-8x22b",
		"spotlight": "arcee-ai/spotlight",
		"step3": "stepfun-ai/step3",
		"tng-r1t-chimera": "tngtech/tng-r1t-chimera",
		"tng-r1t-chimera:free": "tngtech/tng-r1t-chimera:free",
		"tongyi-deepresearch-30b-a3b": "alibaba/tongyi-deepresearch-30b-a3b",
		"trinity-mini": "arcee-ai/trinity-mini",
		"trinity-mini:free": "arcee-ai/trinity-mini:free",
		"ui-tars-1.5-7b": "bytedance/ui-tars-1.5-7b",
		"unslopnemo-12b": "thedrummer/unslopnemo-12b",
		"virtuoso-large": "arcee-ai/virtuoso-large",
		"voxtral-small-24b-2507": "mistralai/voxtral-small-24b-2507",
		"weaver": "mancer/weaver",
		"wizardlm-2-8x22b": "microsoft/wizardlm-2-8x22b",
	}
}
