models:
  ai21/jamba-large-1.7:
    description_cn: Jamba Large 1.7 是 Jamba 开源系列的最新模型，在事实依据、指令遵循和整体效率方面均有提升。该模型基于混合 SSM-Transformer 架构，支持 256K 上下文窗口，相比前代版本可提供更准确、上下文关联更强的响应以及更优的可控性。
  ai21/jamba-mini-1.7:
    description_cn: Jamba Mini 1.7 是 Jamba 开源模型家族中一款紧凑高效的成员，在保持 SSM-Transformer 混合架构和 256K 上下文窗口优势的同时，显著提升了事实依据能力和指令遵循能力。尽管体积小巧，仍能提供准确、上下文关联性强的响应及增强的可控性。
  aion-labs/aion-1.0:
    description_cn: Aion-1.0 是一个多模型系统，旨在在推理、编码等多种任务上实现高性能。该系统基于 DeepSeek-R1 构建，并融合了思维树（Tree of Thoughts, ToT）和混合专家（Mixture of Experts, MoE）等额外模型与技术，是 Aion Lab 最强大的推理模型。
  aion-labs/aion-1.0-mini:
    description_cn: Aion-1.0-Mini 是一个 32B 参数模型，为 DeepSeek-R1 模型的蒸馏版本，专为数学、编码和逻辑等推理领域提供强大性能。该模型是 FuseAI 模型的一个改进变体，在基准测试中优于 R1-Distill-Qwen-32B 和 R1-Distill-Llama-70B，其基准结果可在其 [Hugging Face 页面](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview) 查阅，并已由第三方独立复现验证。
  aion-labs/aion-rp-llama-3.1-8b:
    description_cn: Aion-RP-Llama-3.1-8B 在 RPBench-Auto 基准的角色扮演评估部分中排名第一。RPBench-Auto 是 Arena-Hard-Auto 的角色扮演专用变体，采用大语言模型相互评估回复质量。该模型是一个经过微调的基础模型（非指令微调模型），旨在生成更自然、更多样化的文本。
  alfredpros/codellama-7b-instruct-solidity:
    description_cn: 基于 PEFT 库提供的 4 位 QLoRA 微调方法，对拥有 70 亿参数的 Code LLaMA - Instruct 模型进行微调，专用于生成 Solidity 智能合约。
  alibaba/tongyi-deepresearch-30b-a3b:
    description_cn: |-
      通义深度研究（Tongyi DeepResearch）是通义实验室研发的智能体大语言模型，总参数量为300亿，每token仅激活30亿参数。该模型专为长周期、深度信息检索任务优化，在Humanity's Last Exam、BrowserComp、BrowserComp-ZH、WebWalkerQA、GAIA、xbench-DeepSearch和FRAMES等基准测试中达到业界领先水平，相较前代模型在复杂智能体搜索、推理及多步问题求解方面表现更优。

      该模型采用全自动合成数据流水线，支持可扩展的预训练、微调与强化学习。通过大规模持续预训练多样化智能体数据，提升推理能力并保持知识时效性。同时，模型引入端到端在线策略强化学习，采用定制化的分组相对策略优化（Group Relative Policy Optimization），结合token级梯度与负样本过滤机制，确保训练稳定性。模型支持ReAct用于核心能力验证，并提供基于IterResearch的“重型”模式，通过测试时扩展实现极致性能，适用于高级研究智能体、工具调用及高负载推理工作流。
  allenai/molmo-2-8b:free:
    description_cn: Molmo2-8B 是艾伦人工智能研究所（AI2）开发的开源视觉语言模型，属于 Molmo2 系列，支持图像、视频及多图理解与定位。该模型基于 Qwen3-8B 构建，采用 SigLIP 2 作为视觉主干网络，在短视频、计数和图像描述等任务上优于其他开源权重与开源数据的模型，同时在长视频任务中仍保持竞争力。
  allenai/olmo-2-0325-32b-instruct:
    description_cn: OLMo-2 32B Instruct 是 OLMo-2 32B（2025年3月基础模型）的监督指令微调版本，在 GSM8K、MATH、IFEval 等复杂推理与指令遵循基准测试及通用 NLP 评估中表现卓越。该模型由 AI2 开发，属于一项开放、面向研究的计划，主要基于英文数据集训练，旨在推动开源语言模型的理解与发展。
  allenai/olmo-3-7b-instruct:
    description_cn: Olmo 3 7B Instruct 是 Olmo 3 7B 基础模型的监督指令微调版本，专为指令遵循、问答和自然对话交互优化。通过高质量指令数据与开源训练流程，该模型在日常 NLP 任务中表现优异，同时保持易于集成和使用。由 AI2 基于 Apache 2.0 许可证开发，为指令驱动型应用提供透明且社区友好的选择。
  allenai/olmo-3-7b-think:
    description_cn: Olmo 3 7B Think 是 Olmo 系列中面向研究的语言模型，专为高级推理和指令驱动任务设计，在多步问题求解、逻辑推理及维持连贯对话上下文方面表现卓越。该模型由 AI2 基于 Apache 2.0 许可证开发，支持完全透明的开放式实验，为学术研究和实用 NLP 工作流提供轻量但功能强大的基础。
  allenai/olmo-3-32b-think:
    description_cn: Olmo 3 32B Think 是一款专为深度推理、复杂逻辑链和高级指令遵循场景设计的大规模语言模型，参数量达320亿。其强大的能力使其在高难度评估任务和高度细致的对话推理中表现出色。该模型由艾伦人工智能研究所（AI2）基于 Apache 2.0 许可证开发，体现了 Olmo 项目对开放性的承诺，全面公开了模型权重、代码及训练方法。
  allenai/olmo-3.1-32b-instruct:
    description_cn: Olmo 3.1 32B Instruct 是一款大规模、320亿参数的指令微调语言模型，专为高性能对话式 AI、多轮对话及实用指令遵循而设计。作为 Olmo 3.1 系列的成员，该变体强调对复杂用户指令的响应能力与稳健的聊天交互，同时在推理和编程基准测试中保持强大性能。该模型由 AI2 在 Apache 2.0 许可下开发，体现了 Olmo 计划对开放性与透明度的承诺。
  allenai/olmo-3.1-32b-think:
    description_cn: Olmo 3.1 32B Think 是一款大规模、320亿参数的模型，专为深度推理、复杂多步逻辑及高级指令遵循而设计。基于 Olmo 3 系列构建，3.1 版本在严苛评估和细致对话任务中展现出更精细的推理行为与更强的性能。该模型由 AI2 在 Apache 2.0 许可下开发，延续了 Olmo 计划对开放性的承诺，全面公开模型权重、代码及训练方法。
  alpindale/goliath-120b:
    description_cn: |-
      一款大型语言模型，通过将两个经过微调的 Llama 70B 模型合并为一个 120B 模型构建而成，融合了 Xwin 与 Euryale。

      致谢：
      - [@chargoddard](https://huggingface.co/chargoddard) 开发了用于模型合并的框架 [mergekit](https://github.com/cg123/mergekit)。
      - [@Undi95](https://huggingface.co/Undi95) 协助确定了模型合并比例。

      #merge
  amazon/nova-2-lite-v1:
    description_cn: |-
      Nova 2 Lite 是一款快速、高性价比的推理模型，适用于日常工作负载，可处理文本、图像和视频以生成文本。

      Nova 2 Lite 在文档处理、视频信息提取、代码生成、提供准确的事实依据型答案以及自动化多步骤智能体工作流方面表现出色。
  amazon/nova-lite-v1:
    description_cn: |-
      Amazon Nova Lite 1.0 是亚马逊推出的超低成本多模态模型，专注于快速处理图像、视频和文本输入以生成文本输出。该模型可高精度处理实时客户交互、文档分析和视觉问答任务。

      凭借 30 万 token 的输入上下文长度，它可在单次输入中分析多张图像或最多 30 分钟的视频。
  amazon/nova-micro-v1:
    description_cn: Amazon Nova Micro 1.0 是纯文本模型，在 Amazon Nova 系列中提供最低延迟的响应，且成本极低。其上下文长度达 12.8 万 token，针对速度与成本进行了优化，擅长文本摘要、翻译、内容分类、交互式聊天和头脑风暴等任务，并具备基础的数学推理与编码能力。
  amazon/nova-premier-v1:
    description_cn: Amazon Nova Premier 是亚马逊多模态模型中能力最强的版本，专为复杂推理任务设计，同时也是蒸馏定制模型的最佳教师模型。
  amazon/nova-pro-v1:
    description_cn: |-
      Amazon Nova Pro 1.0 是亚马逊推出的高性能多模态模型，旨在为广泛任务提供精度、速度与成本的最佳平衡。截至 2024 年 12 月，该模型在关键基准测试中达到业界领先水平，包括视觉问答（TextVQA）和视频理解（VATEX）。

      Amazon Nova Pro 在处理视觉与文本信息以及分析金融文档方面展现出强大能力。

      **注意**：当前暂不支持视频输入。
  anthracite-org/magnum-v4-72b:
    description_cn: |-
      该系列模型旨在复现 Claude 3 系列模型（特别是 Sonnet（https://openrouter.ai/anthropic/claude-3.5-sonnet）和 Opus（https://openrouter.ai/anthropic/claude-3-opus））的散文质量。

      本模型基于 [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct) 进行微调。
  anthropic/claude-3-haiku:
    description_cn: |-
      Claude 3 Haiku 是 Anthropic 推出的最快、最紧凑的模型，可实现近乎即时的响应，兼具快速性与精准的定向性能。

      详见发布公告及基准测试结果：[此处](https://www.anthropic.com/news/claude-3-haiku)

      #multimodal
  anthropic/claude-3.5-haiku:
    description_cn: |-
      Claude 3.5 Haiku 在速度、代码准确性和工具使用方面能力显著增强。专为实时应用场景优化，可提供快速响应，适用于聊天交互和即时代码建议等动态任务。

      因此，该模型非常适合对速度与精度均有高要求的场景，如软件开发、客户服务机器人和数据管理系统。

      当前此模型指向 [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022)。
  anthropic/claude-3.5-sonnet:
    description_cn: |-
      全新 Claude 3.5 Sonnet 在性能上超越 Opus，速度优于原有 Sonnet，价格维持 Sonnet 水平。Sonnet 尤其擅长以下领域：

      - 编程：在 SWE-Bench Verified 上得分约 49%，高于此前最佳成绩，且无需复杂的提示工程；
      - 数据科学：增强人类数据科学家的专业能力，能结合多种工具从非结构化数据中提取洞见；
      - 视觉处理：擅长解读图表、图形和图像，不仅能准确转录文本，还能从中挖掘超越文本本身的深层信息；
      - 智能体任务：具备卓越的工具调用能力，非常适合执行需与其他系统交互的复杂多步问题求解任务。

      #多模态
  anthropic/claude-3.7-sonnet:
    description_cn: |-
      Claude 3.7 Sonnet 是一款先进的大语言模型，在推理、编程和问题解决能力方面均有显著提升。该模型引入了混合推理方法，允许用户在快速响应与针对复杂任务的逐步深入处理之间进行选择。其在编程方面表现尤为突出，尤其在前端开发和全栈更新场景中，并在智能体工作流（agentic workflows）中表现出色，能够自主执行多步骤流程。

      Claude 3.7 Sonnet 在标准模式下保持与其前代模型相当的性能，同时提供扩展推理模式，以在数学、编程及指令遵循任务中实现更高精度。

      更多详情请参阅[此博客文章](https://www.anthropic.com/news/claude-3-7-sonnet)
  anthropic/claude-3.7-sonnet:thinking:
    description_cn: |-
      Claude 3.7 Sonnet 是一款先进的大语言模型，在推理、编程和问题解决能力方面均有显著提升。该模型引入了混合推理方法，允许用户在快速响应与针对复杂任务的逐步深入处理之间进行选择。其在编程方面表现尤为突出，尤其在前端开发和全栈更新场景中，并在智能体工作流（agentic workflows）中表现出色，能够自主执行多步骤流程。

      Claude 3.7 Sonnet 在标准模式下保持与其前代模型相当的性能，同时提供扩展推理模式，以在数学、编程及指令遵循任务中实现更高精度。

      更多详情请参阅[此博客文章](https://www.anthropic.com/news/claude-3-7-sonnet)
  anthropic/claude-haiku-4.5:
    description_cn: |-
      Claude Haiku 4.5 是 Anthropic 推出的最快、最高效的模型，以远低于更大规模 Claude 模型的成本和延迟提供接近前沿水平的智能。其在推理、编码和计算机使用任务上的表现媲美 Claude Sonnet 4，将前沿能力带入实时和高吞吐量应用场景。

      该模型首次为 Haiku 系列引入扩展思考能力，支持可控的推理深度、摘要式或交错式思维输出，以及全面支持编码、Bash、网页搜索和计算机使用工具的工具辅助工作流。在 SWE-bench Verified 基准上得分超过 73%，Haiku 4.5 跻身全球顶尖编码模型之列，同时在子代理、并行执行和规模化部署中保持卓越响应速度。
  anthropic/claude-opus-4:
    description_cn: |-
      Claude Opus 4 在发布时被公认为全球最强的编程模型，可在复杂、长时间运行的任务和智能体工作流中保持稳定性能。该模型在软件工程领域树立了新标杆，在 SWE-bench（72.5%）和 Terminal-bench（43.2%）上均取得领先成绩。Opus 4 支持扩展型智能体工作流，可连续数小时处理数千个任务步骤而性能不衰减。

      [在此处阅读博客文章](https://www.anthropic.com/news/claude-4)
  anthropic/claude-opus-4.1:
    description_cn: Claude Opus 4.1 是 Anthropic 旗舰模型的更新版本，在编码、推理和智能体任务方面性能显著提升。该模型在 SWE-bench Verified 上达到 74.5% 的准确率，并在多文件代码重构、调试精度和细节导向推理方面取得显著进步。模型支持最多 64K tokens 的扩展推理，专为研究、数据分析和工具辅助推理等任务优化。
  anthropic/claude-opus-4.5:
    name_cn: Claude 4.5 Opus
    description_cn: Anthropic 最强大的模型，具备极高的推理能力。
    aliases:
      - claude-opus-4.5
      - opus-4.5
  anthropic/claude-sonnet-4:
    description_cn: |-
      Claude Sonnet 4 相较前代 Sonnet 3.7 显著提升，在编程与推理任务中展现出更高的精度与可控性。该模型在 SWE-bench（72.7%）上达到业界领先水平，兼顾强大能力与计算效率，适用于从日常编码到复杂软件开发的广泛场景。关键改进包括更优的自主代码库导航能力、更低的智能体工作流错误率，以及对复杂指令更强的遵循可靠性。Sonnet 4 针对日常实用场景优化，在各类内外部应用中提供先进推理能力的同时，保持高效响应。

      [在此处阅读博客文章](https://www.anthropic.com/news/claude-4)
  anthropic/claude-sonnet-4.5:
    description_cn: |-
      Claude Sonnet 4.5 是 Anthropic 迄今最先进的 Sonnet 模型，专为现实世界智能体和编码工作流优化。该模型在 SWE-bench Verified 等编码基准测试中达到业界领先水平，在系统设计、代码安全性和规范遵循方面均有显著提升。其设计支持长时间自主运行，可在会话间保持任务连续性，并提供基于事实的进度追踪。

      Sonnet 4.5 还增强了智能体能力，包括改进的工具编排、推测性并行执行以及更高效的上下文与内存管理。凭借强化的上下文追踪能力和对工具调用中 token 使用情况的感知，该模型特别适用于多上下文及长时间运行的工作流。典型应用场景涵盖软件工程、网络安全、金融分析、研究智能体及其他需要持续推理与工具调用的领域。
  arcee-ai/coder-large:
    description_cn: Coder‑Large 是基于 Qwen 2.5‑Instruct 微调的 320 亿参数模型，进一步在采用宽松许可证的 GitHub、CodeSearchNet 及合成缺陷修复语料库上训练而成。该模型支持 32k 上下文窗口，可在单次调用中完成多文件重构或长差异审查，并支持 30 多种编程语言，尤其针对 TypeScript、Go 和 Terraform 进行了优化。内部基准测试表明，得益于强化学习阶段对可编译输出的奖励机制，其在 HumanEval 上比 CodeLlama‑34B‑Python 高出 5–8 分，在 BugFix 任务上表现同样优异。模型默认在代码块旁生成结构化解释，既适用于教育工具，也适用于生产级编程助手场景。在成本方面，Together AI 的定价远低于主流闭源竞品，使团队能在控制支出的同时规模化部署交互式编码功能。
  arcee-ai/maestro-reasoning:
    description_cn: Maestro Reasoning 是 Arcee 旗舰级分析模型：基于 Qwen 2.5‑32B 构建的 320 亿参数模型，采用 DPO 与思维链强化学习（chain‑of‑thought RL）进行微调，专精于逐步逻辑推理。相比早期 70 亿参数预览版，正式发布的 320 亿参数版本将上下文窗口扩展至 128k tokens，并在 MATH 与 GSM‑8K 基准测试中的通过率翻倍，同时提升了代码补全准确率。其指令风格鼓励生成结构化的“思考 → 答案”轨迹，用户可根据偏好选择解析或隐藏该轨迹。这种透明性特别契合金融、医疗等注重审计的行业，因其需追溯推理路径。在 Arcee Conductor 中，Maestro 会自动用于处理小型 SLM 无法应对的复杂多约束查询。
  arcee-ai/spotlight:
    description_cn: Spotlight 是一款 70 亿参数的视觉语言模型，基于 Qwen 2.5‑VL 开发，并由 Arcee AI 针对紧密图文对齐任务进行微调。该模型支持 32k tokens 的上下文窗口，可实现融合长篇文档与单张或多张图像的丰富多模态对话。训练重点在于消费级 GPU 上的快速推理，同时保持强大的图像描述、视觉问答（VQA）及图表分析准确性。因此，Spotlight 能无缝嵌入智能体工作流，实时解读截图、图表或 UI 原型。早期基准测试显示，其在主流 VQA 与 POPE 对齐测试中表现媲美甚至超越 LLaVA‑1.6 13B 等更大规模的视觉语言模型。
  arcee-ai/trinity-mini:
    description_cn: Trinity Mini 是一款稀疏混合专家（MoE）语言模型，总参数量260亿（每 token 激活约30亿），包含128个专家，每 token 激活其中8个。专为高效处理长上下文（131k tokens）而设计，具备强大的函数调用能力和多步智能体工作流支持。
  arcee-ai/trinity-mini:free:
    description_cn: Trinity Mini 是一款稀疏混合专家（MoE）语言模型，总参数量260亿（每 token 激活约30亿），包含128个专家，每 token 激活其中8个。专为高效处理长上下文（131k tokens）而设计，具备强大的函数调用能力和多步智能体工作流支持。
  arcee-ai/virtuoso-large:
    description_cn: Virtuoso‑Large 是 Arcee 旗下 720 亿参数的顶级通用大语言模型，专为跨领域推理、创意写作及企业级问答任务优化。不同于多数 700 亿级同类模型，它保留了源自 Qwen 2.5 的 128k 上下文窗口，可一次性处理整本书籍、代码库或财务文件。训练过程融合 DeepSeek R1 蒸馏、多轮监督微调及最终的 DPO/RLHF 对齐阶段，在 BIG‑Bench‑Hard、GSM‑8K 及长上下文“大海捞针”测试中表现卓越。企业常将其作为 Conductor 流水线中的“兜底”智能核心，当其他小型 SLM 置信度不足时自动启用。尽管模型规模庞大，但凭借激进的 KV 缓存优化，在 8× H100 节点上首 token 延迟仍控制在低秒级，是一款实用的生产级高性能模型。
  baidu/ernie-4.5-21b-a3b:
    description_cn: 这是一款先进的纯文本混合专家（MoE）模型，总参数量达 210 亿，每 token 激活 30 亿参数，通过异构 MoE 结构与模态隔离路由机制实现卓越的多模态理解与生成能力。模型支持长达 131K token 的上下文，并借助多专家并行协作与量化技术实现高效推理；结合 SFT、DPO 和 UPO 等先进后训练方法，辅以专用路由与均衡损失函数，确保在各类应用场景中均具备优异性能。
  baidu/ernie-4.5-21b-a3b-thinking:
    description_cn: ERNIE-4.5-21B-A3B-Thinking 是百度推出的升级版轻量级 MoE 模型，经过优化以提升推理深度与质量，在逻辑谜题、数学、科学、编程、文本生成及专家级学术基准测试中表现卓越。
  baidu/ernie-4.5-300b-a47b:
    description_cn: ERNIE-4.5-300B-A47B 是百度推出的 ERNIE 4.5 系列中的 3000 亿参数混合专家（MoE）语言模型，每 token 激活 470 亿参数，支持中英文文本生成。该模型采用异构 MoE 架构，结合先进的路由机制与量化策略（包括 FP8 和 2-bit 格式），针对高吞吐推理和高效扩展进行了优化。此版本专为纯语言任务微调，支持推理、工具调用及最长 131k tokens 的上下文长度，适用于对推理能力和吞吐量要求较高的通用大模型应用场景。
  baidu/ernie-4.5-vl-28b-a3b:
    description_cn: 这是一款强大的多模态混合专家（MoE）对话模型，总参数量达 280 亿，每 token 激活 30 亿参数，依托创新的异构 MoE 架构与模态隔离路由机制，实现卓越的文本与视觉理解能力。模型基于高吞吐训练与推理的高效扩展基础设施构建，采用 SFT、DPO 和 UPO 等先进后训练技术优化性能，支持高达 131K 的上下文长度，并通过 RLVR 对齐机制显著提升跨模态推理与生成能力。
  baidu/ernie-4.5-vl-424b-a47b:
    description_cn: ERNIE-4.5-VL-424B-A47B 是百度 ERNIE 4.5 系列中的多模态混合专家（MoE）模型，总参数量达 4240 亿，每 token 激活 470 亿参数。该模型基于异构 MoE 架构，采用模态隔离路由机制，在文本与图像数据上联合训练，实现高保真跨模态推理、图像理解及长达 131k tokens 的上下文生成。通过 SFT、DPO、UPO 和 RLVR 等技术微调，支持“思考”与非思考两种推理模式，专为中英文视觉-语言任务设计，并针对高效扩展优化，可在 4-bit/8-bit 量化下运行。
  bytedance-seed/seed-1.6:
    description_cn: Seed 1.6 是字节跳动 Seed 团队发布的一款通用模型，具备多模态能力与自适应深度思考功能，上下文窗口达 256K。
  bytedance-seed/seed-1.6-flash:
    description_cn: Seed 1.6 Flash 是字节跳动 Seed 团队推出的超高速多模态深度思考模型，支持文本与视觉理解，具备 256K 上下文窗口，并可生成最多 16K 个输出 token。
  bytedance/ui-tars-1.5-7b:
    description_cn: |-
      UI-TARS-1.5 是一款专为图形用户界面（GUI）环境优化的多模态视觉语言智能体，适用于桌面界面、网页浏览器、移动系统及游戏场景。该模型由字节跳动开发，在 UI-TARS 框架基础上引入基于强化学习的推理机制，可在各类虚拟界面中实现稳健的动作规划与执行。

      该模型在多项交互式与具身智能基准测试中达到业界领先水平，包括 OSworld、WebVoyager、AndroidWorld 和 ScreenSpot。此外，它在多种 Poki 游戏中实现了完美任务完成率，并在《我的世界》（Minecraft）智能体任务中显著超越先前模型。UI-TARS-1.5 支持推理过程中的思维分解，并在不同规模版本中展现出强大的性能扩展能力，其中 1.5 版本的性能明显优于早期的 72B 和 7B 检查点。
  cognitivecomputations/dolphin-mistral-24b-venice-edition:free:
    description_cn: Venice Uncensored Dolphin Mistral 24B Venice Edition 是 Mistral-Small-24B-Instruct-2501 的微调变体，由 dphn.ai 与 Venice.ai 联合开发。该模型定位为“无审查”的指令微调大语言模型，保留用户对对齐策略、系统提示及行为模式的完全控制权。面向高级且无限制的应用场景，Venice Uncensored 强调可引导性与行为透明性，移除了主流助手模型中常见的默认安全与对齐机制。
  cohere/command-a:
    description_cn: Command A 是一款开源权重的 1110 亿参数模型，支持 256k 上下文窗口，专注于在智能体、多语言和编程等应用场景中提供卓越性能。相较于其他主流闭源及开源模型，Command A 在显著降低硬件成本的同时实现最高性能，尤其擅长处理对业务至关重要的智能体与多语言任务。
  cohere/command-r-08-2024:
    description_cn: |-
      command-r-08-2024 是 [Command R](/models/cohere/command-r) 的更新版本，在多语言检索增强生成（RAG）和工具使用方面性能更优。总体而言，该模型在数学、代码和推理能力上均有提升，性能可与上一代更大规模的 Command R+ 模型相媲美。

      发布详情请参阅[此处](https://docs.cohere.com/changelog/command-gets-refreshed)。

      使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。
  cohere/command-r-plus-08-2024:
    description_cn: |-
      command-r-plus-08-2024 是 [Command R+](/models/cohere/command-r-plus) 的更新版本，相较于前代 Command R+，吞吐量提升约 50%，延迟降低 25%，同时保持相同的硬件资源占用。

      发布详情请参阅[此处](https://docs.cohere.com/changelog/command-gets-refreshed)。

      使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。
  cohere/command-r7b-12-2024:
    description_cn: |-
      Command R7B（2024 年 12 月版）是 Command R+ 模型的小幅快速更新版本，于 2024 年 12 月发布。该模型在检索增强生成（RAG）、工具调用、智能体等需要复杂推理和多步骤操作的任务中表现卓越。

      使用本模型需遵守 Cohere 的[使用政策](https://docs.cohere.com/docs/usage-policy)和[SaaS 协议](https://cohere.com/saas-agreement)。
  deepcogito/cogito-v2-preview-llama-70b:
    description_cn: Cogito v2 70B 是一款稠密型混合推理模型，兼具直接作答能力与高级自省机制。通过迭代策略优化构建，在保持较短推理链和更强直觉的同时，在各类推理任务中展现出卓越性能。
  deepcogito/cogito-v2-preview-llama-109b-moe:
    description_cn: 一款基于 Llama-4-Scout-17B-16E 构建的指令微调混合推理专家混合（Mixture-of-Experts）模型。Cogito v2 可直接作答，也可启用扩展的“思考”阶段，其对齐机制由迭代蒸馏与放大（IDA）引导。该模型专注于编程、STEM、指令遵循和通用助理性任务，在多语言能力、工具调用和推理性能方面均优于同等规模的基线模型。支持长上下文使用（最高达 1000 万 tokens）及标准 Transformers 工作流。用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  deepcogito/cogito-v2-preview-llama-405b:
    description_cn: Cogito v2 405B 是一种密集型混合推理模型，兼具直接回答能力与高级自省机制。该模型采用密集架构，在性能上可与领先的闭源模型相媲美，代表了迈向前沿智能的重要一步。这一先进推理系统结合策略优化与超大规模，展现出卓越能力。
  deepcogito/cogito-v2.1-671b:
    description_cn: Cogito v2.1 671B MoE 是全球最强的开源模型之一，性能媲美前沿闭源与开源模型。该模型通过自博弈强化学习训练，在指令遵循、编程、长查询及创意写作等多个领域达到业界领先水平，展现了通过策略优化迈向可扩展超级智能的重要进展。
  deepseek/deepseek-chat:
    description_cn: |-
      DeepSeek-V3 是 DeepSeek 团队推出的最新模型，在前代版本的指令遵循与编码能力基础上进一步提升。该模型在近 15 万亿 token 上完成预训练，公开评测显示其性能超越其他开源模型，并可媲美主流闭源模型。

      有关模型详情，请访问 [DeepSeek-V3 代码仓库](https://github.com/deepseek-ai/DeepSeek-V3) 或参阅[发布公告](https://api-docs.deepseek.com/news/news1226)。
  deepseek/deepseek-chat-v3-0324:
    description_cn: |-
      DeepSeek V3 是 DeepSeek 团队最新推出的旗舰级对话模型系列的最新版本，采用混合专家（Mixture-of-Experts）架构，参数量达 6850 亿。

      该模型继 [DeepSeek V3](/deepseek/deepseek-chat-v3) 之后推出，在各类任务中均表现出色。
  deepseek/deepseek-chat-v3.1:
    description_cn: |-
      DeepSeek-V3.1 是一款大型混合推理模型（总参数 6710 亿，激活参数 370 亿），通过提示模板支持“思考”与“非思考”两种模式。该模型在 DeepSeek-V3 基础上采用两阶段长上下文训练流程，支持最多 128K tokens，并利用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该模型在工具调用、代码生成和推理效率方面均有提升，在高难度基准测试中性能媲美 DeepSeek-R1，同时响应速度更快。支持结构化工具调用、代码代理和搜索代理，适用于科研、编程及智能体工作流。

      此模型接替 [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324)，在多种任务上表现优异。
  deepseek/deepseek-r1:
    description_cn: |-
      DeepSeek R1 现已发布：性能媲美 [OpenAI o1](/openai/o1)，但完全开源且推理 token 全部开放。模型总参数量达 6710 亿，单次推理激活 370 亿参数。

      完全开源模型及[技术报告](https://api-docs.deepseek.com/news/news250120)。

      采用 MIT 许可证：可自由蒸馏与商业化！
  deepseek/deepseek-r1-0528:
    description_cn: |-
      5月28日更新版[原始 DeepSeek R1](/deepseek/deepseek-r1)，性能与[OpenAI o1](/openai/o1)相当，但完全开源且推理过程中的所有推理 token 均公开。模型总参数量为6710亿，单次推理激活370亿参数。

      完全开源模型。
  deepseek/deepseek-r1-0528:free:
    description_cn: |-
      5月28日更新版[原始 DeepSeek R1](/deepseek/deepseek-r1)，性能与[OpenAI o1](/openai/o1)相当，但完全开源且推理过程中的所有推理 token 均公开。模型总参数量为6710亿，单次推理激活370亿参数。

      完全开源模型。
  deepseek/deepseek-r1-distill-llama-70b:
    description_cn: |-
      DeepSeek R1 Distill Llama 70B 是基于 [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct) 并利用 [DeepSeek R1](/deepseek/deepseek-r1) 输出蒸馏而成的大语言模型。该模型融合先进蒸馏技术，在多项基准测试中表现卓越，包括：

      - AIME 2024 pass@1：70.0
      - MATH-500 pass@1：94.5
      - CodeForces 评分：1633

      通过 DeepSeek R1 输出的微调，该模型实现了可与更大规模前沿模型相媲美的竞争力。
  deepseek/deepseek-r1-distill-qwen-32b:
    description_cn: |-
      DeepSeek R1 Distill Qwen 32B 是基于 [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B) 并利用 [DeepSeek R1](/deepseek/deepseek-r1) 输出蒸馏而成的大语言模型。该模型在多项基准测试中超越 OpenAI o1-mini，为稠密模型树立了新的性能标杆。

      其他基准测试结果包括：

      - AIME 2024 pass@1：72.6
      - MATH-500 pass@1：94.3
      - CodeForces 评分：1691

      通过 DeepSeek R1 输出的微调，该模型实现了可与更大规模前沿模型相媲美的竞争力。
  deepseek/deepseek-v3.1-terminus:
    description_cn: |-
      DeepSeek-V3.1 Terminus 是 [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) 的一次更新，在保留模型原有能力的同时，解决了用户反馈的问题（包括语言一致性和智能体能力），并进一步优化了其在代码和搜索智能体方面的性能。该模型是一个大型混合推理模型（总参数 671B，激活参数 37B），支持“思考”与“非思考”两种模式。它在 DeepSeek-V3 基础上通过两阶段长上下文训练流程扩展至最多 128K tokens，并采用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该模型提升了工具使用、代码生成和推理效率，在高难度基准测试中表现媲美 DeepSeek-R1，同时响应速度更快。它支持结构化工具调用、代码智能体和搜索智能体，适用于研究、编码及智能体工作流。
  deepseek/deepseek-v3.1-terminus:exacto:
    description_cn: |-
      DeepSeek-V3.1 Terminus 是 [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) 的一次更新，在保留模型原有能力的同时，解决了用户反馈的问题（包括语言一致性和智能体能力），并进一步优化了其在代码和搜索智能体方面的性能。该模型是一个大型混合推理模型（总参数 671B，激活参数 37B），支持“思考”与“非思考”两种模式。它在 DeepSeek-V3 基础上通过两阶段长上下文训练流程扩展至最多 128K tokens，并采用 FP8 微缩放技术实现高效推理。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该模型提升了工具使用、代码生成和推理效率，在高难度基准测试中表现媲美 DeepSeek-R1，同时响应速度更快。它支持结构化工具调用、代码智能体和搜索智能体，适用于研究、编码及智能体工作流。
  deepseek/deepseek-v3.2:
    description_cn: |-
      DeepSeek-V3.2 是一款在计算效率与强推理及智能体工具使用能力之间取得平衡的大语言模型。它引入了 DeepSeek 稀疏注意力（DSA）机制——一种细粒度稀疏注意力方法，在长上下文场景中显著降低训练与推理成本的同时保持性能。通过可扩展的强化学习后训练框架进一步提升推理能力，据报告其性能达到 GPT-5 级别，并在2025年国际数学奥林匹克（IMO）和国际信息学奥林匹克（IOI）中斩获金牌。V3.2 还采用大规模智能体任务合成流水线，将推理能力更有效地融入工具使用场景，从而提升交互环境中的指令遵循性与泛化能力。

      用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  deepseek/deepseek-v3.2-exp:
    description_cn: |-
      DeepSeek-V3.2-Exp 是 DeepSeek 发布的实验性大语言模型，作为 V3.1 与未来架构之间的中间版本。该模型引入了 DeepSeek 稀疏注意力（DSA）机制——一种细粒度稀疏注意力机制，旨在长上下文场景下提升训练与推理效率，同时保持输出质量。用户可通过 `reasoning` `enabled` 布尔参数控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该模型在与 V3.1-Terminus 对齐的条件下训练，便于直接对比。基准测试表明，其在推理、编码及智能体工具使用任务上的性能大致与 V3.1 相当，不同领域略有取舍。本次发布重点在于验证面向扩展上下文长度的架构优化，而非提升原始任务准确率，因此主要作为研究导向模型，用于探索高效 Transformer 设计。
  deepseek/deepseek-v3.2-speciale:
    description_cn: DeepSeek-V3.2-Speciale 是 DeepSeek-V3.2 的高性能计算变体，专为极致推理与智能体性能优化。它基于 DeepSeek 稀疏注意力（DSA）实现高效的长上下文处理，并通过更大规模的强化学习后训练进一步超越基础模型的能力。评估结果显示，Speciale 在高难度推理任务上优于 GPT-5，能力接近 Gemini-3.0-Pro，同时保持出色的代码生成与工具使用可靠性。与 V3.2 一样，它也受益于大规模智能体任务合成流水线，显著提升交互环境中的指令遵循性与泛化能力。
  eleutherai/llemma_7b:
    description_cn: Llemma 7B 是一款面向数学领域的语言模型。该模型以 Code Llama 7B 的权重初始化，并在 Proof-Pile-2 数据集上训练了 2000 亿个 token。Llemma 系列模型在数学领域的思维链推理以及使用 Python 和形式化定理证明器等计算工具方面表现尤为突出。
  essentialai/rnj-1-instruct:
    description_cn: Rnj-1 是由 Essential AI 开发的 80 亿参数密集型开源权重模型系列，从零开始训练，专注于编程、数学和科学推理。该模型在多种编程语言、工具调用工作流及智能体执行环境（如 mini-SWE-agent）中均展现出强大性能。
  google/gemini-2.0-flash-001:
    description_cn: Gemini Flash 2.0 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首令牌延迟（TTFT），同时保持与 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大模型相当的质量。该版本在多模态理解、编码能力、复杂指令遵循和函数调用方面均有显著增强，共同带来更流畅、更稳健的智能体体验。
  google/gemini-2.0-flash-exp:free:
    description_cn: Gemini Flash 2.0 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首 token 延迟（TTFT），同时保持与 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大模型相当的质量。该版本在多模态理解、编码能力、复杂指令遵循及函数调用方面均有显著增强，共同带来更流畅、稳健的智能体体验。
  google/gemini-2.0-flash-lite-001:
    description_cn: Gemini 2.0 Flash Lite 相较于 [Gemini Flash 1.5](/google/gemini-flash-1.5) 显著缩短了首 token 延迟（TTFT），同时在输出质量上媲美 [Gemini Pro 1.5](/google/gemini-pro-1.5) 等更大规模模型，并以极具性价比的 token 价格提供服务。
  google/gemini-2.5-flash:
    description_cn: |-
      Gemini 2.5 Flash 是 Google 最先进的主力模型，专为高级推理、编程、数学和科学任务设计。其内置“思考”能力，可提供更高准确性和更精细的上下文处理。

      此外，Gemini 2.5 Flash 可通过“推理最大 token 数”参数进行配置，详见文档（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。
  google/gemini-2.5-flash-image:
    description_cn: Gemini 2.5 Flash Image（又称“Nano Banana”）现已正式上线。这是一款具备上下文理解能力的前沿图像生成模型，支持图像生成、编辑及多轮对话。可通过 [image_config API 参数](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration) 控制图像宽高比。
  google/gemini-2.5-flash-lite:
    description_cn: Gemini 2.5 Flash-Lite 是 Gemini 2.5 系列中的轻量级推理模型，专为超低延迟与高成本效益而优化。相比早期 Flash 模型，它在吞吐量、令牌生成速度及常见基准测试性能方面均有提升。默认情况下，“思考”功能（即多轮推理）已禁用以优先保障速度，但开发者可通过 [Reasoning API 参数](https://openrouter.ai/docs/use-cases/reasoning-tokens) 启用该功能，在特定场景下以成本换取更高智能水平。
  google/gemini-2.5-flash-lite-preview-09-2025:
    description_cn: Gemini 2.5 Flash-Lite 是 Gemini 2.5 系列中的轻量级推理模型，专为超低延迟和成本效益优化。相比早期 Flash 模型，它在吞吐量、令牌生成速度及常见基准测试性能方面均有提升。默认禁用“思考”（即多轮推理）以优先保障速度，但开发者可通过 [Reasoning API 参数](https://openrouter.ai/docs/use-cases/reasoning-tokens) 启用该功能，在成本与智能之间进行权衡。
  google/gemini-2.5-flash-preview-09-2025:
    description_cn: |-
      Gemini 2.5 Flash Preview（2025 年 9 月检查点）是 Google 最先进的主力模型，专为高级推理、编程、数学和科学任务设计。它内置“思考”能力，可提供更高准确度和更精细的上下文处理。

      此外，Gemini 2.5 Flash 可通过“max tokens for reasoning”参数进行配置，详情参见文档（https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning）。
  google/gemini-2.5-pro:
    description_cn: Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学和科学任务设计。该模型具备“思考”能力，能以更高的准确性和更精细的上下文理解生成响应。Gemini 2.5 Pro 在多项基准测试中表现卓越，包括在 LMArena 排行榜上位列第一，体现出优异的人类偏好对齐能力和复杂问题解决能力。
  google/gemini-2.5-pro-preview:
    description_cn: Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学和科学任务设计。该模型具备“思考”能力，能以更高的准确性和更精细的上下文理解生成响应。Gemini 2.5 Pro 在多项基准测试中表现卓越，包括在 LMArena 排行榜上位列第一，体现出优异的人类偏好对齐能力和复杂问题解决能力。
  google/gemini-2.5-pro-preview-05-06:
    description_cn: Gemini 2.5 Pro 是 Google 最先进的 AI 模型，专为高级推理、编程、数学及科学任务设计。其具备“思考”能力，可通过增强的准确性与细致的上下文处理进行推理。Gemini 2.5 Pro 在多项基准测试中表现顶尖，包括在 LMArena 排行榜上位列第一，体现出卓越的人类偏好对齐能力与复杂问题解决实力。
  google/gemini-3-flash-preview:
    description_cn: |-
      Gemini 3 Flash Preview 是一款高速、高性价比的思维模型，专为智能体工作流、多轮对话和编程辅助而设计。其推理与工具使用性能接近 Pro 级别，但延迟显著低于更大的 Gemini 变体，非常适合交互式开发、长时间运行的智能体循环及协作编程任务。相比 Gemini 2.5 Flash，它在推理、多模态理解及可靠性方面均有全面提升。

      该模型支持 100 万 token 的上下文窗口，可处理包括文本、图像、音频、视频和 PDF 在内的多模态输入，并输出文本。支持通过思维层级（minimal、low、medium、high）配置推理强度、结构化输出、工具调用及自动上下文缓存。Gemini 3 Flash Preview 面向希望获得强大推理与智能体行为，同时避免前沿大模型高昂成本与延迟的用户。
  google/gemini-3-pro-image-preview:
    description_cn: |-
      Nano Banana Pro 是谷歌基于 Gemini 3 Pro 构建的最先进图像生成与编辑模型。相比初代 Nano Banana，它在多模态推理、现实世界对齐和高保真视觉合成方面显著提升。该模型可生成富含上下文的图形，涵盖信息图、示意图到电影级合成图像，并可通过搜索对齐整合实时信息。

      其在图像中文本渲染（包括长段落与多语言排版）、多图一致性融合以及最多五个主体的身份精准保留方面处于行业领先地位。Nano Banana Pro 新增细粒度创意控制功能，如局部编辑、光照与焦点调节、相机视角变换，并支持 2K/4K 输出及灵活宽高比。该模型专为专业级设计、产品可视化、分镜脚本及复杂多元素构图打造，同时兼顾通用图像创作工作流的高效性。
  google/gemini-3-pro-preview:
    description_cn: |-
      Gemini 3 Pro 是谷歌面向高精度多模态推理的旗舰前沿模型，融合文本、图像、视频、音频和代码领域的强大能力，并支持 100 万 token 的上下文窗口。使用多轮工具调用时必须保留推理细节，详见文档：https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks。该模型在通用推理、STEM 问题求解、事实问答及多模态理解等基准测试中表现卓越，在 LMArena、GPQA Diamond、MathArena Apex、MMMU-Pro 和 Video-MMMU 等评测中均取得领先分数。其交互强调深度与可解释性：模型旨在以最少提示推断用户意图，并输出直接、聚焦洞察的回应。

      专为高级开发与智能体工作流构建，Gemini 3 Pro 提供强大的工具调用能力、长周期规划稳定性，以及在复杂 UI、可视化和编程任务中的出色零样本生成能力。它在智能体编程（SWE-Bench Verified、Terminal-Bench 2.0）、多模态分析及结构化长文本任务（如研究综述、规划与交互式学习体验）方面尤为突出。适用场景包括自主智能体、编程助手、多模态分析、科学推理及高上下文信息处理。
  google/gemma-2-9b-it:
    description_cn: |-
      Google 推出的 Gemma 2 9B 是一款先进的开源语言模型，在其规模级别中树立了效率与性能的新标杆。

      该模型专为广泛任务设计，赋能开发者和研究人员构建创新应用，同时兼顾可访问性、安全性与成本效益。

      更多详情请参阅 [发布公告](https://blog.google/technology/developers/google-gemma-2/)。Gemma 的使用需遵守 Google 的 [Gemma 使用条款](https://ai.google.dev/gemma/terms)。
  google/gemma-2-27b-it:
    description_cn: |-
      Google 推出的 Gemma 2 27B 是一款开源模型，基于与 [Gemini 系列模型](/models?q=gemini) 相同的研究和技术构建。

      Gemma 模型适用于多种文本生成任务，包括问答、摘要和推理。

      更多详情请参阅 [发布公告](https://blog.google/technology/developers/google-gemma-2/)。Gemma 的使用需遵守 Google 的 [Gemma 使用条款](https://ai.google.dev/gemma/terms)。
  google/gemma-3n-e2b-it:free:
    description_cn: Gemma 3n E2B IT 是 Google DeepMind 开发的多模态指令微调模型，基于 60 亿参数架构，有效参数规模约为 20 亿。该模型采用 MatFormer 架构，支持嵌套子模型及通过 Mix-and-Match 框架进行模块化组合。Gemma 3n 系列针对低资源部署优化，提供 32K 上下文长度，在主流基准测试中展现出卓越的多语言能力与推理性能。此变体在包含代码、数学、网页及多模态数据的多样化语料上进行训练。
  google/gemma-3n-e4b-it:
    description_cn: |-
      Gemma 3n E4B-it 针对手机、笔记本电脑和平板等移动及低资源设备的高效执行进行了优化。该模型支持多模态输入——包括文本、视觉数据和音频——可执行文本生成、语音识别、翻译及图像分析等多样化任务。借助逐层嵌入（PLE）缓存和 MatFormer 架构等创新技术，Gemma 3n 能动态管理内存使用与计算负载，通过选择性激活模型参数显著降低运行时资源需求。

      该模型支持广泛的语种（训练涵盖 140 多种语言），并具备灵活的 32K tokens 上下文窗口。Gemma 3n 可根据任务或设备能力选择性加载参数，优化内存与计算效率，非常适合注重隐私、支持离线运行的应用及端侧 AI 解决方案。[阅读博客文章了解更多](https://developers.googleblog.com/en/introducing-gemma-3n/)
  google/gemma-3n-e4b-it:free:
    description_cn: |-
      Gemma 3n E4B-it 针对手机、笔记本电脑和平板等移动及低资源设备的高效执行进行了优化。该模型支持多模态输入——包括文本、视觉数据和音频——可执行文本生成、语音识别、翻译及图像分析等多样化任务。借助逐层嵌入（PLE）缓存和 MatFormer 架构等创新技术，Gemma 3n 能动态管理内存使用与计算负载，通过选择性激活模型参数显著降低运行时资源需求。

      该模型支持广泛的语种（训练涵盖 140 多种语言），并具备灵活的 32K tokens 上下文窗口。Gemma 3n 可根据任务或设备能力选择性加载参数，优化内存与计算效率，非常适合注重隐私、支持离线运行的应用及端侧 AI 解决方案。[阅读博客文章了解更多](https://developers.googleblog.com/en/introducing-gemma-3n/)
  google/gemma-3-4b-it:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。
  google/gemma-3-4b-it:free:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。
  google/gemma-3-12b-it:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 12B 是 Gemma 3 系列中仅次于 [Gemma 3 27B](google/gemma-3-27b-it) 的第二大模型。
  google/gemma-3-12b-it:free:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 12B 是 Gemma 3 系列中仅次于 [Gemma 3 27B](google/gemma-3-27b-it) 的第二大模型。
  google/gemma-3-27b-it:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 27B 是 Google 最新推出的开源模型，为 [Gemma 2](google/gemma-2-27b-it) 的继任者。
  google/gemma-3-27b-it:free:
    description_cn: Gemma 3 引入多模态能力，支持视觉-语言输入与文本输出，可处理长达 128k token 的上下文，支持超过 140 种语言，并在数学、推理和对话能力方面均有提升，包括结构化输出与函数调用功能。Gemma 3 27B 是 Google 最新推出的开源模型，为 [Gemma 2](google/gemma-2-27b-it) 的继任者。
  gryphe/mythomax-l2-13b:
    description_cn: Llama 2 13B 表现最佳且最受欢迎的微调模型之一，擅长生成丰富描述和角色扮演。#merge
  ibm-granite/granite-4.0-h-micro:
    description_cn: Granite-4.0-H-Micro 是 IBM Granite 4 系列中的一个 30 亿参数模型。该系列是 IBM 最新发布的模型家族，专为长上下文工具调用场景进行了微调。
  inception/mercury:
    description_cn: Mercury 是全球首款扩散式大语言模型（dLLM）。该模型采用突破性的离散扩散方法，推理速度比 GPT-4.1 Nano 和 Claude 3.5 Haiku 等已优化速度的模型快 5–10 倍，同时性能相当。Mercury 的高速度使开发者能够构建响应迅速的用户体验，适用于语音助手、搜索界面和聊天机器人等场景。更多详情请参阅[博客文章](https://www.inceptionlabs.ai/blog/introducing-mercury)。
  inception/mercury-coder:
    description_cn: Mercury Coder 是全球首款扩散式大语言模型（dLLM）。该模型采用突破性的离散扩散方法，运行速度比 Claude 3.5 Haiku 和 GPT-4o Mini 等已优化速度的模型快 5–10 倍，同时性能相当。其卓越的速度使开发者在编码时能保持流畅状态，享受快速的聊天式迭代和响应迅速的代码补全建议。在 Copilot Arena 中，Mercury Coder 在速度方面排名第一，质量方面并列第二。更多详情请参阅[此博客文章](https://www.inceptionlabs.ai/blog/introducing-mercury)。
  inflection/inflection-3-pi:
    description_cn: |-
      Inflection 3 Pi 为 Inflection 的 [Pi](https://pi.ai) 聊天机器人提供支持，涵盖背景故事、情感智能、生产力和安全性。该模型可访问最新新闻，在客户服务和角色扮演等场景中表现卓越。

      Pi 经过训练可模仿您的语气和风格——若您使用更多表情符号，Pi 也会如此！不妨尝试各种提示词和对话风格。
  inflection/inflection-3-productivity:
    description_cn: |-
      Inflection 3 Productivity 针对指令遵循进行了优化，更适合需要 JSON 输出或严格遵循指定指南的任务。该模型可访问最新新闻。

      如需类似 Pi 的情感智能，请参阅 [Inflection 3 Pi](/inflection/inflection-3-pi)。

      更多详情请见 [Inflection 官方公告](https://inflection.ai/blog/enterprise)。
  kwaipilot/kat-coder-pro:
    description_cn: |-
      KAT-Coder-Pro V1 是快手 KwaiKAT 推出的 KAT-Coder 系列中最先进的智能体编程模型，专为智能体编程任务设计，在真实软件工程场景中表现卓越，在 SWE-Bench Verified 基准测试中达到 73.4% 的解决率。

      该模型通过多阶段训练流程（包括中期训练、监督微调（SFT）、强化微调（RFT）及可扩展智能体强化学习）优化了工具使用能力、多轮交互、指令遵循、泛化能力及综合性能。
  liquid/lfm-2.2-6b:
    description_cn: LFM2 是 Liquid AI 开发的新一代混合模型，专为边缘 AI 和端侧部署而设计，在质量、速度和内存效率方面树立了新标准。
  liquid/lfm2-8b-a1b:
    description_cn: 通过收件箱界面创建的模型
  mancer/weaver:
    description_cn: 旨在复现 Claude 风格的详尽表达，但连贯性与记忆能力不及原版，适用于角色扮演或叙事场景。
  meituan/longcat-flash-chat:
    description_cn: |-
      LongCat-Flash-Chat 是一个大规模混合专家（MoE）模型，总参数量达5600亿，每输入动态激活186亿至313亿参数（平均约270亿）。其创新性地采用捷径连接的MoE架构，有效降低通信开销，在保证训练稳定性的同时实现高吞吐量，这得益于超参迁移、确定性计算和多阶段优化等先进扩展策略。

      本次发布的 LongCat-Flash-Chat 是一款非推理型基础模型，专为对话与智能体任务优化。支持最长128K token上下文窗口，在推理、编程、指令遵循及领域基准测试中表现优异，尤其擅长工具调用与复杂多步交互场景。
  meta-llama/llama-3-8b-instruct:
    description_cn: |-
      Meta 最新推出的 Llama 3 系列模型包含多种规模与版本。此 8B 指令微调版本专为高质量对话场景优化。

      在人工评估中，其表现优于主流闭源模型。

      了解更多模型发布信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3-70b-instruct:
    description_cn: |-
      Meta 最新推出的 Llama 3 系列模型包含多种规模与版本。此 70B 指令微调版本专为高质量对话场景优化。

      在人工评估中，其表现优于主流闭源模型。

      了解更多模型发布信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.1-8b-instruct:
    description_cn: |-
      Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 8B 指令微调版本兼具速度与效率。

      在人工评估中，该模型展现出优于主流闭源模型的强劲性能。

      欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.1-70b-instruct:
    description_cn: |-
      Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 70B 指令微调版本专为高质量对话场景优化。

      在人工评估中，该模型展现出优于主流闭源模型的强劲性能。

      欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.1-405b:
    description_cn: |-
      Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此为 405B 参数的基础预训练版本。

      在人工评估中，该模型展现出优于主流闭源模型的强劲性能。

      欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.1-405b-instruct:
    description_cn: |-
      万众期待的 Llama 3 百亿级（400B）模型现已发布！支持 128k 上下文长度，并在多项评测中表现卓越，Meta AI 团队持续推动开源大语言模型的前沿边界。

      Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 405B 指令微调版本专为高质量对话场景优化。

      在评测中，其性能显著优于包括 GPT-4o 和 Claude 3.5 Sonnet 在内的主流闭源模型。

      欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.1-405b-instruct:free:
    description_cn: |-
      万众期待的 Llama 3 百亿级（400B）模型现已发布！支持 128k 上下文长度，并在多项评测中表现卓越，Meta AI 团队持续推动开源大语言模型的前沿边界。

      Meta 最新推出的 Llama 3.1 系列模型包含多种规模与版本，此 405B 指令微调版本专为高质量对话场景优化。

      在评测中，其性能显著优于包括 GPT-4o 和 Claude 3.5 Sonnet 在内的主流闭源模型。

      欲了解模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3-1/)。使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-3.2-1b-instruct:
    description_cn: |-
      Llama 3.2 1B 是一款拥有 10 亿参数的语言模型，专注于高效执行自然语言任务，如摘要生成、对话交互和多语言文本分析。其较小的体量使其能在低资源环境中高效运行，同时保持出色的性能。

      该模型支持八种核心语言，并可微调以支持更多语言，是寻求轻量级但功能强大的 AI 解决方案的企业或开发者的理想选择，可在多样化的多语言场景中运行，且无需大型模型所需的高计算资源。

      点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。

      使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。
  meta-llama/llama-3.2-3b-instruct:
    description_cn: |-
      Llama 3.2 3B 是一款拥有 30 亿参数的多语言大语言模型，专为高级自然语言处理任务（如对话生成、推理和摘要）而优化。基于最新的 Transformer 架构，支持包括英语、西班牙语和印地语在内的八种语言，并可适配更多语言。

      该模型在 9 万亿 token 上进行训练，在指令遵循、复杂推理和工具调用方面表现出色。其均衡的性能使其成为跨语言文本生成应用的理想选择，兼顾准确性与效率。

      点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。

      使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。
  meta-llama/llama-3.2-3b-instruct:free:
    description_cn: |-
      Llama 3.2 3B 是一款拥有 30 亿参数的多语言大语言模型，专为高级自然语言处理任务（如对话生成、推理和摘要）而优化。基于最新的 Transformer 架构，支持包括英语、西班牙语和印地语在内的八种语言，并可适配更多语言。

      该模型在 9 万亿 token 上进行训练，在指令遵循、复杂推理和工具调用方面表现出色。其均衡的性能使其成为跨语言文本生成应用的理想选择，兼顾准确性与效率。

      点击此处查看[原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。

      使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。
  meta-llama/llama-3.2-11b-vision-instruct:
    description_cn: |-
      Llama 3.2 11B Vision 是一款拥有 110 亿参数的多模态模型，专为处理视觉与文本融合任务而设计，在图像描述生成和视觉问答等任务中表现卓越，有效弥合了语言生成与视觉推理之间的鸿沟。该模型在海量图文对数据集上预训练，在复杂高精度图像分析任务中表现出色。

      其将视觉理解与语言处理相结合的能力，使其成为内容创作、AI 驱动的客户服务及科研等需要综合视觉-语言 AI 应用领域的理想解决方案。

      点击[此处](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)查看原始模型卡。

      使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。
  meta-llama/llama-3.3-70b-instruct:
    description_cn: |-
      Meta Llama 3.3 多语言大语言模型（LLM）是一款经过预训练和指令微调的 70B 参数生成式模型（纯文本输入/输出）。该模型专为多语言对话场景优化，在主流行业基准测试中性能优于众多现有开源及闭源聊天模型。

      支持语言：英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。

      [模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)
  meta-llama/llama-3.3-70b-instruct:free:
    description_cn: |-
      Meta Llama 3.3 多语言大语言模型（LLM）是一款经过预训练和指令微调的 70B 参数生成式模型（纯文本输入/输出）。该模型专为多语言对话场景优化，在主流行业基准测试中性能优于众多现有开源及闭源聊天模型。

      支持语言：英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语和泰语。

      [模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)
  meta-llama/llama-4-maverick:
    description_cn: |-
      Llama 4 Maverick 17B Instruct（128E）是 Meta 推出的高性能多模态语言模型，基于混合专家（MoE）架构，包含 128 个专家，每次前向传播激活 170 亿参数（总计 4000 亿参数）。该模型支持 12 种语言的多语言文本与图像输入，并生成多语言文本与代码输出。Maverick 针对视觉-语言任务进行了优化，通过指令微调实现类助手行为、图像推理及通用多模态交互。

      Maverick 采用早期融合（early fusion）实现原生多模态能力，并支持 100 万 token 的上下文窗口。其训练数据涵盖约 22 万亿 token，包括精选的公开数据、授权数据及 Meta 平台数据，知识截止于 2024 年 8 月。该模型于 2025 年 4 月 5 日依据 Llama 4 社区许可证发布，适用于需要高级多模态理解与高吞吐性能的研究及商业应用。
  meta-llama/llama-4-scout:
    description_cn: |-
      Llama 4 Scout 17B Instruct（16E）是 Meta 开发的混合专家（MoE）语言模型，总参数量为 1090 亿，每次前向传播激活 170 亿参数。该模型支持原生多模态输入（文本与图像）及 12 种语言的多语言输出（文本与代码），专为助手式交互与视觉推理设计，每次前向传播使用 16 个专家，上下文长度达 1000 万 token，训练语料规模约为 40 万亿 token。

      Llama 4 Scout 采用早期融合技术以实现无缝模态集成，兼顾高效率与本地或商业部署需求。该模型经过指令微调，适用于多语言对话、图像描述生成及图像理解等任务。依据 Llama 4 社区许可证发布，训练数据截止于 2024 年 8 月，并于 2025 年 4 月 5 日公开发布。
  meta-llama/llama-guard-2-8b:
    description_cn: |-
      此安全防护模型拥有 80 亿参数，基于 Llama 3 系列构建。与前代 [LlamaGuard 1](https://huggingface.co/meta-llama/LlamaGuard-7b) 类似，它可对提示（prompt）和响应（response）进行分类。

      LlamaGuard 2 的工作方式类似于普通大语言模型，会生成文本以判断给定输入/输出是否安全。若判定为不安全，还会指出违反的内容类别。

      为获得最佳效果，请使用原始提示输入或 `/completions` 端点，而非聊天 API。

      在人工评估中，其表现优于主流闭源模型。

      有关模型发布的更多信息，请[点击此处](https://ai.meta.com/blog/meta-llama-3/)。本模型的使用须遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  meta-llama/llama-guard-3-8b:
    description_cn: |-
      Llama Guard 3 是基于 Llama-3.1-8B 预训练模型微调而成的内容安全分类模型。与早期版本类似，它可用于对大语言模型的输入（提示分类）和输出（响应分类）进行内容安全评估。该模型以文本生成方式输出判断结果，明确指出给定提示或响应是否安全；若不安全，还会列出违反的具体内容类别。

      Llama Guard 3 依据 MLCommons 标准化风险分类体系进行对齐，并专为支持 Llama 3.1 的能力而设计。具体而言，它支持 8 种语言的内容审核，并针对搜索及代码解释器工具调用场景的安全性与可靠性进行了优化。
  meta-llama/llama-guard-4-12b:
    description_cn: |-
      Llama Guard 4 是基于 Llama 4 Scout 衍生的多模态预训练模型，经微调用于内容安全分类。与早期版本类似，它可用于对大语言模型输入（提示分类）和输出（响应分类）进行安全评估。该模型以文本形式输出判断结果，明确指出给定提示或响应是否安全；若不安全，还会列出违反的具体内容类别。

      Llama Guard 4 依据标准化的 MLCommons 危害分类体系进行对齐，并专为支持 Llama 4 的多模态能力而设计。具体而言，它融合了前代 Llama Guard 模型的特性，提供对英语及多种支持语言的内容审核能力，并增强了对混合文本与图像提示（包括多图输入）的处理能力。此外，Llama Guard 4 已集成至 Llama Moderations API，为文本和图像提供强大的安全分类支持。
  microsoft/phi-4:
    description_cn: |-
      [Microsoft Research](/microsoft) Phi-4 专为复杂推理任务设计，可在内存受限或需快速响应的场景中高效运行。

      该模型拥有 140 亿参数，训练数据涵盖高质量合成数据集、精选网站内容及学术资料，并经过精细优化以准确遵循指令并维持高标准的安全性。其对英文输入效果最佳。

      更多信息请参阅 [Phi-4 技术报告](https://arxiv.org/pdf/2412.08905)
  microsoft/wizardlm-2-8x22b:
    description_cn: |-
      WizardLM-2 8x22B 是微软 AI 推出的最先进的 Wizard 系列模型，在性能上可与领先闭源模型高度竞争，并持续超越所有现有开源顶尖模型。

      该模型基于 [Mixtral 8x22B](/models/mistralai/mixtral-8x22b) 进行指令微调。

      了解更多模型发布信息，请[点击此处](https://wizardlm.github.io/WizardLM2/)。

      #moe
  minimax/minimax-01:
    description_cn: |-
      MiniMax-01 融合了用于文本生成的 MiniMax-Text-01 与用于图像理解的 MiniMax-VL-01。模型总参数量达 4560 亿，每次推理激活 459 亿参数，支持高达 400 万 token 的上下文长度。

      文本模型采用混合架构，结合 Lightning Attention、Softmax Attention 与混合专家（MoE）机制；图像模型采用“ViT-MLP-LLM”框架，并在文本模型基础上进行训练。

      更多发布详情请见：https://www.minimaxi.com/en/news/minimax-01-series-2
  minimax/minimax-m1:
    description_cn: |-
      MiniMax-M1 是一款大规模开源权重推理模型，专为长上下文和高效率推理而设计。该模型采用混合专家（MoE）架构，并结合自研的“闪电注意力”机制，可处理长达100万 token 的序列，同时保持出色的 FLOP 效率。模型总参数量达4560亿，每 token 激活459亿参数，专为复杂多步推理任务优化。

      通过自研强化学习流程（CISPO）训练，M1 在长上下文理解、软件工程、智能体工具调用和数学推理方面表现突出。在 FullStackBench、SWE-bench、MATH、GPQA 和 TAU-Bench 等基准测试中成绩优异，常优于 DeepSeek R1 和 Qwen3-235B 等其他开源模型。
  minimax/minimax-m2:
    description_cn: |-
      MiniMax-M2 是一款紧凑高效的大型语言模型，针对端到端编码和智能体工作流进行了优化。该模型激活参数达 100 亿（总计 2300 亿），在通用推理、工具调用和多步骤任务执行方面提供接近前沿的智能水平，同时保持低延迟和高部署效率。

      该模型在代码生成、多文件编辑、编译-运行-修复循环以及测试验证修复等任务上表现卓越，在 SWE-Bench Verified、Multi-SWE-Bench 和 Terminal-Bench 基准测试中取得优异成绩。在 BrowseComp 和 GAIA 等智能体评估中也具有竞争力，能有效处理长周期规划、信息检索及执行错误恢复。

      据 [Artificial Analysis](https://artificialanalysis.ai/models/minimax-m2) 测评，MiniMax-M2 在综合智能（涵盖数学、科学和指令遵循）方面位列顶尖开源模型之列。其较小的激活占用空间支持快速推理、高并发处理和更优的单位经济性，非常适合大规模智能体、开发者助手及对响应速度和成本效益有高要求的推理驱动型应用。

      为避免性能下降，MiniMax 强烈建议在对话轮次间保留推理过程。更多关于如何使用 reasoning_details 回传推理内容的信息，请参阅我们的 [文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)。
  minimax/minimax-m2.1:
    description_cn: |-
      MiniMax-M2.1 是一款轻量级、业界领先的大型语言模型，针对编程、智能体工作流及现代应用开发进行了优化。仅激活 100 亿参数，即可实现真实场景能力的显著跃升，同时保持卓越的延迟表现、可扩展性与成本效益。

      相比前代模型，M2.1 输出更简洁清晰，感知响应速度更快。其在主流系统与应用语言中展现出领先的多语言编程性能，在 Multi-SWE-Bench 上达到 49.4%，在 SWE-Bench Multilingual 上达到 72.5%，可作为 IDE、编程工具及通用助手中的多功能智能体“大脑”。

      为避免性能下降，MiniMax 强烈建议在对话轮次间保留推理过程。更多关于如何通过 reasoning_details 传递推理信息的内容，请参阅[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)。
  mistralai/codestral-2508:
    description_cn: |-
      Mistral 于 2025 年 7 月底发布的前沿代码语言模型。Codestral 专为低延迟、高频率任务设计，例如中间填充（FIM）、代码修正和测试生成。

      [博客文章](https://mistral.ai/news/codestral-25-08)
  mistralai/devstral-2512:
    description_cn: |-
      Devstral 2 是 Mistral AI 推出的前沿开源模型，专精于智能体编程。该模型为 1230 亿参数的稠密 Transformer 架构，支持 256K 上下文窗口。

      Devstral 2 能够在保持架构级上下文的前提下探索代码库并协调多文件变更。它可追踪框架依赖关系、检测失败并自动重试修正，有效应对缺陷修复和遗留系统现代化等挑战。该模型支持微调以优先处理特定语言或针对大型企业代码库进行优化，并采用修改版 MIT 许可证发布。
  mistralai/devstral-2512:free:
    description_cn: |-
      Devstral 2 是 Mistral AI 推出的前沿开源模型，专精于智能体编程。该模型为 1230 亿参数的稠密 Transformer 架构，支持 256K 上下文窗口。

      Devstral 2 能够在保持架构级上下文的前提下探索代码库并协调多文件变更。它可追踪框架依赖关系、检测失败并自动重试修正，有效应对缺陷修复和遗留系统现代化等挑战。该模型支持微调以优先处理特定语言或针对大型企业代码库进行优化，并采用修改版 MIT 许可证发布。
  mistralai/devstral-medium:
    description_cn: |-
      Devstral Medium 是由 Mistral AI 与 All Hands AI 联合开发的高性能代码生成与智能体推理模型。作为 Devstral Small 的升级版，其在 SWE-Bench Verified 基准上达到 61.6% 的准确率，超越 Gemini 2.5 Pro 与 GPT-4.1 等模型，同时成本显著更低。该模型专为在各类代码智能体与框架中泛化不同提示风格及工具使用而设计。

      Devstral Medium 仅通过 API 提供（非开源权重），支持在私有基础设施上进行企业级部署，并可选配微调功能。
  mistralai/devstral-small:
    description_cn: |-
      Devstral Small 1.1 是由 Mistral AI 与 All Hands AI 联合开发的 240 亿参数开源语言模型，专为软件工程智能体打造。该模型基于 Mistral Small 3.1 微调而成，采用 Apache 2.0 许可证发布，支持 128K 令牌上下文窗口，并兼容 Mistral 风格的函数调用与 XML 输出格式。

      专为智能体编码工作流设计，Devstral Small 1.1 针对代码库探索、多文件编辑及集成至 OpenHands、Cline 等自主开发智能体等任务进行了优化。其在 SWE-Bench Verified 基准上取得 53.6% 的成绩，超越所有其他开源模型，同时足够轻量，可在单张 RTX 4090 GPU 或 Apple Silicon 设备上运行。该模型采用 Tekken 分词器，词表大小达 131K，可通过 vLLM、Transformers、Ollama、LM Studio 及其他兼容 OpenAI 的运行时部署。
  mistralai/ministral-3b:
    description_cn: Ministral 3B 是一款拥有 30 亿参数的模型，专为设备端和边缘计算优化。它在知识理解、常识推理和函数调用方面表现卓越，在多数基准测试中优于 Mistral 7B 等更大模型。支持高达 128K 的上下文长度，非常适合编排智能体工作流及高效执行专业任务。
  mistralai/ministral-3b-2512:
    description_cn: Ministral 3 系列中最小的模型 Ministral 3 3B 是一款高效、强大的微型多模态语言模型。
  mistralai/ministral-8b:
    description_cn: Ministral 8B 是一款拥有 80 亿参数的模型，采用独特的交错滑动窗口注意力机制，实现更快、更节省内存的推理。专为边缘应用场景设计，支持高达 128K 的上下文长度，在知识理解和推理任务中表现优异。在 100 亿参数以下模型中性能领先，是低延迟、注重隐私应用的理想之选。
  mistralai/ministral-8b-2512:
    description_cn: Ministral 3 系列中的均衡之选 Ministral 3 8B 是一款高效、强大的微型多模态语言模型。
  mistralai/ministral-14b-2512:
    description_cn: Ministral 3 系列中最大的模型 Ministral 3 14B 具备前沿能力，性能可媲美更大的 Mistral Small 3.2 24B 模型，是一款兼具强大性能与高效性的多模态语言模型。
  mistralai/mistral-7b-instruct:
    description_cn: |-
      这是一款高性能、业界标准的 73 亿参数模型，针对速度和上下文长度进行了优化。

      *Mistral 7B Instruct 包含多个版本变体，此处指最新版本。*
  mistralai/mistral-7b-instruct-v0.1:
    description_cn: 一款拥有 73 亿参数的模型，在所有基准测试中均优于 Llama 2 13B，并针对推理速度和上下文长度进行了优化。
  mistralai/mistral-7b-instruct-v0.2:
    description_cn: |-
      一款高性能、行业标准的 73 亿参数模型，针对推理速度和上下文长度进行了优化。

      这是 [Mistral 7B Instruct](/modelsmistralai/mistral-7b-instruct-v0.1) 的改进版本，主要变更包括：

      - 上下文窗口扩展至 32k（v0.1 为 8k）
      - Rope-theta = 1e6
      - 移除了滑动窗口注意力机制
  mistralai/mistral-7b-instruct-v0.3:
    description_cn: |-
      这是一款高性能、业界标准的 73 亿参数模型，针对速度和上下文长度进行了优化。

      这是 [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2) 的改进版本，主要变更包括：

      - 词表扩展至 32768
      - 支持 v3 分词器
      - 支持函数调用

      注意：函数调用支持取决于具体服务提供商。
  mistralai/mistral-large:
    description_cn: |-
      Mistral Large 2（版本 `mistral-large-2407`）是 Mistral AI 的旗舰模型。作为一款提供权重的专有模型，它在推理、代码、JSON、聊天等任务上表现出色。发布详情请见[此处](https://mistral.ai/news/mistral-large-2407/)。

      支持数十种语言，包括法语、德语、西班牙语、意大利语、葡萄牙语、阿拉伯语、印地语、俄语、中文、日语和韩语，以及 80 多种编程语言，如 Python、Java、C、C++、JavaScript 和 Bash。其超长上下文窗口可从大型文档中精准提取信息。
  mistralai/mistral-large-2407:
    description_cn: |-
      这是 Mistral AI 的旗舰模型 Mistral Large 2（版本 mistral-large-2407）。该模型为闭源权重可用模型，在推理、代码、JSON、对话等方面表现卓越。发布详情请参阅[此处](https://mistral.ai/news/mistral-large-2407/)。

      支持数十种语言，包括法语、德语、西班牙语、意大利语、葡萄牙语、阿拉伯语、印地语、俄语、中文、日语和韩语，并支持 80 多种编程语言，如 Python、Java、C、C++、JavaScript 和 Bash。其长上下文窗口可精准从大型文档中检索信息。
  mistralai/mistral-large-2411:
    description_cn: |-
      Mistral Large 2 2411 是与 [Pixtral Large 2411](/mistralai/pixtral-large-2411) 同时发布的 [Mistral Large 2](/mistralai/mistral-large) 的更新版本。

      相比此前的 [Mistral Large 24.07](/mistralai/mistral-large-2407)，它在长上下文理解、新系统提示以及函数调用准确性方面均有显著提升。
  mistralai/mistral-large-2512:
    description_cn: Mistral Large 3 2512 是 Mistral 迄今为止最强大的模型，采用稀疏混合专家架构，激活参数达 410 亿（总计 6750 亿），并以 Apache 2.0 许可证发布。
  mistralai/mistral-medium-3:
    description_cn: |-
      Mistral Medium 3 是一款高性能的企业级语言模型，旨在以显著降低的运营成本提供前沿级能力。该模型在尖端推理与多模态性能之间取得平衡，成本仅为传统大模型的八分之一，适用于专业及工业场景的大规模部署。

      该模型在编程、STEM 推理及企业适配等领域表现卓越，支持混合、本地及 VPC 内部署，并针对自定义工作流集成进行了优化。Mistral Medium 3 在准确性方面可与 Claude Sonnet 3.5/3.7、Llama 4 Maverick 和 Command R+ 等更大模型竞争，同时在各类云环境中保持广泛的兼容性。
  mistralai/mistral-medium-3.1:
    description_cn: |-
      Mistral Medium 3.1 是 Mistral Medium 3 的升级版，是一款高性能企业级语言模型，旨在以显著降低的运营成本提供前沿能力。相比传统大模型，其成本降低 8 倍，同时兼顾顶尖的推理与多模态性能，适用于专业及工业场景的大规模部署。

      该模型在编程、STEM 推理和企业适配等领域表现卓越，支持混合部署、本地部署及 VPC 内部署，并针对自定义工作流集成进行了优化。在精度方面可与 Claude Sonnet 3.5/3.7、Llama 4 Maverick 和 Command R+ 等更大模型相媲美，同时保持广泛的云环境兼容性。
  mistralai/mistral-nemo:
    description_cn: |-
      Mistral 与 NVIDIA 联合开发的 120 亿参数模型，支持 128k token 上下文长度。

      该模型支持多语言，包括英语、法语、德语、西班牙语、意大利语、葡萄牙语、中文、日语、韩语、阿拉伯语和印地语。

      支持函数调用，并以 Apache 2.0 许可证发布。
  mistralai/mistral-saba:
    description_cn: Mistral Saba 是一款专为中东和南亚地区设计的 240 亿参数语言模型，在保持高效性能的同时提供准确且符合本地语境的响应。该模型基于精选的区域性数据集训练，支持多种印度本土语言（包括泰米尔语和马拉雅拉姆语）以及阿拉伯语，适用于广泛的区域性和多语言应用场景。更多详情请参阅[此博客文章](https://mistral.ai/en/news/mistral-saba)
  mistralai/mistral-small-3.1-24b-instruct:
    description_cn: Mistral Small 3.1 24B Instruct 是 Mistral Small 3（2501）的升级版本，拥有 240 亿参数并具备先进的多模态能力。该模型在文本推理与视觉任务（包括图像分析、编程、数学推理及数十种语言的多语言支持）方面达到业界领先水平。配备高达 128K token 的上下文窗口，并针对高效本地推理进行优化，适用于对话代理、函数调用、长文档理解及隐私敏感型部署等场景。其更新版本为 [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)。
  mistralai/mistral-small-3.1-24b-instruct:free:
    description_cn: Mistral Small 3.1 24B Instruct 是 Mistral Small 3（2501）的升级版本，拥有 240 亿参数并具备先进的多模态能力。该模型在文本推理与视觉任务（包括图像分析、编程、数学推理及数十种语言的多语言支持）方面达到业界领先水平。配备高达 128K token 的上下文窗口，并针对高效本地推理进行优化，适用于对话代理、函数调用、长文档理解及隐私敏感型部署等场景。其更新版本为 [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)。
  mistralai/mistral-small-3.2-24b-instruct:
    description_cn: |-
      Mistral-Small-3.2-24B-Instruct-2506 是 Mistral 推出的更新版 240 亿参数模型，专为指令遵循、减少重复输出及改进函数调用而优化。相比 3.1 版本，3.2 版在 WildBench 和 Arena Hard 基准上显著提升准确性，有效抑制无限生成问题，并在工具使用和结构化输出任务中取得明显进步。

      该模型支持图文输入与结构化输出、函数/工具调用，在编程（HumanEval+、MBPP）、STEM（MMLU、MATH、GPQA）及视觉（ChartQA、DocVQA）等基准测试中表现优异。
  mistralai/mistral-small-24b-instruct-2501:
    description_cn: |-
      Mistral Small 3 是一款 240 亿参数的语言模型，针对常见 AI 任务优化了低延迟性能。该模型以 Apache 2.0 许可证发布，提供预训练版和指令微调版，适用于高效的本地部署。

      模型在 MMLU 基准测试中达到 81% 的准确率，性能可与 Llama 3.3 70B 和 Qwen 32B 等更大模型竞争，且在同等硬件上运行速度提升三倍。[点击此处阅读模型博客文章。](https://mistral.ai/news/mistral-small-3/)
  mistralai/mistral-small-creative:
    description_cn: Mistral Small Creative 是一款实验性小型模型，专为创意写作、叙事生成、角色扮演与人物驱动对话、通用指令遵循及对话代理而设计。
  mistralai/mistral-tiny:
    description_cn: |-
      注意：此模型即将弃用。推荐使用更新的 [Ministral 8B](/mistral/ministral-8b)。

      当前该模型基于 Mistral-7B-v0.2 构建，并采用了受社区工作启发的“更优”微调策略，相较于 [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1) 有所提升。适用于对成本敏感但对推理能力要求不高的大批量处理任务。
  mistralai/mixtral-8x7b-instruct:
    description_cn: |-
      Mixtral 8x7B Instruct 是 Mistral AI 开发的预训练生成式稀疏混合专家（Sparse Mixture of Experts）模型，专为聊天和指令遵循场景设计，包含 8 个专家（前馈网络），总参数量达 470 亿。

      该指令微调版本由 Mistral 官方提供。#moe
  mistralai/mixtral-8x22b-instruct:
    description_cn: |-
      Mistral 官方发布的 [Mixtral 8x22B](/models/mistralai/mixtral-8x22b) 指令微调版本。该模型在 141B 总参数中激活 39B 参数，以同等规模实现无与伦比的成本效益。其优势包括：
      - 出色的数学、编程与推理能力
      - 超长上下文长度（64k）
      - 流利支持英语、法语、意大利语、德语和西班牙语

      基准测试结果详见发布公告：[此处](https://mistral.ai/news/mixtral-8x22b/)。
      #moe
  mistralai/pixtral-12b:
    description_cn: Mistral AI 推出的首款多模态文本+图像到文本模型。其权重已通过种子文件发布：https://x.com/mistralai/status/1833758285167722836。
  mistralai/pixtral-large-2411:
    description_cn: |-
      Pixtral Large 是一个拥有 1240 亿参数的开源权重多模态模型，基于 [Mistral Large 2](/mistralai/mistral-large-2411) 构建，能够理解文档、图表和自然图像。

      该模型依据 Mistral 研究许可（MRL）可用于研究与教育用途，并可通过 Mistral 商业许可用于商业目的的实验、测试和生产部署。
  mistralai/voxtral-small-24b-2507:
    description_cn: Voxtral Small 是 Mistral Small 3 的增强版，在保留业界领先的文本性能的同时，集成了先进的音频输入能力，擅长语音转录、翻译和音频理解。音频输入定价为每百万秒 100 美元。
  moonshotai/kimi-dev-72b:
    description_cn: Kimi-Dev-72B 是一款面向软件工程和问题修复任务微调的开源大语言模型。基于 Qwen2.5-72B，通过大规模强化学习进行优化：在真实代码仓库中应用代码补丁，并通过完整测试套件验证，仅对正确且鲁棒的补全结果给予奖励。该模型在 SWE-bench Verified 上达到60.4%，在开源模型中树立了软件缺陷修复与代码推理的新标杆。
  moonshotai/kimi-k2:
    description_cn: Kimi K2 Instruct 是月之暗面（Moonshot AI）开发的大规模混合专家（MoE）语言模型，总参数量达 1 万亿，每次前向传播激活 320 亿参数。该模型针对智能体能力进行优化，支持高级工具调用、复杂推理与代码合成。Kimi K2 在多项基准测试中表现卓越，尤其在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）及工具使用（Tau2、AceBench）任务中优势显著。模型支持最长 128K 令牌的长上下文推理，并采用包含 MuonClip 优化器的新型训练栈，确保大规模 MoE 模型训练的稳定性。
  moonshotai/kimi-k2-0905:
    description_cn: |-
      Kimi K2 0905 是 [Kimi K2 0711](moonshotai/kimi-k2) 的九月更新版本，由月之暗面（Moonshot AI）开发的大规模专家混合（MoE）语言模型，总参数量达 1 万亿，每次前向传递激活 320 亿参数。支持最长 256k tokens 的长上下文推理（此前为 128k）。

      本次更新提升了智能体编程的准确率与跨框架泛化能力，并增强了前端编程在 Web、3D 等相关任务中的输出美观性与功能性。Kimi K2 针对智能体能力进行了优化，包括高级工具使用、推理与代码合成，在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）和工具使用（Tau2、AceBench）等基准测试中表现优异。该模型采用包含 MuonClip 优化器的新训练栈，以实现稳定的大规模 MoE 训练。
  moonshotai/kimi-k2-0905:exacto:
    description_cn: |-
      Kimi K2 0905 是 [Kimi K2 0711](moonshotai/kimi-k2) 的九月更新版本，由月之暗面（Moonshot AI）开发的大规模专家混合（MoE）语言模型，总参数量达 1 万亿，每次前向传递激活 320 亿参数。支持最长 256k tokens 的长上下文推理（此前为 128k）。

      本次更新提升了智能体编程的准确率与跨框架泛化能力，并增强了前端编程在 Web、3D 等相关任务中的输出美观性与功能性。Kimi K2 针对智能体能力进行了优化，包括高级工具使用、推理与代码合成，在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）和工具使用（Tau2、AceBench）等基准测试中表现优异。该模型采用包含 MuonClip 优化器的新训练栈，以实现稳定的大规模 MoE 训练。
  moonshotai/kimi-k2-thinking:
    description_cn: |-
      Kimi K2 Thinking 是月之暗面（Moonshot AI）迄今最先进的开源推理模型，将 K2 系列拓展至智能体驱动的长周期推理领域。该模型基于 Kimi K2 引入的万亿参数混合专家（MoE）架构，每次前向传递激活 320 亿参数，支持 256k token 的上下文窗口。模型针对持续逐步推理、动态工具调用及跨越数百轮次的复杂推理工作流进行了优化，能将逐步推理与工具使用交错执行，实现无需漂移的数百步连续自主研究、编程与写作。

      该模型在 HLE、BrowseComp、SWE-Multilingual 和 LiveCodeBench 等开源基准上创下新纪录，并能在 200–300 次工具调用中保持稳定的多智能体行为。依托大规模 MoE 架构与 MuonClip 优化，它在高推理深度与高推理效率之间取得平衡，适用于高要求的智能体与分析任务。
  moonshotai/kimi-k2:free:
    description_cn: Kimi K2 Instruct 是月之暗面（Moonshot AI）开发的大规模混合专家（MoE）语言模型，总参数量达 1 万亿，每次前向传播激活 320 亿参数。该模型针对智能体能力进行优化，支持高级工具调用、复杂推理与代码合成。Kimi K2 在多项基准测试中表现卓越，尤其在编程（LiveCodeBench、SWE-bench）、推理（ZebraLogic、GPQA）及工具使用（Tau2、AceBench）任务中优势显著。模型支持最长 128K 令牌的长上下文推理，并采用包含 MuonClip 优化器的新型训练栈，确保大规模 MoE 模型训练的稳定性。
  morph/morph-v3-fast:
    description_cn: |-
      Morph 最快的代码编辑应用模型，推理速度约 10,500 tokens/秒，准确率达 96%，适用于快速代码转换。

      该模型要求提示格式如下：
      <instruction>{instruction}</instruction>
      <code>{initial_code}</code>
      <update>{edit_snippet}</update>

      Morph 已启用零数据留存策略。更多模型信息请参阅其[文档](https://docs.morphllm.com/quickstart)。
  morph/morph-v3-large:
    description_cn: |-
      Morph 高精度代码编辑应用模型，适用于复杂代码修改，推理速度约 4,500 tokens/秒，准确率达 98%，可实现精准代码转换。

      该模型要求提示格式如下：
      <instruction>{instruction}</instruction>
      <code>{initial_code}</code>
      <update>{edit_snippet}</update>

      Morph 已启用零数据留存策略。更多模型信息请参阅其[文档](https://docs.morphllm.com/quickstart)。
  neversleep/llama-3.1-lumimaid-8b:
    description_cn: |-
      Lumimaid v0.2 8B 是基于 [Llama 3.1 8B](/models/meta-llama/llama-3.1-8b-instruct) 微调的模型，相比 Lumimaid v0.1 在数据集方面实现了“巨大飞跃”，并清理了低质量的聊天输出。

      使用本模型需遵守 [Meta 可接受使用政策](https://llama.meta.com/llama3/use-policy/)。
  neversleep/noromaid-20b:
    description_cn: |-
      由 IkariDev 与 Undi 联合开发的合并模型，适用于角色扮演（RP）、成人角色扮演（ERP）及通用知识任务。

      #merge #uncensored
  nex-agi/deepseek-v3.1-nex-n1:
    description_cn: |-
      DeepSeek V3.1 Nex-N1 是 Nex-N1 系列的旗舰版本——一款经过后训练的模型，旨在突出智能体自主性、工具使用能力及现实生产力。

      Nex-N1 在所有评估场景中均展现出竞争力，在实际编码和 HTML 生成任务中表现尤为突出。
  nousresearch/deephermes-3-mistral-24b-preview:
    description_cn: |-
      DeepHermes 3（Mistral 24B 预览版）是由 Nous Research 基于 Mistral-Small-24B 开发的指令微调语言模型，专为聊天、函数调用及高级多轮推理设计。该模型引入双模式系统，可通过特殊系统提示在直观聊天响应与结构化“深度推理”模式间切换。经 R1 蒸馏微调，支持结构化输出（JSON 模式）及面向智能体应用的函数调用语法。

      DeepHermes 3 支持**通过系统提示启用推理切换**，允许用户在快速直观响应与深思熟虑的多步推理之间自由转换。当使用以下特定系统指令激活时，模型将进入*“深度思考”*模式——在给出最终答案前，生成包含在 `<think></think>` 标签内的长链思维过程。

      系统提示：你是一个深度思考型 AI，可使用极长的思维链深入分析问题，并通过系统化推理过程与自身反复推演，以得出正确解答。请将你的思考与内心独白置于 <think> </think> 标签内，随后提供问题的解决方案或回答。
  nousresearch/hermes-2-pro-llama-3-8b:
    description_cn: Hermes 2 Pro 是 Nous Hermes 2 的升级再训练版本，采用更新并清洗后的 OpenHermes 2.5 数据集，并新增了内部开发的函数调用与 JSON 模式数据集。
  nousresearch/hermes-3-llama-3.1-70b:
    description_cn: |-
      Hermes 3 是一款通用语言模型，在 [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo) 基础上进行了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理、多轮对话、长上下文连贯性，以及全方位性能提升。

      Hermes 3 70B 是对 [Llama-3.1 70B 基础模型](/models/meta-llama/llama-3.1-70b-instruct) 进行的具有竞争力（甚至更优）的微调版本，专注于将大语言模型与用户需求对齐，赋予终端用户强大的引导能力和控制权。

      Hermes 3 系列在 Hermes 2 能力基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手能力，以及改进的代码生成技能。
  nousresearch/hermes-3-llama-3.1-405b:
    description_cn: |-
      Hermes 3 是一款通用语言模型，在 Hermes 2 基础上实现了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理能力、多轮对话连贯性、长上下文一致性，以及全方位性能提升。

      Hermes 3 405B 是基于 Llama-3.1 405B 基础模型进行全参数微调的前沿级别模型，专注于将大语言模型与用户意图对齐，赋予终端用户强大的引导与控制能力。

      Hermes 3 系列在 Hermes 2 能力集基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手功能，以及增强的代码生成技能。

      在通用能力方面，Hermes 3 与 Llama-3.1 Instruct 模型相比具备竞争力，甚至更优，二者各有优势与不足。
  nousresearch/hermes-3-llama-3.1-405b:free:
    description_cn: |-
      Hermes 3 是一款通用语言模型，在 Hermes 2 基础上进行了多项改进，包括更先进的智能体能力、更出色的角色扮演、推理、多轮对话、长上下文连贯性，以及全方位性能提升。

      Hermes 3 405B 是基于 Llama-3.1 405B 基础模型进行的全参数微调前沿级模型，专注于将大语言模型与用户需求对齐，赋予终端用户强大的引导能力和控制权。

      Hermes 3 系列在 Hermes 2 能力基础上进一步拓展，包括更强大可靠的函数调用与结构化输出能力、通用助手能力，以及改进的代码生成技能。

      在通用能力方面，Hermes 3 与 Llama-3.1 Instruct 模型相比具有竞争力，甚至更优，两者各有优势与不足。
  nousresearch/hermes-4-70b:
    description_cn: |-
      Hermes 4 70B 是 Nous Research 基于 Meta-Llama-3.1-70B 开发的混合推理模型，引入了与更大规模 405B 版本相同的混合模式，允许模型直接响应或在回答前生成显式的 <think>...</think> 推理轨迹。用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该 70B 版本使用扩展后的后训练语料库（约 600 亿 tokens）进行训练，重点强化经验证的推理数据，从而在数学、编程、STEM、逻辑和结构化输出方面取得显著提升，同时保持通用助手性能。支持 JSON 模式、模式遵循、函数调用和工具使用，具备更强的可控性并降低拒答率。
  nousresearch/hermes-4-405b:
    description_cn: |-
      Hermes 4 是由 Nous Research 基于 Meta-Llama-3.1-405B 构建的大规模推理模型，引入了混合推理模式：模型可选择通过 <think>...</think> 标记进行内部推理，或直接响应，从而在速度与深度之间灵活权衡。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)

      该模型经过指令微调，使用了约 600 亿 token 的扩展后训练语料，重点强化推理轨迹，在数学、代码、STEM 及逻辑推理方面性能显著提升，同时保留广泛的助手功能。此外，支持结构化输出（包括 JSON 模式、模式遵循、函数调用和工具使用）。Hermes 4 经过专门训练，具备更强的可控性、更低的拒答率，并对齐至中立、用户导向的行为模式。
  nvidia/llama-3.1-nemotron-70b-instruct:
    description_cn: |-
      NVIDIA 的 Llama 3.1 Nemotron 70B 是一款旨在生成精准且实用回复的语言模型。基于 [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) 架构并结合人类反馈强化学习（RLHF），在自动对齐基准测试中表现卓越。该模型专为需要高准确性的帮助性和回复生成任务而设计，适用于跨领域的多样化用户查询。

      使用本模型需遵守 [Meta 可接受使用政策](https://www.llama.com/llama3/use-policy/)。
  nvidia/llama-3.1-nemotron-ultra-253b-v1:
    description_cn: |-
      Llama-3.1-Nemotron-Ultra-253B-v1 是一款专为高级推理、人机交互对话、检索增强生成（RAG）及工具调用任务优化的大语言模型（LLM）。该模型基于 Meta 的 Llama-3.1-405B-Instruct，通过神经架构搜索（NAS）进行了深度定制，显著提升了效率、降低了内存占用并改善了推理延迟。模型支持最高 128K token 的上下文长度，并可在 8 块 NVIDIA H100 GPU 节点上高效运行。

      注意：必须在系统提示中包含 `detailed thinking on` 才能启用推理功能。更多详情请参阅 [使用建议](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations)。
  nvidia/llama-3.3-nemotron-super-49b-v1.5:
    description_cn: |-
      Llama-3.3-Nemotron-Super-49B-v1.5 是一个以英语为中心的 490 亿参数推理/对话模型，基于 Meta 的 Llama-3.3-70B-Instruct 并支持 128K 上下文。该模型通过监督微调（SFT）针对智能体工作流（RAG、工具调用）在数学、代码、科学及多轮对话任务上进行后训练，随后经历多阶段强化学习：基于奖励感知的偏好优化（RPO）用于对齐，基于可验证奖励的强化学习（RLVR）用于逐步推理，以及迭代式 DPO 以优化工具使用行为。通过蒸馏驱动的神经架构搜索（“Puzzle”）替换部分注意力模块并调整前馈网络宽度，显著降低内存占用并提升吞吐量，从而支持单 GPU（H100/H200）部署，同时保留指令遵循能力和思维链（CoT）质量。

      在内部评估中（NeMo-Skills，最多 16 次运行，温度=0.6，top_p=0.95），该模型展现出强大的推理与编码能力，例如 MATH500 pass@1 = 97.4、AIME-2024 = 87.5、AIME-2025 = 82.71、GPQA = 71.97、LiveCodeBench（24.10–25.02）= 73.58 以及 MMLU-Pro（CoT）= 79.53。该模型面向实际推理效率（高 tokens/s、低显存占用），支持 Transformers/vLLM，并提供显式的“推理开/关”模式（默认聊天优先，关闭时推荐使用贪心解码）。适用于构建对准确率与成本平衡性及可靠工具使用有要求的智能体、助手和长上下文检索系统。
  nvidia/nemotron-3-nano-30b-a3b:
    description_cn: |-
      NVIDIA Nemotron 3 Nano 30B A3B 是一款面向开发者的小型语言专家混合（MoE）模型，在计算效率与准确性方面表现卓越，适用于构建专用智能体 AI 系统。

      该模型完全开源，提供开放权重、数据集及训练方案，便于开发者轻松定制、优化并在自有基础设施上部署，以实现最高级别的隐私与安全。

      注意：免费端点的所有提示与输出均会被记录，用于改进提供商的模型及其产品与服务。请勿上传任何个人、机密或其他敏感信息。此为试用版本，不得用于生产环境或关键业务系统。
  nvidia/nemotron-3-nano-30b-a3b:free:
    description_cn: |-
      NVIDIA Nemotron 3 Nano 30B A3B 是一款面向开发者的小型语言专家混合（MoE）模型，在计算效率与准确性方面表现卓越，适用于构建专用智能体 AI 系统。

      该模型完全开源，提供开放权重、数据集及训练方案，便于开发者轻松定制、优化并在自有基础设施上部署，以实现最高级别的隐私与安全。

      注意：免费端点的所有提示与输出均会被记录，用于改进提供商的模型及其产品与服务。请勿上传任何个人、机密或其他敏感信息。此为试用版本，不得用于生产环境或关键业务系统。
  nvidia/nemotron-nano-9b-v2:
    description_cn: |-
      NVIDIA-Nemotron-Nano-9B-v2 是 NVIDIA 从零开始训练的大语言模型（LLM），设计为统一处理推理与非推理任务。模型在响应用户查询和任务时，首先生成推理轨迹，再给出最终答案。

      其推理能力可通过系统提示词进行控制。若用户希望模型直接提供最终答案而不显示中间推理轨迹，亦可进行相应配置。
  nvidia/nemotron-nano-9b-v2:free:
    description_cn: |-
      NVIDIA-Nemotron-Nano-9B-v2 是 NVIDIA 从零开始训练的大语言模型（LLM），设计为统一处理推理与非推理任务。模型在响应用户查询和任务时，首先生成推理轨迹，再给出最终答案。

      其推理能力可通过系统提示词进行控制。若用户希望模型直接提供最终答案而不显示中间推理轨迹，亦可进行相应配置。
  nvidia/nemotron-nano-12b-v2-vl:
    description_cn: |-
      NVIDIA Nemotron Nano 2 VL 是一款拥有 120 亿参数的开源多模态推理模型，专为视频理解和文档智能设计。该模型采用混合 Transformer-Mamba 架构，结合了 Transformer 级别的准确性与 Mamba 的内存高效序列建模能力，显著提升吞吐量并降低延迟。

      该模型支持文本和多图像文档输入，并生成自然语言输出。其训练数据为 NVIDIA 精心构建的高质量合成数据集，针对光学字符识别（OCR）、图表推理和多模态理解进行了优化。

      Nemotron Nano 2 VL 在 OCRBench v2 上取得领先成果，并在 MMMU、MathVista、AI2D、OCRBench、OCR-Reasoning、ChartQA、DocVQA 和 Video-MME 等基准测试中平均得分约 74，超越此前所有开源视觉语言基线模型。借助高效视频采样（EVS）技术，该模型可处理长视频内容，同时降低推理成本。

      模型权重、训练数据和微调方案均以宽松的 NVIDIA 开源许可证发布，并支持在 NeMo、NIM 及主流推理运行时环境中部署。
  nvidia/nemotron-nano-12b-v2-vl:free:
    description_cn: |-
      NVIDIA Nemotron Nano 2 VL 是一款拥有 120 亿参数的开源多模态推理模型，专为视频理解和文档智能设计。该模型采用混合 Transformer-Mamba 架构，结合了 Transformer 级别的准确性与 Mamba 的内存高效序列建模能力，显著提升吞吐量并降低延迟。

      该模型支持文本和多图像文档输入，并生成自然语言输出。其训练数据为 NVIDIA 精心构建的高质量合成数据集，针对光学字符识别（OCR）、图表推理和多模态理解进行了优化。

      Nemotron Nano 2 VL 在 OCRBench v2 上取得领先成果，并在 MMMU、MathVista、AI2D、OCRBench、OCR-Reasoning、ChartQA、DocVQA 和 Video-MME 等基准测试中平均得分约 74，超越此前所有开源视觉语言基线模型。借助高效视频采样（EVS）技术，该模型可处理长视频内容，同时降低推理成本。

      模型权重、训练数据和微调方案均以宽松的 NVIDIA 开源许可证发布，并支持在 NeMo、NIM 及主流推理运行时环境中部署。
  openai/chatgpt-4o-latest:
    description_cn: |-
      OpenAI ChatGPT-4o 由 OpenAI 持续更新，始终指向 ChatGPT 当前使用的 GPT-4o 版本。因此，与 API 版本的 [GPT-4o](/models/openai/gpt-4o) 略有不同，它额外应用了 RLHF 微调，主要用于研究与评估。

      OpenAI 指出，该模型不适用于生产环境，未来可能被移除或重定向至其他模型。
  openai/gpt-3.5-turbo:
    description_cn: |-
      GPT-3.5 Turbo 是 OpenAI 速度最快的模型，可理解和生成自然语言或代码，专为聊天及传统补全任务优化。

      训练数据截止至 2021 年 9 月。
  openai/gpt-3.5-turbo-16k:
    description_cn: 该模型的上下文长度是 gpt-3.5-turbo 的四倍，单次请求可处理约 20 页文本，但成本更高。训练数据截止至 2021 年 9 月。
  openai/gpt-3.5-turbo-0613:
    description_cn: |-
      GPT-3.5 Turbo 是 OpenAI 最快的模型，可理解并生成自然语言或代码，专为聊天及传统补全任务优化。

      训练数据截止于 2021 年 9 月。
  openai/gpt-3.5-turbo-instruct:
    description_cn: 该模型是 GPT-3.5 Turbo 的变体，专为指令类提示进行调优，未包含面向聊天场景的优化。训练数据截止至 2021 年 9 月。
  openai/gpt-4:
    description_cn: OpenAI 旗舰模型 GPT-4 是一款大规模多模态语言模型，凭借更广泛的知识储备和更强的推理能力，在解决复杂问题方面比以往模型更为精准。训练数据截止至 2021 年 9 月。
  openai/gpt-4o:
    description_cn: |-
      GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本与图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。

      在与其他模型的基准对比中，该模型曾短暂命名为 ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)。

      #multimodal
  openai/gpt-4o-2024-05-13:
    description_cn: |-
      GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本和图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但推理速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。

      在与其他模型的基准测试中，该模型曾短暂使用代号 ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)。

      #多模态
  openai/gpt-4o-2024-08-06:
    description_cn: |-
      GPT-4o 2024-08-06 版本在结构化输出方面表现更佳，支持在 response_format 中指定 JSON Schema。更多详情请参阅[此处](https://openai.com/index/introducing-structured-outputs-in-the-api/)。

      GPT-4o（“o”代表“omni”）是 OpenAI 最新的人工智能模型，支持文本与图像输入并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但推理速度提升一倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面亦有显著增强。

      在与其他模型的基准测试中，该模型曾短暂使用代号 ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)。
  openai/gpt-4o-2024-11-20:
    description_cn: |-
      GPT-4o 2024-11-20 版本提升了创意写作能力，生成更自然、生动且高度定制化的文本，显著增强相关性与可读性。同时，在处理上传文件方面表现更优，可提供更深入的洞察和更全面的回答。

      GPT-4o（“o”代表“omni”）是 OpenAI 最新一代 AI 模型，支持文本和图像输入并输出文本。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升一倍，成本降低 50%。此外，该版本在非英语语言处理和视觉能力方面也有进一步增强。
  openai/gpt-4o-audio-preview:
    description_cn: gpt-4o-audio-preview 模型新增对音频输入作为提示的支持。此增强功能使模型能够识别音频录音中的细微差别，从而丰富生成的用户体验。目前暂不支持音频输出。音频 token 的定价为每百万输入音频 token 40 美元。
  openai/gpt-4o-mini:
    description_cn: |-
      GPT-4o mini 是继 [GPT-4 Omni](/models/openai/gpt-4o) 之后 OpenAI 推出的最新模型，支持文本和图像输入，并生成文本输出。

      作为其最先进的小型模型，其成本远低于其他近期前沿模型，比 [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo) 便宜 60% 以上，同时保持业界领先的智能水平，性价比显著提升。

      GPT-4o mini 在 MMLU 基准测试中取得 82% 的得分，目前在主流聊天偏好排行榜上超越 GPT-4（参见 [常见排行榜](https://arena.lmsys.org/)）。

      了解更多详情，请查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)。

      #多模态
  openai/gpt-4o-mini-2024-07-18:
    description_cn: |-
      GPT-4o mini 是继 [GPT-4 Omni](/models/openai/gpt-4o) 之后 OpenAI 推出的最新模型，支持文本和图像输入，并生成文本输出。

      作为其最先进的小型模型，其成本远低于其他近期前沿模型，比 [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo) 便宜 60% 以上，同时保持业界领先的智能水平，性价比显著提升。

      GPT-4o mini 在 MMLU 基准测试中取得 82% 的得分，目前在主流聊天偏好排行榜上超越 GPT-4（参见 [常见排行榜](https://arena.lmsys.org/)）。

      了解更多详情，请查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)。

      #多模态
  openai/gpt-4o-mini-search-preview:
    description_cn: GPT-4o mini Search Preview 是专用于聊天补全（Chat Completions）中网络搜索的专用模型，经过训练以理解并执行网络搜索查询。
  openai/gpt-4o-search-preview:
    description_cn: GPT-4o Search Preview 是专用于聊天补全（Chat Completions）中网络搜索的专用模型，经过训练以理解并执行网络搜索查询。
  openai/gpt-4o:extended:
    description_cn: |-
      GPT-4o（“o”代表“omni”）是 OpenAI 最新推出的 AI 模型，支持文本与图像输入，并生成文本输出。其智能水平与 [GPT-4 Turbo](/models/openai/gpt-4-turbo) 相当，但速度提升两倍，成本降低 50%。此外，GPT-4o 在非英语语言处理和视觉能力方面均有显著增强。

      在与其他模型的基准对比中，该模型曾短暂命名为 ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)。

      #multimodal
  openai/gpt-4-0314:
    description_cn: GPT-4-0314 是 GPT-4 的首个发布版本，上下文长度为 8,192 个 token，支持持续至 6 月 14 日。训练数据截止至 2021 年 9 月。
  openai/gpt-4-1106-preview:
    description_cn: |-
      最新的 GPT-4 Turbo 模型，具备视觉能力。视觉请求现已支持 JSON 模式与函数调用。

      训练数据截止至 2023 年 4 月。
  openai/gpt-4-turbo:
    name_cn: GPT-4 Turbo
    description_cn: OpenAI 的高性能模型，支持 128k 上下文。
    aliases:
      - gpt-4-turbo
      - gpt4t
  openai/gpt-4-turbo-preview:
    description_cn: |-
      预览版 GPT-4 模型，改进了指令遵循能力，支持 JSON 模式、可复现输出、并行函数调用等功能。训练数据截止于 2023 年 12 月。

      **注意**：预览期间受 OpenAI 严格速率限制。
  openai/gpt-4.1:
    description_cn: GPT-4.1 是一款旗舰级大语言模型，针对高级指令遵循、现实世界软件工程和长上下文推理进行了优化。支持 100 万 token 的上下文窗口，在编码（SWE-bench Verified 达 54.6%）、指令遵循（IFEval 达 87.4%）和多模态理解等基准测试中均优于 GPT-4o 和 GPT-4.5。该模型专为精确代码差异生成、智能体可靠性以及在大型文档上下文中实现高召回率而调优，非常适合用于智能体、IDE 工具和企业知识检索场景。
  openai/gpt-4.1-mini:
    description_cn: GPT-4.1 Mini 是一款中等规模模型，在显著降低延迟和成本的同时，性能可与 GPT-4o 相媲美。它保留了 100 万 token 的上下文窗口，在困难指令评估中得分为 45.1%，MultiChallenge 得分为 35.8%，IFEval 得分为 84.1%。Mini 在编码能力（如 Aider 多语言差异基准达 31.6%）和视觉理解方面也表现出色，适用于对性能要求严苛的交互式应用。
  openai/gpt-4.1-nano:
    description_cn: 对于低延迟任务，GPT-4.1 Nano 是 GPT-4.1 系列中速度最快、成本最低的模型。其体积小巧却性能卓越，配备 100 万 token 上下文窗口，在 MMLU 上得分 80.1%，GPQA 上得分 50.3%，Aider 多语言编码基准上得分 9.8%——甚至高于 GPT-4o Mini。该模型非常适合分类或自动补全等任务。
  openai/gpt-5:
    description_cn: GPT-5 是 OpenAI 最先进的模型，在推理能力、代码质量和用户体验方面实现重大提升。该模型针对需要逐步推理、指令遵循以及在高风险场景中保持高准确性的复杂任务进行了优化。支持测试时路由功能和高级提示理解能力，包括用户指定的意图（如“认真思考此问题”）。改进包括减少幻觉和迎合倾向，并在编码、写作及健康相关任务中表现更佳。
  openai/gpt-5-chat:
    description_cn: GPT-5 Chat 专为企业级应用设计，支持高级、自然、多模态且具备上下文感知能力的对话。
  openai/gpt-5-codex:
    description_cn: |-
      GPT-5-Codex 是 GPT-5 的专用版本，针对软件工程和编码工作流进行了优化。它既适用于交互式开发会话，也适用于长时间独立执行复杂工程任务。该模型支持从零构建项目、功能开发、调试、大规模重构和代码审查。相比 GPT-5，Codex 更具可控性，能更严格遵循开发者指令，并生成更简洁、高质量的代码。推理强度可通过 `reasoning.effort` 参数调节。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)

      Codex 可集成至 CLI、IDE 插件、GitHub 和云任务等开发者环境。它能动态调整推理强度——对小型任务快速响应，对大型项目则可持续运行数小时。该模型经过训练，可执行结构化代码审查，通过推理依赖关系并验证测试行为来发现关键缺陷。它还支持图像或截图等多模态输入用于 UI 开发，并集成工具用于搜索、依赖安装和环境配置。Codex 专为智能体驱动的编码应用而设计。
  openai/gpt-5-image:
    description_cn: '[GPT-5](https://openrouter.ai/openai/gpt-5) Image 将 OpenAI 的 GPT-5 模型与尖端图像生成能力相结合，在推理能力、代码质量和用户体验方面实现显著提升，同时继承了 GPT Image 1 在指令遵循、文本渲染和精细图像编辑方面的卓越表现。'
  openai/gpt-5-image-mini:
    description_cn: GPT-5 Image Mini 结合了由 [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini) 驱动的先进语言能力与 GPT Image 1 Mini 的高效图像生成能力。这一原生多模态模型具备卓越的指令遵循、文本渲染和精细图像编辑能力，同时显著降低延迟与成本。它在高质量视觉内容创作方面表现出色，并保持强大的文本理解能力，非常适合需要大规模高效图像生成与文本处理的应用场景。
  openai/gpt-5-mini:
    description_cn: GPT-5 Mini 是 GPT-5 的紧凑版本，专为轻量级推理任务设计。它继承了 GPT-5 的指令遵循与安全对齐优势，同时显著降低延迟和成本。GPT-5 Mini 是 OpenAI o4-mini 模型的继任者。
  openai/gpt-5-nano:
    description_cn: GPT-5-Nano 是 GPT-5 系列中最小、最快的变体，专为开发者工具、快速交互和超低延迟环境优化。尽管其推理深度较大型版本有所限制，但仍保留了关键的指令遵循与安全特性。该模型是 GPT-4.1-nano 的继任者，为成本敏感或实时应用场景提供轻量级选择。
  openai/gpt-5-pro:
    description_cn: GPT-5 Pro 是 OpenAI 最先进的模型，在推理能力、代码质量和用户体验方面均有重大提升。该模型针对需逐步推理、精准遵循指令及高风险场景下高准确性的复杂任务进行了优化。支持运行时路由功能和高级提示理解能力，包括识别用户指定意图（如“认真思考此问题”）。改进包括降低幻觉与迎合倾向，并在编程、写作及健康相关任务中表现更优。
  openai/gpt-5.1:
    description_cn: |-
      GPT-5.1 是 GPT-5 系列最新的前沿级模型，在通用推理能力、指令遵循性和自然对话风格方面均优于 GPT-5。它采用自适应推理机制，动态分配计算资源：对简单查询快速响应，对复杂任务投入更深推理。模型输出更清晰、更贴近事实的解释，减少术语使用，即使面对技术性或多步骤问题也易于理解。

      为广泛任务覆盖而构建，GPT-5.1 在数学、编程和结构化分析负载上持续提升，提供更连贯的长文本回答及更可靠的工具调用能力。同时，其对话对齐经过精细调优，在不牺牲准确性的前提下实现更亲切、直观的回应。GPT-5.1 是 GPT-5 的主要全功能继任者。
  openai/gpt-5.1-chat:
    description_cn: GPT-5.1 Chat（又名 Instant）是 5.1 系列中轻量、快速的成员，专为低延迟聊天优化，同时保留强大的通用智能。它采用自适应推理机制，仅在处理较难查询时选择性“深入思考”，从而在不影响常规对话速度的前提下提升数学、编程及多步骤任务的准确性。该模型默认更具亲和力和对话感，指令遵循能力更强，短文本推理更稳定。GPT-5.1 Chat 专为高吞吐、交互式工作负载设计，在响应速度与一致性比深度推理更重要的场景中表现出色。
  openai/gpt-5.1-codex:
    description_cn: |-
      GPT-5.1-Codex 是 GPT-5.1 的专用版本，针对软件工程与编程工作流优化。它既适用于交互式开发会话，也能独立执行复杂的长期工程任务。模型支持从零构建项目、功能开发、调试、大规模重构及代码审查。相比 GPT-5.1，Codex 更具可控性，严格遵循开发者指令，并生成更简洁、高质量的代码。可通过 `reasoning.effort` 参数调节推理强度，详见[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)。

      Codex 可集成至 CLI、IDE 插件、GitHub 及云任务等开发者环境，能动态调整推理强度——小任务快速响应，大型项目则可持续运行数小时。模型经过专门训练，可执行结构化代码审查，通过依赖分析与测试验证识别关键缺陷。此外，它还支持 UI 开发所需的多模态输入（如图像或截图），并集成工具用于搜索、依赖安装及环境配置。Codex 专为智能体编程应用而设计。
  openai/gpt-5.1-codex-max:
    description_cn: |-
      GPT-5.1-Codex-Max 是 OpenAI 最新的智能体编程模型，专为长时间运行、高上下文的软件开发任务而设计。该模型基于更新版的 5.1 推理架构，并在涵盖软件工程、数学和科研领域的智能体工作流上进行训练。
      GPT-5.1-Codex-Max 在整个开发生命周期中提供更快的性能、更强的推理能力以及更高的 Token 利用效率。
  openai/gpt-5.1-codex-mini:
    description_cn: GPT-5.1-Codex-Mini 是 GPT-5.1-Codex 的更小、更快版本。
  openai/gpt-5.2:
    description_cn: |-
      GPT-5.2 是 GPT-5 系列中最新的前沿级模型，在智能体能力和长上下文性能方面较 GPT-5.1 有显著提升。该模型采用自适应推理机制，动态分配计算资源，对简单查询快速响应，同时对复杂任务投入更深层次的处理。

      GPT-5.2 面向广泛任务场景设计，在数学、编程、科学及工具调用等负载上均实现一致性能提升，并能生成更具连贯性的长篇回答，同时提高工具使用的可靠性。
  openai/gpt-5.2-chat:
    description_cn: GPT-5.2 Chat（又称 Instant）是 GPT-5.2 系列中轻量、高速的成员，专为低延迟对话优化，同时保留强大的通用智能。该模型采用自适应推理机制，仅在面对较难查询时才启动深度思考，从而在不拖慢常规对话的前提下提升数学、编程及多步骤任务的准确性。默认设置下，模型语气更亲切自然，指令遵循能力更强，短程推理也更加稳定。GPT-5.2 Chat 专为高吞吐、交互式工作负载设计，在响应速度与一致性比深度推理更为关键的场景中表现优异。
  openai/gpt-5.2-codex:
    description_cn: |-
      GPT-5.2-Codex 是 GPT-5.1-Codex 的升级版，针对软件工程与编程工作流进行了优化，适用于交互式开发会话及长时间独立执行复杂工程任务。该模型支持从零构建项目、功能开发、调试、大规模重构及代码审查。相比 GPT-5.1-Codex，5.2-Codex 更具可控性，严格遵循开发者指令，并生成更简洁、高质量的代码。可通过 `reasoning.effort` 参数调节推理强度。详见[文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)。

      Codex 可集成至 CLI、IDE 插件、GitHub 及云任务等开发者环境，能动态调整推理强度——小任务快速响应，大型项目则可持续运行数小时。该模型经过训练，可执行结构化代码审查，通过分析依赖关系并对照测试验证行为，识别关键缺陷。此外，它还支持 UI 开发所需的图像或多模态输入，并集成工具用于搜索、依赖安装及环境配置。Codex 专为智能体编程应用场景而设计。
  openai/gpt-5.2-pro:
    description_cn: GPT-5.2 Pro 是 OpenAI 最先进的模型，在智能体编程和长上下文性能方面相较 GPT-5 Pro 有重大提升。该模型专为需要逐步推理、精准指令遵循及高可靠性输出的高风险应用场景而优化。它支持运行时路由功能和高级提示理解能力，包括识别用户指定意图（如“对此深入思考”）。改进之处包括显著降低幻觉与迎合倾向，并在编程、写作及健康相关任务中表现更佳。
  openai/gpt-oss-20b:
    description_cn: gpt-oss-20b 是 OpenAI 在 Apache 2.0 许可下发布的开源权重、210 亿参数模型。采用混合专家（MoE）架构，每次前向传递激活 36 亿参数，针对低延迟推理和消费级或单 GPU 硬件部署进行了优化。该模型基于 OpenAI 的 Harmony 响应格式训练，支持推理层级配置、微调以及智能体能力，包括函数调用、工具使用和结构化输出。
  openai/gpt-oss-20b:free:
    description_cn: gpt-oss-20b 是 OpenAI 在 Apache 2.0 许可下发布的开源权重、210 亿参数模型。采用混合专家（MoE）架构，每次前向传递激活 36 亿参数，针对低延迟推理和消费级或单 GPU 硬件部署进行了优化。该模型基于 OpenAI 的 Harmony 响应格式训练，支持推理层级配置、微调以及智能体能力，包括函数调用、工具使用和结构化输出。
  openai/gpt-oss-120b:
    description_cn: gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。
  openai/gpt-oss-120b:exacto:
    description_cn: gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。
  openai/gpt-oss-120b:free:
    description_cn: gpt-oss-120b 是 OpenAI 发布的开源权重、1170 亿参数的混合专家（MoE）语言模型，面向高阶推理、智能体及通用生产场景。每次前向传递激活 51 亿参数，并针对单张 H100 GPU 与原生 MXFP4 量化进行优化。模型支持可配置的推理深度、完整的思维链访问以及原生工具调用能力，包括函数调用、网页浏览和结构化输出生成。
  openai/gpt-oss-safeguard-20b:
    description_cn: |-
      gpt-oss-safeguard-20b 是 OpenAI 基于 gpt-oss-20b 构建的安全推理模型。这款开源权重、210 亿参数的混合专家（MoE）模型在内容分类、大模型过滤及信任与安全标注等安全任务上具备更低延迟。

      更多关于此模型的信息，请参阅 OpenAI 的 gpt-oss-safeguard [用户指南](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide)。
  openai/o1:
    description_cn: |-
      o1 是 OpenAI 最新且最强的模型系列，设计上会花更多时间进行思考后再响应。该系列模型通过大规模强化学习训练，采用思维链（Chain of Thought）方式进行推理。

      o1 模型针对数学、科学、编程及其他 STEM 领域任务进行了优化，在物理、化学和生物学等领域的基准测试中持续展现出博士级准确率。更多信息请参阅[发布公告](https://openai.com/o1)。
  openai/o1-pro:
    description_cn: o1 系列模型通过强化学习训练，能够在回答前进行思考并执行复杂推理。o1-pro 模型投入更多计算资源进行深度思考，从而持续提供更优质的答案。
  openai/o3:
    description_cn: o3 是一款在多个领域表现均衡且强大的模型，在数学、科学、编程和视觉推理任务中树立了新标杆。同时，它在技术写作和指令遵循方面也极为出色。适用于需结合文本、代码和图像进行多步骤分析的问题求解。
  openai/o3-deep-research:
    description_cn: |-
      o3-deep-research 是 OpenAI 专为深度研究设计的高级模型，旨在处理复杂的多步骤研究任务。

      注意：此模型始终使用“web_search”工具，会产生额外费用。
  openai/o3-mini:
    description_cn: |-
      OpenAI o3-mini 是一款高性价比语言模型，专为 STEM 推理任务优化，在科学、数学和编程领域表现尤为突出。

      该模型支持 `reasoning_effort` 参数，可设为“high”、“medium”或“low”，以控制模型的思考时间，默认值为“medium”。OpenRouter 还提供模型别名 `openai/o3-mini-high`，默认将该参数设为“high”。

      模型具备三级可调推理强度，并支持函数调用、结构化输出和流式响应等关键开发者功能，但不包含视觉处理能力。

      相比前代模型，o3-mini 实现显著改进：专家评测者在 56% 的情况下更偏好其回答，且在复杂问题上重大错误减少 39%。在中等推理强度设置下，o3-mini 在 AIME 和 GPQA 等高难度推理评测中性能媲美更大的 o1 模型，同时保持更低延迟与成本。
  openai/o3-mini-high:
    description_cn: |-
      OpenAI o3-mini-high 与 [o3-mini](/openai/o3-mini) 为同一模型，仅将 reasoning_effort 参数设为 high。

      o3-mini 是一款高性价比的语言模型，专为 STEM 推理任务优化，在科学、数学和编程领域表现尤为突出。该模型提供三种可调节的推理强度级别，并支持函数调用、结构化输出和流式响应等关键开发者功能，但不包含视觉处理能力。

      相比前代模型，o3-mini 表现显著提升：专家评测者在 56% 的情况下更偏好其回答，并在复杂问题上观察到重大错误减少 39%。在中等推理强度设置下，o3-mini 在 AIME 和 GPQA 等高难度推理评测中达到与更大规模的 o1 模型相当的性能，同时保持更低的延迟和成本。
  openai/o3-pro:
    description_cn: |-
      o 系列模型通过强化学习训练，在回答前进行思考，以执行复杂推理。o3-pro 模型投入更多算力进行深度思考，从而持续提供更优答案。

      注意：使用此模型需自带密钥（BYOK）。设置地址：https://openrouter.ai/settings/integrations
  openai/o4-mini:
    description_cn: |-
      OpenAI o4-mini 是 o 系列中一款紧凑型推理模型，在保持强大多模态和智能体能力的同时，针对快速、高性价比的性能进行了优化。该模型支持工具调用，在 AIME（配合 Python 达 99.5%）和 SWE-bench 等基准测试中展现出具有竞争力的推理与编码能力，超越前代 o3-mini，甚至在某些领域接近 o3。

      尽管模型规模较小，o4-mini 在 STEM 任务、视觉问题求解（如 MathVista、MMMU）和代码编辑方面仍具备高准确率。特别适用于对延迟或成本敏感的高吞吐场景。得益于其高效架构和精细化的强化学习训练，o4-mini 能够串联工具、生成结构化输出，并在极短时间内（通常不到一分钟）完成多步骤任务。
  openai/o4-mini-deep-research:
    description_cn: |-
      o4-mini-deep-research 是 OpenAI 推出的速度更快、成本更低的深度研究模型，非常适合处理复杂的多步骤研究任务。

      注意：此模型始终使用“web_search”工具，会产生额外费用。
  openai/o4-mini-high:
    description_cn: |-
      OpenAI o4-mini-high 与 [o4-mini](/openai/o4-mini) 为同一模型，仅将 reasoning_effort 参数设为 high。

      OpenAI o4-mini 是 o 系列中一款紧凑型推理模型，在保持强大多模态和智能体能力的同时，针对快速、高性价比的性能进行了优化。该模型支持工具调用，在 AIME（配合 Python 达 99.5%）和 SWE-bench 等基准测试中展现出具有竞争力的推理与编码能力，超越前代 o3-mini，甚至在某些领域接近 o3。

      尽管模型规模较小，o4-mini 在 STEM 任务、视觉问题求解（如 MathVista、MMMU）和代码编辑方面仍具备高准确率。特别适用于对延迟或成本敏感的高吞吐场景。得益于其高效架构和精细化的强化学习训练，o4-mini 能够串联工具、生成结构化输出，并在极短时间内（通常不到一分钟）完成多步骤任务。
  opengvlab/internvl3-78b:
    description_cn: |-
      InternVL3 系列是先进的多模态大语言模型（MLLM）。相较于 InternVL 2.5，InternVL3 展现出更强的多模态感知与推理能力。

      此外，InternVL3 在基准测试中对标 Qwen2.5 Chat 模型，其语言模块以 Qwen2.5 预训练基础模型为初始化起点。得益于原生多模态预训练（Native Multimodal Pre-Training），InternVL3 系列在整体文本性能上超越 Qwen2.5 系列。
  openrouter/auto:
    description_cn: |-
      您的提示将由元模型处理，并自动路由至以下数十种模型之一（见下文），以优化输出质量。

      要查看实际使用的模型，请访问 [Activity](/activity)，或读取响应中的 `model` 字段。计费将按所路由模型的标准费率执行。

      更多详情（包括如何自定义路由模型）请参阅我们的 [文档](/docs/guides/routing/routers/auto-router)。

      请求可能被路由至以下模型：
      - [openai/gpt-5.1](/openai/gpt-5.1)
      - [openai/gpt-5](/openai/gpt-5)
      - [openai/gpt-5-mini](/openai/gpt-5-mini)
      - [openai/gpt-5-nano](/openai/gpt-5-nano)
      - [openai/gpt-4.1](/openai/gpt-4.1)
      - [openai/gpt-4.1-mini](/openai/gpt-4.1-mini)
      - [openai/gpt-4.1-nano](/openai/gpt-4.1-nano)
      - [openai/gpt-4o](/openai/gpt-4o)
      - [openai/gpt-4o-2024-05-13](/openai/gpt-4o-2024-05-13)
      - [openai/gpt-4o-2024-08-06](/openai/gpt-4o-2024-08-06)
      - [openai/gpt-4o-2024-11-20](/openai/gpt-4o-2024-11-20)
      - [openai/gpt-4o-mini](/openai/gpt-4o-mini)
      - [openai/gpt-4o-mini-2024-07-18](/openai/gpt-4o-mini-2024-07-18)
      - [openai/gpt-4-turbo](/openai/gpt-4-turbo)
      - [openai/gpt-4-turbo-preview](/openai/gpt-4-turbo-preview)
      - [openai/gpt-4-1106-preview](/openai/gpt-4-1106-preview)
      - [openai/gpt-4](/openai/gpt-4)
      - [openai/gpt-3.5-turbo](/openai/gpt-3.5-turbo)
      - [openai/gpt-oss-120b](/openai/gpt-oss-120b)
      - [anthropic/claude-opus-4.5](/anthropic/claude-opus-4.5)
      - [anthropic/claude-opus-4.1](/anthropic/claude-opus-4.1)
      - [anthropic/claude-opus-4](/anthropic/claude-opus-4)
      - [anthropic/claude-sonnet-4.5](/anthropic/claude-sonnet-4.5)
      - [anthropic/claude-sonnet-4](/anthropic/claude-sonnet-4)
      - [anthropic/claude-3.7-sonnet](/anthropic/claude-3.7-sonnet)
      - [anthropic/claude-haiku-4.5](/anthropic/claude-haiku-4.5)
      - [anthropic/claude-3.5-haiku](/anthropic/claude-3.5-haiku)
      - [anthropic/claude-3-haiku](/anthropic/claude-3-haiku)
      - [google/gemini-3-pro-preview](/google/gemini-3-pro-preview)
      - [google/gemini-2.5-pro](/google/gemini-2.5-pro)
      - [google/gemini-2.0-flash-001](/google/gemini-2.0-flash-001)
      - [google/gemini-2.5-flash](/google/gemini-2.5-flash)
      - [mistralai/mistral-large](/mistralai/mistral-large)
      - [mistralai/mistral-large-2407](/mistralai/mistral-large-2407)
      - [mistralai/mistral-large-2411](/mistralai/mistral-large-2411)
      - [mistralai/mistral-medium-3.1](/mistralai/mistral-medium-3.1)
      - [mistralai/mistral-nemo](/mistralai/mistral-nemo)
      - [mistralai/mistral-7b-instruct](/mistralai/mistral-7b-instruct)
      - [mistralai/mixtral-8x7b-instruct](/mistralai/mixtral-8x7b-instruct)
      - [mistralai/mixtral-8x22b-instruct](/mistralai/mixtral-8x22b-instruct)
      - [mistralai/codestral-2508](/mistralai/codestral-2508)
      - [x-ai/grok-4](/x-ai/grok-4)
      - [x-ai/grok-3](/x-ai/grok-3)
      - [x-ai/grok-3-mini](/x-ai/grok-3-mini)
      - [deepseek/deepseek-r1](/deepseek/deepseek-r1)
      - [meta-llama/llama-3.3-70b-instruct](/meta-llama/llama-3.3-70b-instruct)
      - [meta-llama/llama-3.1-405b-instruct](/meta-llama/llama-3.1-405b-instruct)
      - [meta-llama/llama-3.1-70b-instruct](/meta-llama/llama-3.1-70b-instruct)
      - [meta-llama/llama-3.1-8b-instruct](/meta-llama/llama-3.1-8b-instruct)
      - [meta-llama/llama-3-70b-instruct](/meta-llama/llama-3-70b-instruct)
      - [meta-llama/llama-3-8b-instruct](/meta-llama/llama-3-8b-instruct)
      - [qwen/qwen3-235b-a22b](/qwen/qwen3-235b-a22b)
      - [qwen/qwen3-32b](/qwen/qwen3-32b)
      - [qwen/qwen3-14b](/qwen/qwen3-14b)
      - [cohere/command-r-plus-08-2024](/cohere/command-r-plus-08-2024)
      - [cohere/command-r-08-2024](/cohere/command-r-08-2024)
      - [moonshotai/kimi-k2-thinking](/moonshotai/kimi-k2-thinking)
      - [perplexity/sonar](/perplexity/sonar)
  openrouter/bodybuilder:
    description_cn: |-
      将您的自然语言请求转换为结构化的 OpenRouter API 请求对象。只需描述您希望 AI 模型完成的任务，Body Builder 即可自动生成相应的 API 调用。例如：“使用 gemini 和 opus 从 1 数到 10。”

      此功能适用于创建多模型请求、自定义模型路由，或根据人类描述程序化生成 API 调用。

      **Beta 公告**：Body Builder 目前处于 Beta 阶段，免费使用。未来定价与功能可能发生变化。
  perplexity/sonar:
    description_cn: Sonar 轻量、经济、快速且易于使用——现已支持引用来源并可自定义信息源。该模型专为希望集成轻量级、高速问答功能的企业设计。
  perplexity/sonar-deep-research:
    description_cn: |-
      Sonar Deep Research 是一款专注于研究的模型，专为跨复杂主题的多步骤检索、信息整合与推理而设计。它能自主搜索、阅读并评估信息源，并在收集信息过程中不断优化策略，从而在金融、科技、健康和时事等领域生成全面的研究报告。

      定价说明（[来源](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-deep-research)）：
      - 输入 token 包含提示 token（用户输入）和引用 token（执行搜索后处理的 token）
      - Deep Research 会执行多次搜索以完成深度研究，搜索费用为每 1000 次搜索 5 美元。例如，一次包含 30 次搜索的请求将产生 0.15 美元的搜索费用
      - 推理是 Deep Research 中的一个独立步骤，模型会对研究阶段收集的所有材料进行大量自动化推理。此处的推理 token 与最终答案中的思维链（CoT）不同，指的是在生成输出前用于分析研究材料的内部推理 token，定价为每百万 token 3 美元
  perplexity/sonar-pro:
    description_cn: |-
      注意：Sonar Pro 的定价已包含 Perplexity 搜索费用。详见[此处说明](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)

      面向需要更高级功能的企业用户，Sonar Pro API 可处理深入的多步骤查询，并具备更强的扩展性，例如平均每轮搜索可提供的引用数量是普通 Sonar 的两倍。此外，凭借更大的上下文窗口，它能够处理更长、更复杂的搜索请求及后续追问。
  perplexity/sonar-pro-search:
    description_cn: |-
      Sonar Pro 的全新 Pro Search 模式仅在 OpenRouter API 上提供，是 Perplexity 最先进的智能体搜索系统，专为深度推理与分析而设计。定价基于 token 数量外加每千次请求 18 美元。该模型驱动 Perplexity 平台上的 Pro Search 模式。

      Sonar Pro Search 在 Sonar Pro 基础上增加了自主多步推理能力，不再局限于单次查询与结果整合，而是能够规划并执行完整的研究工作流，调用多种工具完成任务。
  perplexity/sonar-reasoning-pro:
    description_cn: |-
      注意：Sonar Pro 的定价已包含 Perplexity 搜索费用。详见[此处说明](https://docs.perplexity.ai/guides/pricing#detailed-pricing-breakdown-for-sonar-reasoning-pro-and-sonar-pro)

      Sonar Reasoning Pro 是一款由 DeepSeek R1 驱动、采用思维链（Chain of Thought, CoT）技术的高端推理模型。专为高级应用场景设计，支持深入的多步骤查询，拥有更大的上下文窗口，并可在每次搜索中返回更多引用，从而生成更全面、更具扩展性的回答。
  prime-intellect/intellect-3:
    description_cn: INTELLECT-3 是一款1060亿参数的混合专家（MoE）模型（激活参数约120亿），基于 GLM-4.5-Air-Base 经过监督微调（SFT）及大规模强化学习（RL）后训练而成。在数学、代码、科学及通用推理等任务上，其单位参数性能达到业界领先水平，持续超越众多更大规模的前沿模型。该模型专为强大多步问题求解设计，在结构化任务中保持高准确性，同时凭借 MoE 架构实现高效的推理性能。
  qwen/qwen-2.5-7b-instruct:
    description_cn: |-
      Qwen2.5 7B 是通义千问大语言模型系列的最新版本，在 Qwen2 基础上带来以下改进：

      - 凭借领域专家模型，在知识储备、代码及数学能力方面显著增强。

      - 指令遵循能力大幅提升，支持生成长文本（超过 8K tokens），更好地理解结构化数据（如表格），并能高效生成结构化输出（尤其是 JSON）。对系统提示的多样性更具鲁棒性，显著提升聊天机器人的角色扮演和条件设定能力。

      - 支持最长 128K tokens 的上下文，最大生成长度达 8K tokens。

      - 支持 29 种以上语言，包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。

      使用本模型需遵守 [通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。
  qwen/qwen-2.5-72b-instruct:
    description_cn: 通义千问 2.5 72B 指令微调版。
    aliases:
      - qwen-2.5-72b
      - qwen2.5
    provider: Qwen
  qwen/qwen-2.5-coder-32b-instruct:
    description_cn: |-
      Qwen2.5-Coder 是最新一代专注于代码的 Qwen 大语言模型系列（前身为 CodeQwen）。相较于 CodeQwen1.5，Qwen2.5-Coder 带来了以下改进：

      - 在**代码生成**、**代码推理**和**代码修复**方面显著提升；
      - 为实际应用场景（如**代码智能体**）提供了更全面的基础，不仅强化了编码能力，还保持了在数学和通用能力方面的优势。

      更多评测结果详见 [Qwen 2.5 Coder 博客](https://qwenlm.github.io/blog/qwen2.5-coder-family/)。
  qwen/qwen-2.5-vl-7b-instruct:
    description_cn: |-
      Qwen2.5 VL 7B 是通义千问团队推出的多模态大语言模型，具备以下关键增强特性：

      - 多分辨率与比例图像的 SOTA 理解能力：Qwen2.5-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中达到业界领先水平。

      - 支持 20 分钟以上视频理解：可高质量完成基于长视频的问答、对话、内容创作等任务。

      - 可操作手机、机器人等设备的智能体：凭借强大的复杂推理与决策能力，Qwen2.5-VL 可集成至手机、机器人等设备，根据视觉环境与文本指令实现自动化操作。

      - 多语言支持：除英语和中文外，现支持识别图像内多种语言文字，包括主流欧洲语言、日语、韩语、阿拉伯语、越南语等，服务全球用户。

      更多详情请参见[博客文章](https://qwenlm.github.io/blog/qwen2-vl/)和[GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。

      使用本模型需遵守[通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。
  qwen/qwen-2.5-vl-7b-instruct:free:
    description_cn: |-
      Qwen2.5 VL 7B 是通义千问团队推出的多模态大语言模型，具备以下关键增强特性：

      - 多分辨率与比例图像的 SOTA 理解能力：Qwen2.5-VL 在 MathVista、DocVQA、RealWorldQA、MTVQA 等视觉理解基准测试中达到业界领先水平。

      - 支持 20 分钟以上视频理解：可高质量完成基于长视频的问答、对话、内容创作等任务。

      - 可操作手机、机器人等设备的智能体：凭借强大的复杂推理与决策能力，Qwen2.5-VL 可集成至手机、机器人等设备，根据视觉环境与文本指令实现自动化操作。

      - 多语言支持：除英语和中文外，现支持识别图像内多种语言文字，包括主流欧洲语言、日语、韩语、阿拉伯语、越南语等，服务全球用户。

      更多详情请参见[博客文章](https://qwenlm.github.io/blog/qwen2-vl/)和[GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。

      使用本模型需遵守[通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。
  qwen/qwen-max:
    description_cn: Qwen-Max 基于 Qwen2.5 构建，是 [Qwen 系列模型](/qwen) 中推理性能最强的版本，尤其擅长复杂的多步骤任务。该大规模 MoE 模型在超过 20 万亿 token 上完成预训练，并进一步通过精选的监督微调（SFT）与人类反馈强化学习（RLHF）方法进行后训练。具体参数量未公开。
  qwen/qwen-plus:
    description_cn: Qwen-Plus 基于 Qwen2.5 基础模型，上下文长度达 131K，兼顾性能、速度与成本。
  qwen/qwen-plus-2025-07-28:
    description_cn: Qwen Plus 0728 基于 Qwen3 基础模型，是一款支持百万上下文的混合推理模型，在性能、速度与成本之间取得均衡。
  qwen/qwen-plus-2025-07-28:thinking:
    description_cn: Qwen Plus 0728 基于 Qwen3 基础模型，是一款支持百万上下文的混合推理模型，在性能、速度与成本之间取得均衡。
  qwen/qwen-turbo:
    description_cn: Qwen-Turbo 基于 Qwen2.5，上下文长度达 1M，具备高速度与低成本特性，适用于简单任务。
  qwen/qwen-vl-max:
    description_cn: Qwen VL Max 是一款视觉理解模型，上下文长度达 7500 个 token，在更广泛的复杂任务中提供卓越性能。
  qwen/qwen-vl-plus:
    description_cn: Qwen 增强版大视觉语言模型，显著提升了细节识别与文本识别能力，支持高达百万像素级的超高清分辨率及极端宽高比的图像输入，在各类视觉任务中均展现出显著性能优势。
  qwen/qwen2.5-coder-7b-instruct:
    description_cn: |-
      Qwen2.5-Coder-7B-Instruct 是一款拥有 70 亿参数的指令微调语言模型，专为代码生成、推理和调试等任务优化。基于 Qwen2.5 架构，集成了 RoPE、SwiGLU、RMSNorm 和 GQA 注意力机制，并通过 YaRN 插值技术支持高达 128K token 的上下文长度。该模型在大规模源代码、合成数据及文本-代码对齐语料上训练，可在多种编程语言和智能体编码工作流中提供稳健性能。

      作为 Qwen2.5-Coder 系列的一员，该模型与 vLLM 等高效部署工具高度兼容，并以 Apache 2.0 许可证发布。
  qwen/qwen2.5-vl-32b-instruct:
    description_cn: Qwen2.5-VL-32B 是一款通过强化学习微调的多模态视觉-语言模型，显著增强了数学推理、结构化输出及视觉问题求解能力。该模型在视觉分析任务中表现卓越，包括物体识别、图像内文本解析以及长视频中精确事件定位。在 MMMU、MathVista 和 VideoMME 等多模态基准测试中达到业界领先水平，同时在 MMLU、数学问题求解和代码生成等纯文本任务中也展现出强大的推理能力与清晰度。
  qwen/qwen2.5-vl-72b-instruct:
    description_cn: Qwen2.5-VL 擅长识别花卉、鸟类、鱼类和昆虫等常见物体，同时能高效分析图像中的文字、图表、图标、图形及版式布局。
  qwen/qwen3-4b:free:
    description_cn: Qwen3-4B 是 Qwen3 系列中的 40 亿参数稠密语言模型，兼顾通用任务与高推理强度任务。该模型引入双模架构——“思考”模式与“非思考”模式，可动态切换于高精度逻辑推理与高效对话生成之间，非常适合多轮对话、指令遵循及复杂智能体工作流场景。
  qwen/qwen3-8b:
    description_cn: Qwen3-8B 是 Qwen3 系列中的稠密因果语言模型，参数量达 82 亿，兼顾高推理负载任务与高效对话。该模型支持在“思考”模式（用于数学、编程和逻辑推理）与“非思考”模式（用于通用对话）之间无缝切换。模型经过指令遵循、智能体集成、创意写作及 100 多种语言和方言的多语言任务微调，原生支持 32K token 上下文窗口，并可通过 YaRN 扩展技术将上下文长度提升至 131K tokens。
  qwen/qwen3-14b:
    description_cn: Qwen3-14B 是 Qwen3 系列中的稠密因果语言模型，参数量达 148 亿，兼顾复杂推理与高效对话。该模型支持在“思考”模式（适用于数学、编程和逻辑推理等任务）与“非思考”模式（适用于通用对话）之间无缝切换。模型经过指令遵循、智能体工具调用、创意写作及 100 多种语言和方言的多语言任务微调，原生支持 32K token 上下文，并可通过基于 YaRN 的扩展技术将上下文长度提升至 131K tokens。
  qwen/qwen3-30b-a3b:
    description_cn: |-
      Qwen3 是千问大语言模型系列的最新一代，同时提供稠密架构与专家混合（MoE）架构，在推理能力、多语言支持和高级智能体任务方面表现卓越。其独特之处在于可无缝切换“思考”模式（用于复杂推理）与“非思考”模式（用于高效对话），确保多功能、高质量的性能表现。

      相比 QwQ 和 Qwen2.5 等前代模型，Qwen3 在数学、编程、常识推理、创意写作和交互式对话等方面显著提升。Qwen3-30B-A3B 变体包含 305 亿总参数（每任务激活 33 亿参数）、48 层网络结构、128 个专家（每任务激活 8 个），并借助 YaRN 技术支持高达 131K token 的上下文长度，为开源模型树立了新标杆。
  qwen/qwen3-30b-a3b-instruct-2507:
    description_cn: Qwen3-30B-A3B-Instruct-2507 是千问推出的 305 亿参数混合专家（MoE）语言模型，每次推理激活 33 亿参数。该模型运行于非推理模式，专为高质量指令遵循、多语言理解及智能体工具调用而设计。经指令数据后训练，其在推理（AIME、ZebraLogic）、代码（MultiPL-E、LiveCodeBench）和对齐（IFEval、WritingBench）等基准测试中表现优异。相比非指令微调版本，该模型在主观性和开放式任务上表现更佳，同时保持强大的事实性和代码能力。
  qwen/qwen3-30b-a3b-thinking-2507:
    description_cn: |-
      Qwen3-30B-A3B-Thinking-2507 是一款 300 亿参数的专家混合（MoE）推理模型，专为需要扩展多步思考的复杂任务而优化。该模型专为“思考模式”设计，将内部推理轨迹与最终答案分离。

      相较于早期 Qwen3-30B 版本，此版本在逻辑推理、数学、科学、编程及多语言基准测试中均有性能提升，同时展现出更强的指令遵循能力、工具使用能力以及与人类偏好的对齐度。凭借更高的推理效率和更长的输出预算，该模型特别适用于高级研究、竞赛级问题求解以及需要结构化长上下文推理的智能体应用。
  qwen/qwen3-32b:
    description_cn: Qwen3-32B 是 Qwen3 系列中的稠密因果语言模型，参数量达 328 亿，专为复杂推理与高效对话双重目标优化。该模型支持在“思考”模式（用于数学、编程和逻辑推理等任务）与“非思考”模式（用于更快速的通用对话）之间无缝切换。模型在指令遵循、智能体工具调用、创意写作及 100 多种语言和方言的多语言任务中均表现出色，原生支持 32K token 上下文，并可通过基于 YaRN 的扩展技术将上下文长度提升至 131K tokens。
  qwen/qwen3-235b-a22b:
    description_cn: Qwen3-235B-A22B 是千问推出的 2350 亿参数稀疏专家混合（MoE）模型，每次前向传播激活 220 亿参数。该模型支持在“思考”模式（用于复杂推理、数学和代码任务）与“非思考”模式（用于高效通用对话）之间无缝切换，展现出强大的推理能力、多语言支持（覆盖 100 多种语言和方言）、高级指令遵循能力以及智能体工具调用功能。模型原生支持 32K token 上下文窗口，并可通过基于 YaRN 的扩展技术将上下文长度延伸至 131K tokens。
  qwen/qwen3-235b-a22b-2507:
    description_cn: |-
      Qwen3-235B-A22B-Instruct-2507 是基于 Qwen3-235B 架构的多语言指令微调混合专家语言模型，每次前向传播激活 220 亿参数。该模型针对通用文本生成任务优化，涵盖指令遵循、逻辑推理、数学、编程及工具调用等能力。模型原生支持 262K 令牌上下文长度，且未启用“思考模式”（即无 <think> 区块）。

      相较于基础版本，此版本在知识覆盖广度、长上下文推理、编程基准及开放式任务对齐方面均有显著提升。其在多语言理解、数学推理（如 AIME、HMMT）以及 Arena-Hard、WritingBench 等对齐评估中表现尤为突出。
  qwen/qwen3-235b-a22b-thinking-2507:
    description_cn: |-
      Qwen3-235B-A22B-Thinking-2507 是一款高性能、开源权重的混合专家（MoE）语言模型，专为复杂推理任务优化。每次前向传播激活其 2350 亿参数中的 220 亿，并原生支持最多 262,144 个上下文 token。该“纯推理”变体在结构化逻辑推理、数学、科学及长文本生成方面表现卓越，在 AIME、SuperGPQA、LiveCodeBench 和 MMLU-Redux 等基准测试中成绩优异。模型强制启用特殊推理模式（</think>），并针对高 token 输出（最高达 81,920 个 token）的挑战性领域进行优化。

      该模型经过指令微调，在逐步推理、工具调用、智能体工作流及多语言任务方面表现出色。此版本是 Qwen3-235B 系列中最强大的开源变体，在结构化推理应用场景中超越众多闭源模型。
  qwen/qwen3-coder:
    description_cn: |-
      Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。

      阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。
  qwen/qwen3-coder-30b-a3b-instruct:
    description_cn: |-
      Qwen3-Coder-30B-A3B-Instruct 是一款 305 亿参数的混合专家（MoE）模型，包含 128 个专家（每次前向传播激活 8 个），专为高级代码生成、仓库级理解及智能体工具调用而设计。基于 Qwen3 架构，原生支持 256K token 上下文长度（通过 Yarn 可扩展至 1M），在函数调用、浏览器操作和结构化代码补全等任务中表现强劲。

      该模型针对无“推理模式”的指令遵循进行优化，并与 OpenAI 兼容的工具调用格式良好集成。
  qwen/qwen3-coder-flash:
    description_cn: Qwen3 Coder Flash 是阿里巴巴推出的 Qwen3 Coder Plus 的快速且高性价比版本。该模型是一款强大的编程智能体，专注于通过工具调用与环境交互实现自主编程，兼具卓越的编码能力与通用任务处理能力。
  qwen/qwen3-coder-plus:
    description_cn: Qwen3 Coder Plus 是阿里巴巴基于开源 Qwen3 Coder 480B A35B 打造的专有版本，是一款强大的编码智能体模型，专注于通过工具调用和环境交互实现自主编程，兼具卓越的编码能力与通用任务处理能力。
  qwen/qwen3-coder:exacto:
    description_cn: |-
      Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。

      阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。
  qwen/qwen3-coder:free:
    description_cn: |-
      Qwen3-Coder-480B-A35B-Instruct 是千问团队开发的混合专家（MoE）代码生成模型，专为智能体编程任务优化，如函数调用、工具使用及基于仓库的长上下文推理。该模型总参数量达 4800 亿，每次前向传播激活 350 亿参数（从 160 个专家中激活 8 个）。

      阿里云端点的定价根据上下文长度而异。当请求输入 token 超过 128k 时，将适用更高费率。
  qwen/qwen3-max:
    description_cn: Qwen3-Max 是 Qwen3 系列的最新版本，相较 2025 年 1 月版，在推理能力、指令遵循、多语言支持及长尾知识覆盖方面均有显著提升。它在数学、编程、逻辑和科学任务中精度更高，能更可靠地理解中英文复杂指令，减少幻觉现象，并在开放式问答、写作和对话中生成更高质量的回答。该模型支持 100 多种语言，具备更强的翻译能力和常识推理能力，并针对检索增强生成（RAG）和工具调用进行了优化，但未包含专用的“思考”模式。
  qwen/qwen3-next-80b-a3b-instruct:
    description_cn: |-
      Qwen3-Next-80B-A3B-Instruct 是 Qwen3-Next 系列中的指令微调对话模型，专为快速、稳定响应而优化，不输出“思考”痕迹。该模型面向复杂任务场景，涵盖推理、代码生成、知识问答及多语言应用，同时在对齐性与格式规范方面保持稳健。相较早期 Qwen3 指令模型，本版本聚焦于超长输入与多轮对话下的更高吞吐量与稳定性，特别适用于需一致最终答案而非显式思维链的 RAG、工具调用及智能体工作流。

      模型采用高效扩展的训练与解码策略，提升参数效率与推理速度，并在广泛的公开基准测试中验证：在多个类别上达到或接近更大规模 Qwen3 系统的水平，同时显著优于早期中等规模基线。该模型最适合用于生产环境中需要确定性、严格遵循指令输出的通用助手、代码辅助及长上下文任务求解场景。
  qwen/qwen3-next-80b-a3b-instruct:free:
    description_cn: |-
      Qwen3-Next-80B-A3B-Instruct 是 Qwen3-Next 系列中的指令微调对话模型，专为快速、稳定响应而优化，不输出“思考”痕迹。该模型面向复杂任务场景，涵盖推理、代码生成、知识问答及多语言应用，同时在对齐性与格式规范方面保持稳健。相较早期 Qwen3 指令模型，本版本聚焦于超长输入与多轮对话下的更高吞吐量与稳定性，特别适用于需一致最终答案而非显式思维链的 RAG、工具调用及智能体工作流。

      模型采用高效扩展的训练与解码策略，提升参数效率与推理速度，并在广泛的公开基准测试中验证：在多个类别上达到或接近更大规模 Qwen3 系统的水平，同时显著优于早期中等规模基线。该模型最适合用于生产环境中需要确定性、严格遵循指令输出的通用助手、代码辅助及长上下文任务求解场景。
  qwen/qwen3-next-80b-a3b-thinking:
    description_cn: |-
      Qwen3-Next-80B-A3B-Thinking 是 Qwen3-Next 系列中以推理优先的对话模型，默认输出结构化的“思考”轨迹。该模型专为高难度多步问题设计，包括数学证明、代码生成/调试、逻辑推理及智能体规划，在知识、推理、编程、对齐性与多语言评估中均表现强劲。相较早期 Qwen3 变体，本模型强调长思维链下的稳定性与推理阶段的高效扩展能力，并经过调优以更好遵循复杂指令，减少重复或偏离任务的行为。

      该模型适用于智能体框架与工具调用（函数调用）、重度检索工作流及需逐步解答的标准基准测试场景。支持生成长篇、详尽的回答，并采用面向吞吐量的技术（如多token预测）加速生成。请注意，该模型仅运行于纯思考模式。
  qwen/qwen3-vl-8b-instruct:
    description_cn: |-
      Qwen3-VL-8B-Instruct 是 Qwen3-VL 系列中的多模态视觉语言模型，专为跨文本、图像和视频的高保真理解与推理而构建。该模型采用改进的多模态融合技术，包括用于长时序推理的 Interleaved-MRoPE、用于细粒度视觉-文本对齐的 DeepStack，以及用于精确事件定位的文本-时间戳对齐机制。

      该模型原生支持 256K token 上下文窗口，可扩展至 1M tokens，能够处理静态与动态媒体输入，适用于文档解析、视觉问答、空间推理和 GUI 控制等任务。其文本理解能力媲美主流大语言模型，OCR 支持语言扩展至 32 种，并在多样化视觉条件下展现出更强鲁棒性。
  qwen/qwen3-vl-8b-thinking:
    description_cn: |-
      Qwen3-VL-8B-Thinking 是 Qwen3-VL-8B 多模态模型的推理优化版本，专为复杂场景、文档和时序序列中的高级视觉与文本推理而设计。该模型集成增强的多模态对齐机制与长上下文处理能力（原生 256K，可扩展至 1M tokens），适用于科学视觉分析、因果推断以及基于图像或视频输入的数学推理等任务。

      相比 Instruct 版本，Thinking 版本引入了更深层次的视觉-语言融合与深思熟虑的推理路径，在长链逻辑任务、STEM 问题求解和多步视频理解方面表现更优。通过 Interleaved-MRoPE 和时间戳感知嵌入，该模型实现了更强的时间定位能力，同时保持与大型纯文本 LLM 相当的 OCR、多语言理解及文本生成能力。
  qwen/qwen3-vl-30b-a3b-instruct:
    description_cn: Qwen3-VL-30B-A3B-Instruct 是一款多模态模型，融合强大的文本生成能力与对图像和视频的视觉理解能力。其 Instruct 变体针对通用多模态任务的指令遵循能力进行了优化。该模型在真实/合成类别感知、2D/3D 空间定位及长篇视觉理解方面表现出色，在多模态基准测试中成绩优异。在智能体应用中，可处理多图像多轮指令、视频时间轴对齐、GUI 自动化，以及从草图到调试完成 UI 的可视化编程。其文本性能与旗舰 Qwen3 系列相当，适用于文档 AI、OCR、UI 辅助、空间任务及智能体研究。
  qwen/qwen3-vl-30b-a3b-thinking:
    description_cn: Qwen3-VL-30B-A3B-Thinking 是一款多模态模型，融合强大的文本生成能力与对图像和视频的视觉理解能力。其 Thinking 变体强化了在 STEM、数学及复杂任务中的推理能力。该模型在真实/合成类别感知、2D/3D 空间定位及长篇视觉理解方面表现出色，在多模态基准测试中成绩优异。在智能体应用中，可处理多图像多轮指令、视频时间轴对齐、GUI 自动化，以及从草图到调试完成 UI 的可视化编程。其文本性能与旗舰 Qwen3 系列相当，适用于文档 AI、OCR、UI 辅助、空间任务及智能体研究。
  qwen/qwen3-vl-32b-instruct:
    description_cn: Qwen3-VL-32B-Instruct 是一款大规模多模态视觉语言模型，专为文本、图像和视频的高精度理解与推理而设计。该模型拥有 320 亿参数，融合深度视觉感知与先进文本理解能力，支持细粒度空间推理、文档与场景分析以及长周期视频理解。支持 32 种语言的鲁棒 OCR，并通过 Interleaved-MRoPE 与 DeepStack 架构实现增强的多模态融合。该模型针对智能体交互和视觉工具调用进行了优化，在复杂现实世界的多模态任务中达到业界领先水平。
  qwen/qwen3-vl-235b-a22b-instruct:
    description_cn: |-
      Qwen3-VL-235B-A22B Instruct 是一款开源多模态模型，融合了强大的文本生成能力与对图像和视频的视觉理解能力。Instruct 版本面向通用视觉-语言应用场景（如视觉问答、文档解析、图表/表格提取、多语言 OCR）。该系列强调鲁棒感知能力（识别多样化的现实与合成类别）、空间理解能力（2D/3D 定位）以及长篇幅视觉理解能力，在公开多模态基准测试中，其感知与推理性能均具竞争力。

      除分析能力外，Qwen3-VL 还支持智能体交互与工具调用：可在多图像、多轮对话中执行复杂指令；将文本对齐至视频时间轴以实现精确时序查询；并可操作 GUI 元素完成自动化任务。该模型还支持可视化编码工作流——将草图或原型转化为代码，并辅助 UI 调试，同时保持与旗舰版 Qwen3 语言模型相当的纯文本性能。因此，Qwen3-VL 适用于涵盖文档 AI、多语言 OCR、软件/UI 辅助、空间/具身任务及视觉-语言智能体研究等生产场景。
  qwen/qwen3-vl-235b-a22b-thinking:
    description_cn: |-
      Qwen3-VL-235B-A22B Thinking 是一款多模态模型，融合了强大的文本生成能力与对图像和视频的视觉理解能力。Thinking 版本专为 STEM 和数学领域的多模态推理优化。该系列强调鲁棒感知能力（识别多样化的现实与合成类别）、空间理解能力（2D/3D 定位）以及长篇幅视觉理解能力，在公开多模态基准测试中，其感知与推理性能均具竞争力。

      除分析能力外，Qwen3-VL 还支持智能体交互与工具调用：可在多图像、多轮对话中执行复杂指令；将文本对齐至视频时间轴以实现精确时序查询；并可操作 GUI 元素完成自动化任务。该模型还支持可视化编码工作流——将草图或原型转化为代码，并辅助 UI 调试，同时保持与旗舰版 Qwen3 语言模型相当的纯文本性能。因此，Qwen3-VL 适用于涵盖文档 AI、多语言 OCR、软件/UI 辅助、空间/具身任务及视觉-语言智能体研究等生产场景。
  qwen/qwq-32b:
    description_cn: QwQ 是通义千问（Qwen）系列中的推理专用模型。相较于传统的指令微调模型，具备思考与推理能力的 QwQ 在下游任务（尤其是难题）上性能显著提升。QwQ-32B 是其中的中等规模推理模型，在性能上可与当前领先的推理模型（如 DeepSeek-R1、o1-mini）相媲美。
  raifle/sorcererlm-8x22b:
    description_cn: |-
      SorcererLM 是一款先进的角色扮演与叙事模型，基于 [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) 采用低秩 16 位 LoRA 微调而成。

      - 具备高级推理与情感智能，可实现引人入胜的沉浸式交互；
      - 写作风格生动，融合空间感知与上下文意识；
      - 叙事深度增强，支持富有创意且动态变化的故事创作。
  relace/relace-apply-3:
    description_cn: |-
      Relace Apply 3 是一款专用的代码修补大语言模型，可直接将 AI 建议的编辑内容合并到源文件中。它平均以每秒 10,000 个 token 的速度将 GPT-4o、Claude 等模型的更新应用到您的文件中。

      该模型要求提示格式如下：
      <instruction>{instruction}</instruction>
      <code>{initial_code}</code>
      <update>{edit_snippet}</update>

      Relace 已启用零数据留存策略。更多详情请参阅其[文档](https://docs.relace.ai/api-reference/instant-apply/apply)。
  relace/relace-search:
    description_cn: |-
      relace-search 模型通过并行调用 4–12 个 `view_file` 和 `grep` 工具来探索代码库，并返回与用户请求相关的文件。

      与 RAG 不同，relace-search 采用智能体多步推理机制，生成高度精准的结果，速度比任何前沿模型快 4 倍。该模型设计为子智能体，将其发现传递给“预言机”编程智能体，由后者协调并执行后续编码任务。

      使用 relace-search 需构建合适的智能体框架，并解析其响应以提取相关信息交予预言机。更多详情请参阅 [Relace 文档](https://docs.relace.ai/docs/fast-agentic-search/agent)。
  sao10k/l3-euryale-70b:
    description_cn: |-
      Euryale 70B v2.1 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型。

      - 更强的提示遵循能力
      - 更佳的人体结构与空间感知能力
      - 对独特及自定义格式/回复样式的适应性显著提升
      - 极具创造力，提供大量新颖表达
      - 在角色扮演过程中无过多限制
  sao10k/l3-lunaris-8b:
    description_cn: |-
      Lunaris 8B 是一款基于 Llama 3 的多功能通用及角色扮演模型，通过战略性融合多个模型，在创造力、逻辑推理与通用知识之间取得良好平衡。

      由 [Sao10k](https://huggingface.co/Sao10k) 开发，旨在提供优于 Stheno v3.2 的体验，显著增强创造力与逻辑推理能力。

      为获得最佳效果，建议配合 Llama 3 Instruct 上下文模板使用，温度（temperature）设为 1.4，min_p 设为 0.1。
  sao10k/l3.1-70b-hanami-x1:
    description_cn: 这是 [Sao10K](/sao10k) 在 [Euryale v2.2](/sao10k/l3.1-euryale-70b) 基础上开展的实验。
  sao10k/l3.1-euryale-70b:
    description_cn: Euryale L3.1 70B v2.2 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型，是 [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b) 的继任版本。
  sao10k/l3.3-euryale-70b:
    description_cn: Euryale L3.3 70B 是由 [Sao10k](https://ko-fi.com/sao10k) 开发的专注于创意角色扮演的模型，是 [Euryale L3 70B v2.2](/models/sao10k/l3-euryale-70b) 的继任版本。
  stepfun-ai/step3:
    description_cn: Step3 是一款前沿的多模态推理模型，采用专家混合架构，总参数量达 3210 亿，每次前向传递激活 380 亿参数。该模型端到端设计，旨在最小化解码成本的同时，在视觉–语言推理任务中实现顶尖性能。通过多矩阵分解注意力（MFA）与注意力–前馈网络解耦（AFD）的协同设计，Step3 在旗舰级及低端加速器上均保持卓越效率。
  switchpoint/router:
    description_cn: |-
      Switchpoint AI 的路由模型可即时分析您的请求，并将其动态分发至不断演进的 AI 模型库中最优选项。

      随着大语言模型领域的持续进步，我们的路由系统亦同步进化，确保您无需更改现有工作流即可始终受益于业界最新模型。

      该模型在 OpenRouter 平台上按每条响应收取统一费率，由 [Switchpoint AI](https://www.switchpoint.dev) 完整路由引擎驱动。
  tencent/hunyuan-a13b-instruct:
    description_cn: Hunyuan-A13B 是腾讯开发的 130 亿激活参数混合专家（MoE）语言模型，总参数量达 800 亿，支持思维链（Chain-of-Thought）推理。该模型在数学、科学、编程及多轮推理等任务的基准测试中表现优异，同时通过分组查询注意力（GQA）机制和量化支持（FP8、GPTQ 等）实现高效推理。
  thedrummer/cydonia-24b-v4.1:
    description_cn: 基于 Mistral Small 3.2 24B 构建的无审查创意写作模型，具备良好的记忆能力、提示遵循性与智能水平。
  thedrummer/rocinante-12b:
    description_cn: |-
      Rocinante 12B 专为引人入胜的故事叙述和丰富文采而设计。

      早期测试者反馈：
      - 词汇量更广，用词独特且富有表现力
      - 创造力显著增强，可生成生动叙事
      - 故事情节充满冒险且引人入胜
  thedrummer/skyfall-36b-v2:
    description_cn: Skyfall 36B v2 是 Mistral Small 2501 的增强版本，经过专门微调，显著提升了创造力、细腻文风、角色扮演能力及连贯叙事表现。
  thedrummer/unslopnemo-12b:
    description_cn: UnslopNemo v4.1 是 Rocinante 创作者推出的最新模型，专为冒险题材写作和角色扮演场景设计。
  tngtech/deepseek-r1t-chimera:
    description_cn: |-
      DeepSeek-R1T-Chimera 通过融合 DeepSeek-R1 与 DeepSeek-V3（0324）构建而成，兼具 R1 的推理能力和 V3 的 token 效率优势。该模型基于 DeepSeek-MoE Transformer 架构，针对通用文本生成任务进行了优化。

      模型通过合并两个源模型的预训练权重，在推理能力、效率和指令遵循任务之间实现性能平衡。采用 MIT 许可证发布，适用于研究及商业用途。
  tngtech/deepseek-r1t-chimera:free:
    description_cn: |-
      DeepSeek-R1T-Chimera 通过融合 DeepSeek-R1 与 DeepSeek-V3（0324）构建而成，结合了 R1 的推理能力与 V3 的 Token 效率优化优势。该模型基于 DeepSeek-MoE Transformer 架构，专为通用文本生成任务优化。

      模型通过合并两个源模型的预训练权重，在推理能力、效率和指令遵循任务之间实现性能平衡。本模型采用 MIT 许可证发布，适用于研究及商业用途。
  tngtech/deepseek-r1t2-chimera:
    description_cn: DeepSeek-TNG-R1T2-Chimera 是 TNG Tech 推出的第二代 Chimera 模型，是一款 6710 亿参数的混合专家文本生成模型，由 DeepSeek-AI 的 R1-0528、R1 和 V3-0324 三个检查点通过专家集成（Assembly-of-Experts）融合而成。三亲本设计在 vLLM 下推理速度较原始 R1 提升约 20%，较 R1-0528 提升超 2 倍，在成本与智能之间取得良好平衡。该检查点标准使用支持最长 60k tokens 上下文（实测可达约 130k），并保持一致的 <think> token 行为，适用于长上下文分析、对话及其他开放式生成任务。
  tngtech/deepseek-r1t2-chimera:free:
    description_cn: DeepSeek-TNG-R1T2-Chimera 是 TNG Tech 推出的第二代 Chimera 模型，是一款 6710 亿参数的混合专家文本生成模型，由 DeepSeek-AI 的 R1-0528、R1 和 V3-0324 三个检查点通过专家集成（Assembly-of-Experts）融合而成。三亲本设计在 vLLM 下推理速度较原始 R1 提升约 20%，较 R1-0528 提升超 2 倍，在成本与智能之间取得良好平衡。该检查点标准使用支持最长 60k tokens 上下文（实测可达约 130k），并保持一致的 <think> token 行为，适用于长上下文分析、对话及其他开放式生成任务。
  tngtech/tng-r1t-chimera:
    description_cn: |-
      TNG-R1T-Chimera 是一款实验性大语言模型，擅长创意叙事与角色互动。它是2025年4月发布的原始 TNG/DeepSeek-R1T-Chimera 模型的衍生版本，仅通过 Chutes 和 OpenRouter 平台提供。

      主要特性与改进包括：

      - 具备富有创意且令人愉悦的个性；
      - 初步 EQ-Bench3 评分为约1305；
      - 智能水平显著高于原始版本，尽管推理速度略有下降；
      - “思考令牌”一致性大幅提升，即推理块与答案块边界清晰明确；
      - 工具调用能力显著增强。

      模型开发者 TNG Tech 要求用户遵循微软为其“MAI-DS-R1” DeepSeek 基础模型制定的审慎使用指南。相关指南可在 Hugging Face 获取（https://huggingface.co/microsoft/MAI-DS-R1）。
  tngtech/tng-r1t-chimera:free:
    description_cn: |-
      TNG-R1T-Chimera 是一款实验性大语言模型，擅长创意叙事与角色互动。它是2025年4月发布的原始 TNG/DeepSeek-R1T-Chimera 模型的衍生版本，仅通过 Chutes 和 OpenRouter 平台提供。

      主要特性与改进包括：

      - 具备富有创意且令人愉悦的个性；
      - 初步 EQ-Bench3 评分为约1305；
      - 智能水平显著高于原始版本，尽管推理速度略有下降；
      - “思考令牌”一致性大幅提升，即推理块与答案块边界清晰明确；
      - 工具调用能力显著增强。

      模型开发者 TNG Tech 要求用户遵循微软为其“MAI-DS-R1” DeepSeek 基础模型制定的审慎使用指南。相关指南可在 Hugging Face 获取（https://huggingface.co/microsoft/MAI-DS-R1）。
  undi95/remm-slerp-l2-13b:
    description_cn: 基于更新模型对原始 MythoMax-L2-B13 的复现尝试。#merge
  x-ai/grok-3:
    description_cn: Grok 3 是 xAI 推出的最新模型，作为其旗舰产品，在数据提取、编程和文本摘要等企业级应用场景中表现出色。在金融、医疗、法律和科学领域具备深厚的专业知识。
  x-ai/grok-3-beta:
    description_cn: |-
      Grok 3 是 xAI 最新推出的旗舰模型，在企业级应用场景（如数据提取、编程和文本摘要）中表现卓越，并在金融、医疗、法律和科学等领域具备深厚的领域知识。

      该模型在 GPQA、LCB 和 MMLU-Pro 等结构化任务与基准测试中表现优异，即使在高推理模式下也显著优于 Grok 3 Mini。

      注意：此模型提供两个 xAI 接入端点。默认情况下，使用该模型将始终路由至基础端点。若需使用高速端点，可添加 `provider: { sort: throughput }` 以按吞吐量优先排序。
  x-ai/grok-3-mini:
    description_cn: 一款轻量级模型，在响应前会进行思考。速度快、智能高效，适用于无需深厚领域知识的逻辑类任务。原始思考轨迹可供访问。
  x-ai/grok-3-mini-beta:
    description_cn: |-
      Grok 3 Mini 是一款轻量级、低开销的推理模型。与传统模型即时生成答案不同，Grok 3 Mini 会在响应前进行思考，特别适合无需深厚领域知识但需强推理能力的任务，在数学及量化场景（如解决高难度谜题或数学问题）中表现突出。

      支持透明的“思考”过程追踪。默认采用低推理强度，可通过设置 `reasoning: { effort: "high" }` 提升推理强度。

      注意：此模型提供两个 xAI 接入端点。默认情况下，使用该模型将始终路由至基础端点。若需使用高速端点，可添加 `provider: { sort: throughput }` 以按吞吐量优先排序。
  x-ai/grok-4:
    description_cn: Grok 4 是 xAI 最新推出的推理模型，上下文窗口达 256K 令牌。支持并行工具调用、结构化输出，以及图像与文本双模态输入。请注意：该模型的推理能力不可关闭、不可调节，也无法指定推理强度。当单次请求总令牌数超过 128K 时，计费标准将上调。更多详情请参阅 [xAI 官方文档](https://docs.x.ai/docs/models/grok-4-0709)。
  x-ai/grok-4-fast:
    description_cn: |-
      Grok 4 Fast 是 xAI 最新推出的多模态模型，具备业界领先的性价比，并支持200万token上下文窗口。该模型提供两种模式：非推理模式与推理模式。更多详情请参阅 xAI 官方[新闻公告](http://x.ai/news/grok-4-fast)。

      用户可通过 API 中的 `reasoning` 参数的 `enabled` 字段启用或禁用推理功能。[详见文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)
  x-ai/grok-4.1-fast:
    description_cn: |-
      Grok 4.1 Fast 是 xAI 最佳的智能体工具调用模型，在客服支持与深度研究等真实场景中表现卓越，支持 200 万 token 上下文窗口。

      可通过 API 中的 `reasoning` `enabled` 参数启用或禁用推理功能。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)
  x-ai/grok-code-fast-1:
    description_cn: Grok Code Fast 1 是一款高效经济的推理模型，在智能体编程任务中表现卓越。其响应中包含可见的推理轨迹，便于开发者引导 Grok Code 实现高质量工作流。
  xiaomi/mimo-v2-flash:
    description_cn: |-
      MiMo-V2-Flash 是小米开发的开源基础语言模型，采用混合专家（Mixture-of-Experts）架构，总参数量达 3090 亿，激活参数量为 150 亿，并采用混合注意力机制。MiMo-V2-Flash 支持混合思维开关与 256K 上下文窗口，在推理、编程及智能体场景中表现卓越。在 SWE-bench Verified 与 SWE-bench Multilingual 基准测试中，MiMo-V2-Flash 均位列全球开源模型榜首，性能媲美 Claude Sonnet 4.5，成本却仅为后者的约 3.5%。

      用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)。
  xiaomi/mimo-v2-flash:free:
    description_cn: |-
      MiMo-V2-Flash 是小米开发的开源基础语言模型，采用混合专家（Mixture-of-Experts）架构，总参数量达 3090 亿，激活参数量为 150 亿，并采用混合注意力机制。MiMo-V2-Flash 支持混合思维开关与 256K 上下文窗口，在推理、编程及智能体场景中表现卓越。在 SWE-bench Verified 与 SWE-bench Multilingual 基准测试中，MiMo-V2-Flash 均位列全球开源模型榜首，性能媲美 Claude Sonnet 4.5，成本却仅为后者的约 3.5%。

      用户可通过 `reasoning` 的 `enabled` 布尔值控制推理行为。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)。
  z-ai/glm-4-32b:
    description_cn: |-
      GLM-4-32B 是一款高性价比的基础语言模型。

      可高效执行复杂任务，在工具调用、在线搜索及代码相关智能任务方面能力显著增强。

      由 thudm 模型背后的同一实验室研发。
  z-ai/glm-4.5:
    description_cn: GLM-4.5 是我们最新推出的旗舰基础模型，专为智能体应用打造。采用混合专家（MoE）架构，支持最高 128k token 的上下文长度。GLM-4.5 在推理、代码生成和智能体对齐方面能力显著提升。支持混合推理模式：一种为复杂推理和工具调用设计的“推理模式”，另一种为即时响应优化的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  z-ai/glm-4.5v:
    description_cn: GLM-4.5V 是一款面向多模态智能体应用的视觉语言基础模型。基于混合专家（MoE）架构，总参数量达 1060 亿，每 token 激活 120 亿参数，在视频理解、图像问答、OCR 和文档解析等任务上达到业界领先水平，并在前端网页编码、事实依据和空间推理方面取得显著提升。模型提供混合推理模式：“思考模式”用于深度推理，“非思考模式”用于快速响应。推理行为可通过 `reasoning` `enabled` 布尔值切换。[了解更多](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  z-ai/glm-4.5-air:
    description_cn: GLM-4.5-Air 是我们最新旗舰模型系列的轻量级变体，同样专为以智能体为中心的应用场景设计。与 GLM-4.5 类似，它也采用混合专家（MoE）架构，但参数规模更为紧凑。GLM-4.5-Air 同样支持混合推理模式：提供用于高级推理和工具调用的“推理模式”，以及用于实时交互的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  z-ai/glm-4.5-air:free:
    description_cn: GLM-4.5-Air 是我们最新旗舰模型系列的轻量级变体，同样专为以智能体为中心的应用场景设计。与 GLM-4.5 类似，它也采用混合专家（MoE）架构，但参数规模更为紧凑。GLM-4.5-Air 同样支持混合推理模式：提供用于高级推理和工具调用的“推理模式”，以及用于实时交互的“非推理模式”。用户可通过 `reasoning` `enabled` 布尔值控制推理行为。[了解更多请参阅文档](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
  z-ai/glm-4.6:
    description_cn: |-
      相比 GLM-4.5，本代模型带来多项关键改进：

      更长上下文窗口：上下文窗口从 128K 扩展至 200K tokens，使模型能够处理更复杂的智能体任务。
      卓越编码性能：在代码基准测试中得分更高，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等实际应用中表现更佳，包括生成视觉效果更精美的前端页面。
      高级推理能力：GLM-4.6 的推理性能显著提升，支持推理过程中的工具调用，整体能力更强。
      更强大的智能体：在工具调用型和基于搜索的智能体任务中表现更优，并能更有效地集成到智能体框架中。
      精炼的写作能力：在风格与可读性方面更符合人类偏好，在角色扮演场景中表现更自然。
  z-ai/glm-4.6v:
    description_cn: GLM-4.6V 是一款大型多模态模型，专为高保真视觉理解及跨图像、文档和混合媒体的长上下文推理而设计。该模型支持高达 128K tokens，可直接将复杂页面布局和图表作为视觉输入进行处理，并集成原生多模态函数调用，实现感知与下游工具执行的无缝衔接。此外，该模型还支持交错式图文生成与 UI 重建工作流，包括截图转 HTML 合成及迭代式视觉编辑。
  z-ai/glm-4.6:exacto:
    description_cn: |-
      相比 GLM-4.5，本代模型带来多项关键改进：

      更长上下文窗口：上下文窗口从 128K 扩展至 200K tokens，使模型能够处理更复杂的智能体任务。
      卓越编码性能：在代码基准测试中得分更高，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等实际应用中表现更佳，包括生成视觉效果更精美的前端页面。
      高级推理能力：GLM-4.6 的推理性能显著提升，支持推理过程中的工具调用，整体能力更强。
      更强大的智能体：在工具调用型和基于搜索的智能体任务中表现更优，并能更有效地集成到智能体框架中。
      精炼的写作能力：在风格与可读性方面更符合人类偏好，在角色扮演场景中表现更自然。
  z-ai/glm-4.7:
    description_cn: GLM-4.7 是 Z.AI 最新旗舰模型，在两大核心领域实现升级：增强的编程能力与更稳定的多步推理/执行能力。该模型在执行复杂智能体任务方面表现显著提升，同时提供更自然的对话体验与更出色的前端交互效果。
